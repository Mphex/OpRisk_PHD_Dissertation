\documentclass[]{DissertateUSU}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[top=1in,bottom=1in,right=1in,left=1.5in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Towards a Framework for Operational Risk in the Banking Sector},
            pdfauthor={Mphekeleli Hoohlo},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Towards a Framework for Operational Risk in the Banking Sector}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Mphekeleli Hoohlo}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  
\newcommand{\yeardegree}{ 2019 } \newcommand{\degree}{ Doctor of Philosophy }
 \newcommand{\field}{ Risk Theory (Finance) }
 \newcommand{\chairperson}{ Eric Schaling, Ph.D. }
 \newcommand{\committeeone}{ Thanti Mthanti, Ph.D. }
 \newcommand{\committeetwo}{ Odongo Kodongo, Ph.D. }
 \newcommand{\committeethree}{ Thabang Mokoaleli-Mokoteli, Ph.D. }
 \newcommand{\committeefour}{ Christopher Malikane, Ph.D. }
 \newcommand{\gradschoolguy}{ Paul Alagidede, Ph.D. }
 % Tables
      \usepackage{booktabs}
      \usepackage{threeparttable}
      \usepackage{array}
      \newcolumntype{x}[1]{%
      >{\centering\arraybackslash}m{#1}}%
      \usepackage{placeins}
      \usepackage{chngcntr}
      \counterwithin{figure}{chapter}
      \counterwithin{table}{chapter}
      \usepackage[makeroom]{cancel}

\begin{document}
\maketitle

\pagenumbering{roman} \pagestyle{empty} \copyrightpage

\newpage

\pagestyle{fancy} \fancyhead[L]{Abstract} \fancyhead[R]{\thepage}
\fancyfoot[C]{} \chapter*{ABSTRACT}
\addcontentsline{toc}{section}{Abstract}

\doublespacing

\begin{center}
Towards a Framework for Operational Risk \\ 
in the Banking Sector \\
\vspace{12pt}
by \\
\vspace{12pt}
Mphekeleli Hoohlo \\
University of the Witwatersrand, 2019
\end{center}

\vspace{12pt}

\singlespacing
\noindent Major Professor: Eric Schaling, Ph.D.

\noindent Supervisor: Thanti Mthanti, Ph.D.

\noindent Department: Law, Commerce \& Management

\vspace{12pt}

\doublespacing

There have been a series of destructive events that have threatened the
stability of the financial system due to (OpRisk). In most, if not all
of these cases, human error is at the center of the chain of events that
lead or may lead to (OpRisk) losses. There are many attitudes that can
potentially infect organisational processes, the most persistent of
these attitudes stem from human failings that are exploitable Barberis
and Thaler (2003), thus forming a basis for the theoretical foundation
of \texttt{OpRisk}.

Shefrin (2016) notes that people would rather incur greater risks to
hold on to things they already have, than the risks they would taken to
get into that position in the first place, thereby risking a banks'
survival, rather than expose their trading losses by consciously
deceiving senior management to hide unethical operational practices. In
this paper the application of machine learning techniques on the
observed data demonstrates how these issues can be resolved given their
flexibility to different types of empirical data.

\hspace{11 cm} (116 pages)

\singlespacing

\newpage

\fancyhead[L]{Public Abstract} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\chapter*{PUBLIC ABSTRACT}
\addcontentsline{toc}{section}{Public Abstract}

\doublespacing

\begin{center}
Towards a Framework for Operational Risk \\ 
in the Banking Sector \\
Mphekeleli Hoohlo
\end{center}

\vspace{12pt}

The purpose of this research is to provide clarity; based on theory and
empirical evidence, on how to tackle the specific problems in the
\emph{operational risk} (OpRisk) literature, which have earned a place
in modern day recource in in risk and finance, due to how significantly
its importance has increased over the last few decades. During this
period, until present day, there have been and continues to be series of
destructive events that have threatened the stability of financial
systems due to OpRisk. In most, if not all of these cases, human error
is at the center of the chain of events that lead or may lead to
(OpRisk) losses. There are many attitudes that can potentially infect
organisational processes, the most persistent of these attitudes stem
from human failings that are exploitable Barberis and Thaler (2003),
thus forming a basis for the theoretical foundation of \texttt{OpRisk}.

Shefrin (2016) notes that people would rather incur greater risks to
hold on to things they already have, than the risks they would taken to
get into that position in the first place, thereby risking a banks'
survival, rather than expose their trading losses by consciously
deceiving senior management to hide unethical operational practices. In
this paper the application of machine learning techniques on the
observed data demonstrates how these issues can be resolved given their
flexibility to different types of empirical

\singlespacing

\newpage

\fancyhead[L]{Dedication} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\chapter*{DEDICATION} \addcontentsline{toc}{section}{Dedication}

This work---the dissertation and all work associated with it---is
dedicated to \ldots. I will always be grateful to and for . I also
dedicate this work to my child, who patiently loved a father who was
often busy working, even when at home. Finally, I dedicate this work to
my parents and siblings, who kept me level-headed throughout the
process, providing wise and thoughtful advice.

This work is truly evidence of the love I am surrounded by.

\hspace{10.5 cm} \emph{Mphekeleli Hoohlo}

\newpage

\fancyhead[L]{Acknowledgments} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\chapter*{ACKNOWLEDGEMENTS}
\addcontentsline{toc}{section}{Acknowledgments}

Several individuals deserve recognition for their help in this work.
First, I am greatly indebted to \ldots{}.. for giving me the opportunity
to find where my research passions are---especially quantitatively.
\ldots{}. provided guidance in a productive and capable researcher. I
also owe a great deal \ldots{}.. when my application was delayed. He has
also proven to be an important mentor. Relatedly, I am grateful to
providing opportunities to develop as a researcher in an important
field.

My friends and colleagues in the program also deserve serious
recognition for their help and support. Specifically, as I pursued my
goals in the program. \ldots{}. provided timely and important help
throughout the process. I appreciate .

I also want to acknowledge my committee, in providing feedback,
guidance, and support throughout my graduate student career. The
department's and college's administrative staff and heads, although
working in the background, were a huge help in getting me to where I am
today. Finally, Wits University as a school has proven to be a fantastic
place to grow academically and to mature into a researcher.

\newpage

\fancyhead[L]{Table of Contents} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\tableofcontents

\newpage

\fancyhead[L]{List of Tables} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\listoftables

\newpage

\fancyhead[L]{List of Figures} \fancyhead[R]{\thepage} \fancyfoot[C]{}
\listoffigures

\newpage

\pagenumbering{arabic}

\newpage

\fancyhead[L]{Introduction} \fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{INTRODUCTION}

\doublespacing

\textbf{\section{Purpose of the study}} \label{sec:Purpose of the study}

The purpose of this research is to apply a generalised linear model
(GLM) suitable for exposure-based operational risk (EBOR) treatments
within the operational risk management framework (ORMF), effectively
replacing historical loss severity curves obtained from historical loss
counts, by forward-looking measures using event frequencies based on
actual operational risk (OpRisk) exposures. Preliminary work on EBOR
models was undertaken by (Einemann, Fritscher, and Kalkbrener, 2018).
Secondly, this study provides a comprehensive computational comparison
of various data-intensive techniques amongst each other, and versus
\emph{classical} statistical estimation methods for classification and
regression performances.\medskip

Our understanding of existing ORMF to date is limited to the assumption
that financial institutions (FI's) are risk-neutral. Thirdly, in lieu of
the afore-mentioned, this study finally seeks to invalidate the
risk-neutral assumption, by means of various unsupervised learning
techniques, by proposing that FI's are more risk-averse; this can be
measured by analysing subtle patterns between data features and trends
in the allocated risk capital estimates. In theory, a risk manager who
experiences persistent/excessive losses due to particular risk events,
would over-compensate cover for these particular risk types, and this
would show in reduced losses in these types over time.

\textbf{\section{Fundamentals of ORMF's}}
\label{sec:Fundamentals of ORMF's}

Most banks' estimates for their risk are divided into credit risk
(50\%), market risk (15\%) and OpRisk (35\%). Cruz (2002) postulated
that OpRisk, which focuses on the human side of risk management is
difficult to manage with the reduced ability to measure it. The process
of OpRisk, that is, the how manifests in conscious and/or unconscious
states of the risk manager/s (Hemrit and Arab, 2012), and encompasses
approaches and theories that focus on how one will choose when faced
with a decision, based on how comfortable they are with the situation
and the variables that are present.

\subsection{Definition of operational risk}
\label{ssec:Definition of operational risk}

Operational risk (OpRisk) is defined as:
\emph{The risk of loss resulting from inadequate or failed internal processes, people and systems, and from external events. This definition includes legal risk, but excludes strategic and reputational risk.}(Risk,
2001).\medskip

A major managerial concern for businesses is an inability to identify
and account for their susceptibility to OpRisk events following a number
of very costly and highly publicized operational losses, in particular,
it became popular following a fraudulent trading incident which was
responsible for a catastrophic loss that lead to the collapse of Barings
Bank (the UK's oldest bank) in 1995.\medskip

The term OpRisk began to be used after the afore-mentioned and similar
types of OpRisk events became more common. A (rogue) trader (Nick
Leeson), who risked the banks' survival rather than expose his trading
losses, by consciously deceiving senior management to hide his unethical
acts, was found to have been responsible for unethical trading practices
when he created illegal trades in his account, then used his position in
the front and back offices of the bank to hide his trading losses. Worse
still, he incurred a greater risk to the bank by lying in order to give
a false impression of his profits. Shefrin (2016) notes that people
would rather incur greater risks to hold on to things they already have,
than the risks they would taken to get into that position in the first
place.\medskip

It was later discovered that he was placing illegal bets in the
Asian-markets, and kept these contracts out of sight from senior
management to cover up his illegal activity. When his fraudulent
behaviour was discovered (after an earthquake hit at Kobe in Japan, that
collapsed the Osaka Securities Exchange) he succumbed to unrecoverable
losses due to trading positions he had accumulated, which resulted in a
loss of around \pounds 1.3 billion to the bank, thus resulting in it's
collapse. In most, if not all of these cases, human error is at the
center of the chain of events that lead or may lead to OpRisk
losses.\medskip\medskip

Since then, there have been a series of destructive events that have
threatened the stability of the financial system due to OpRisk. Large
fines have been imposed on the culprits and regulatory scrutiny has been
heightened as a result of a number of operational events, e.g.~the
January 2016 \lq\lq Dark Pool\rq\rq~trading penalties suffered by
Barclays (\$70mn) and Credit Suisse (\$85mn), imposed by the United
States (US) based securities exchange commision (SEC). These OpRisk loss
events were due to fraudulent trading activity consisting of rogue
traders dealing in illegally placed high frequency trades for private
clients where prices were hidden.\medskip 

In South Africa (SA), there is an upcoming case of price fixing and
market allocation in trading foreign exchange (FX) currency pairs,
reffered to the SA based competition tribunal for prosecution. Absa
bank, Standard bank \& Investec may be liable to payment of an
admistrative penalty equal to 10\% of their annual turnover in 2016,
following accusations by the local based competition commission in
February 2017, of rogue traders manipulating the price of the rand
through buying and selling US dollars in exchange for the rand at fixed
prices. According to the competition commission, it has been alleged
that currency traders have been colluding or manipulating the price of
the rand through these buy and sell orders to change supply of the
currency.\medskip

This has compromised the quality and accuracy of risk management's
advisory service and pedigree, and aroused huge interest as the value of
the rand has implications on South African's. Furthermore, this kind of
behaviour can lead to catastrophic operational losses, as with the case
for the Barings event, resulting is a mismatch between business'
expectations and the value the risk management practice was able to
deliver, which is prevalent across FI's and remains unchanged. There are
many attitudes that can potentially infect organisational processes, the
most persistent of these attitudes stem from human failings that are
exploitable (Barberis and Thaler, 2003); i.e.~humans' propensity to be
deceitful during periods of distress, thus forming a basis for a
theoretical foundation of OpRisk management.

\textbf{\section{Basel Committee's quantitative operational risk management framework}}
\label{sec:Basel Committee's quantitative operational risk management framework}

The Bank for International Settlements (BIS) is an organisation
consisting of a group of central bank governors and heads of supervision
of central banks around the world who represent an authority on good
risk management in banking. More specifically, the BIS oversee the
duties of the Basel Committee on Banking Supervision (BCBS)/Basel
Commitee. The role of the BCBS is to set out guidelines on international
financial regulation to cover risks in the banking sector. There have
been three banking accords from the BCBS under the supervision of the
BIS in dealing with financial regulation, viz., Basel I, Basel II \&
Basel III. These accords describe an overview of capital requirements
for financial institutions (FI's) in order to create a level playing
field, by making regulations uniform throughout the world.\medskip 

\subsection{The Capital Adequacy Accord (Basel I)}

Basel I was established in 1988. Basel I meant that FI's were required
to assign capital for credit risk to protect against credit default. In
1996, an amendment to Basel I imposed additional requirements to cover
exposure due to market risk as well as credit risks. Basel I effectively
minimised rules that favoured local FI's over potential foreign
competitors, by opening up global competition so that these banks could
buffer against international solvency. In 2001, the Risk (2001)
consultative package provided an overview of the proposed framework for
regulatory capital (RC) charge for OpRisk. A fiancial institution (FI)
has an OpRisk component, which constitutes a substantial risk component
other than credit and market risk. There are two types of OpRisk's viz.,
potential high severity risk where the probability of an extreme loss is
very small but costly, and high frequency/low severity risk where
frequency plays a major role in the OpRisk capital charge
calculation.\medskip 

\subsection{New Capital Adequacy Accord (Basel II)}

The framework for Basel II was implemented in June 2006. The rationale
for Basel II is to introduces risk sensitivity through more restrictive
capital charge measures and flexibility with specific emphasis on
OpRisk. The structure of the new accord is built upon a three-pillar
framework: Pillar I stipulates minimum capital requirements for the
calcualtion of regulatory capital for credit risk, market risk and
OpRisk in order to retain capital to ward against these risks. Pillar II
imposes a supervisory review process through which additional
requirements can be imposed, such as the bank's internal capital
assessements, or to act on needed adequate capital support or best
practice for mitigating their risks. Pillar III relates to market
discipline, i.e.~transparency requirements which require banks to
publicly provide risk disclosures to keep them in line by enabling
investors to form an accurate view of their capital adequacy, in order
to reward or punish them on the basis of their risk profile.\medskip

\subsection{Basel III}

Basel III establishes tougher capital standards through more restrictive
capital definitions, higher RWA's, additional capital buffers, and
higher requirements for minimum capital ratios (Dorval, 2013). Through
Basel III, the BCBS is introducing a number of fundamental reforms
grouped under three main headings (Committee and others, 2010): 1{]} A
future of more capital through incremental trading book risk (credit
items in trading book treated in the same way as if they were in banking
book), 2{]} More liquidity through the introduction of a global
liquidity risk standard (Basel III will push banks toward holding
greater levels of liquid instruments, such as government bonds and more
liquid corporate instruments), and 3{]} Lower risk under the new
requirements of the capital base, i.e., establish more standardized
risk-adjusted capital requirements.\medskip

Regarding the sequence Basel I and Basel II: Regulation begins as a
qualitative recommendation which requires banks to have an
assets-to-capital multiple of at least 20, then focuses on ratios in
which both on-balance sheet and off-balance sheet items are used to
calculate the bank's total risk-weighted assets
(RWA's)\footnote{Also reffered to as risk-weighted amount, it is a measure of the bank's total credit exposure},
then on tail risk. In other words, auditors' discretion is replaced by
market perception of capital, meaning there is a market risk capital
charge for all items in the trading business line, then exciting new
static risk management approaches which involve calculating a 99.9
percentile left tail confidence interval to measure OpRisk value-at-risk
(VaR) and convert it into a RC charge.\medskip 

The future regulatory environment requires OpRisk professionals, who are
not only intelligent, creative and motivated but also have the courage
to uphold the OpRisk advisory service standards. Businesses that want to
successfuly manage risk, would be well advised to utilize new
theoretical and empirical techniques, such that large and small scale
experiments play an important role in risk analysis and regulatory
research.

\textbf{\section{Modern OpRisk measurement frameworks (ORMF's)}}
\label{sec:modern OpRisk measurement frameworks (ORMF's)}

Basel II describes three methods of calculating capital charge for
OpRisk RC viz., the standardised approach (SA), the basic indicator
approach (BIA) and the internal measurement approach (IMA). The basic
indicator approach (BIA) sets the OpRisk RC equal to a percentage (15\%)
of the annual gross income of the firm as a whole to determine the
annual capital charge. The SA is similar to the BIA except the firm is
split into eight business lines and assigned a different percentage of a
three year average gross income per business line, the summation of
which is the capital charge (Hoohlo, 2015). In the IMA, the bank uses
it's own internal models to calculate OpRisk loss.\medskip

\subsection{Advanced Measurement Approach (AMA)}

The advanced measurement approach (AMA) is an IMA method which applies
estimation techniques of OpRisk capital charge derived from a bank's
internal risk measurement system Cruz (2002). Basel II proposed
measurement of OpRisk to define capital requirements against unexpected
bank losses whereas the unexpected loss (UL) is the quantile for the
level \(\alpha\) minus the mean. According to the AMA, which is thought
to outperform the simpler SA approach and the BIA, RC requirements are
defined according to the UL limit in one year and the loss distribution
at a 99.9\% confidence level (\(\alpha = 0.01\%\)) aggegate loss
distribution\footnote{The aggregate loss distribution is obtained by convoluting a loss event frequency distribution and a loss severity distribution by means of the random sums method.}
used as a measure of RC. The BCBS proposes to define RC as \(RC = UL\).
This involves simulations based on historical data to establish
frequency and severity distributions for losses. In this case the RC is
a VaR measure.\medskip

The Basel III capital adequacy rules permit model-based calculation
methods for capital, including the AMA for OpRisk capital. Under Basel
III, standardised methods for OpRisk capital have been overhauled,
however for a while there was no prospect of an overhaul of the AMA.
Given the relative infancy of the field of OpRisk measurement, banks are
mostly free to choose among various AMA principle-based frameworks to a
significant degree of flexibility (Risk, 2016). A bank that undertakes
an AMA should be able to influence their capital requirements through
modeling techniques resulting in lowered pressure on OpRisk capital
levels, which in turn has a positive impact on the bank.\medskip

A FI's ability to determine the framework used for its regulatory OpRisk
RC calculation, evolves from how advanced the FI is along the spectrum
of available approaches used to determine capital charge (Risk, 2001).
BCBS recognizes that a variety of potentially credible approaches to
quantify OpRisk are currently being developed by the industry, and that
these R\&D activities should be incentivised. Increasing levels of
sophistication of OpRisk measurement methodologies should generally be
rewarded with a reduction in the regulatory OpRisk capital requirement.

\subsection{The standardised measurement approach (SMA)}

The flexibility of internal models was expected to narrow over time as
more accurate OpRisk measurement was obtained and stable measures of RC
were reached, ultimately leading to the emergence of best practice.
Instead, internal models produced wildly differing results of OpRisk RC
capital from bank to bank, contrary to the expectations of the BCBS. In
March 2016, the BCBS published for consultation a standardised
measurement approach (SMA) for OpRisk RC; that proposes to abandon the
freedom of internal modelling (thus ending the AMA) approaches for
OpRisk RC, in exchange for being able to use a simple formula to
facilitate comparability across the industry.\medskip

Under the SMA, RC will be determined using a simple method comprising of
two components: A stylised systemic risk model (business indicator
component), and an idiosyncratic risk model (loss component), which are
combined via an internal loss multiplier (ILM), whose function is to
link capital to a FI's operational loss experience to determine SMA
capital.\medskip

The SMA formula is thought to be consistent with regulators' intent for
simplification and increased comparability across most banks. However,
there is a feeling from some in the banking industry that the SMA is
disadvantaged as it is not the same as measuring OpRisk. Mignola,
Ugoccioni, and Cope (2016) and Peters, Shevchenko, Hassani, and Chapelle
(2016) identified that the SMA does not respond appropriately to changes
in the risk profile of a bank i.e., it is unstable viz., two banks of
the same risk profile and size can exibit OpRisk RC differences
exceeding 100\%, and risk insensitive; that SMA capital results
generally appear to be more variable across banks than AMA results,
where banks had the option of fitting the loss data to statistical
distributions.

\subsection{Argument}
\label{ssec:Argument}

Over the last twenty years, hard-won incremental steps to develop a
measure for the size of OpRisk exposure along with the emergence of
promising technologies presents a unique opportunity for bankers and
treasurers - traditionally risk-averse players - to develop a novel type
of way of looking at decision making under risk/uncertainty. New
technologies have been introduced which make use of up to date technical
solutions (such as homo heuristics developed by Gigerenzer and Brighton
(2009), who mainatain their methods solve practical finance problems by
simple rules of thumb, or Kahneman (2003)'s intuitive judgements and
deliberate decision making), argued to more likely represent the true
embedded OpRisk in financial organisations as these methods are designed
to fit normal behavioral patterns in their formulation, which is
consistent with how decisions are made under risk/uncertainty.\medskip 

What are the important steps toward completing the post crisis reforms
during the current year? Should the risk management fraternity follow
the
chartered\footnote{Meaning as of the publication [@risk2016supporting] the methods brought forth in the consultative document have not been approved for the public, the ideas within an experimental (leased) phase for the exclusive use of BCBS and certain FI's}
path followed in the Risk (2016) consultative document, scrapping away
twenty years of internal measurement approaches (such as the AMA), or
should the focus of financial regulators shift toward improving on what
they see fit within current existing AMA frameworks. The question is
should OpRisk managements' focus be on stimulating active discussions on
practical approaches to quantify, model and manage OpRisk for better
risk management and improved controls, or abandon the adoption of
innovative measurement approaches, such as the AMA, in exchange for
being able to use a simple formula across the whole industry?\medskip 

\textbf{\section{Context of the study}} \label{sec:Context of the study}

Regulatory reforms are designed and fines imposed to protect against
operational errors and other conduct costs connected with wrongdoing and
employee misconduct. Despite the introduction and use of these seemingly
robust strategies, regulations, processes and practices relating to
managing risk in FI's, bank losses continue to occur at a rather
distressing frequency. A cyclical pattern of OpRisk loss events still
persists; as evidenced in the recent price fixing and collusion cases,
defeating the explicit objectives of risk management frameworks. This
demonstrates a scourge of reflexivity prevailing in financial markets
emphasising that, there are theories that seem to work for a time only
to outlive their use and become insufficient for the complexities that
arise in reality.\medskip 

\subsection{Why \texttt{OpRisk?}}

A forceful narrative in management theory is that an organisation
running effective maintenance procedures combined with optimal team and
individual performers i.e., the right balance of skills in the labour
force and adequate technological advancements, means systems and
services can be used to more efficiently produce material gains, enhance
organisational effectiveness, meet business objectives and increase
investment activity. Conversely, the risk of the loss of business
certainty associated with lowered organisational competitiveness and
inadequate systems technology that underpins operations and services is
a key source leading to a potential breakdown in investment services
activity (Hoohlo, 2015). In fact OpRisk control could set banks apart in
competition. This serves as an incentive to support regulation,
particularly Basel III recovery and resolution processes.\medskip

Consider the case of a regulator in a financial system, who assumes that
he/she is consiously and accurately analysing an observed subject,
trusting the validity and relying on the visual information that their
sense of sight reveals. In the absence of visual confirmation they are
hindered from extracting and/or analysing information about the system
and their efforts to regulate could potentialy fail. The organisational
methods and functioning of current information systems in this industry
sector obscure the full extent of OpRisk challenges from the eyes of the
risk practitioner.\medskip 

When an attack such as an operational error occurs at a speed that the
OpRisk agent (an individual legal entity or a group) is unable to react
quickly enough, due to limitations of their processing speed, and they
are not able to process all the information in the given time span, they
could lose control/fail to comply with regulatory standards. The latter
case is more often than not the most accurate reflection of current risk
management practices. The agent represents one end of the spectrum of a
risk management strategy, which mitigates risk and enforces regulation,
dependent on the information recieved. The other end of the spectrum is
one which does not react at all to changes in the system
environment.\medskip 

Current conventional financial systems where information processing is
slow and have a tendency to rely on manual, uncertain, unpredictable and
unrealistic controls, obscure risk management reporting and produce
undesirable market conditions. The OpRisk management function should be
able to assist the firms' ability to mitigate risks by acquiring and/or
refining risk management solutions which deliver reliable and consistent
benefits of improved control and management of the risks inherent in
banking operations (Dorval, 2013). This proposal attempts to fill the
gap in the current system where there is a risk management information
lag or an obstruction from the eyes of the risk practitioner.

\textbf{\section{Analysis and interpretation issues with behavioral finance theory}}
\label{sec:Analysis and interpretation issues with behavioral finance theory}

Behavioral management theory is very much concerned with social factors
such as motivation, support and employee relations. A critical component
of behavioral finance is building models which better reflect actual
behavior. Studies have revealed that these social factors are not easy
to incorporate into finance models or to understand in the traditional
framework.\medskip 

The traditional finance paradigm seeks to understand financial markets
using models in which agents are \lq\lq rational\rq\rq. According to
Barberis and Thaler (2003), this means that agents update their beliefs
on the onset of new information, and that given their beliefs, they make
choices that are normatively acceptable, and that most people do this
most of the time. Neoclassical theory has grown to become the primary
take on modern-day economics formed to solve problems for decision
making under uncertainty/risk. Expected Utility Theory (EUT) has
dominated the analysis and has been generally accepted as the normative
model of rational choice, and widely applied as a descriptive model of
economic choice (Kahneman and Tversky, 2013).

\subsection{Expected utility theory}
\label{ssec:Expected utility theory}

Expected utility
theory\footnote{Expected utility theory provides a model of rationality based on choice.}
(EUT): We see a fundamental relation for expected utility (Expectation)
of a contract \(X\), that yields outcome \(x_i\) with probability
\(p_i\), where \(X = (x_1,p_1; ...; x_n,p_n)\) and
\(p_1+p_2+\ldots+p_n=1\) given by:

\singlespacing

\begin{equation}\label{EUT_extended}
U(x_1,p_1;\ldots;x_n,p_n) = p_1u(x_1)+\ldots+p_nu(x_n) 
\end{equation}

\doublespacing
corroborated by Morgenstern and Von Neumann (1953); Friedman and Savage
(1948); Kahneman and Tversky (2013) \& others.

A common thread running through the rational viz., the neoclassical take
of modern-day economics vs the non-neoclassical schools of thought are
findings of behavioral economics which tend to refute the notion that
individuals behave rationally. Many argue that individuals are
fundamentally irrational because they do not behave rationally giving
rise to a literature and debates as to which heuristics and sociological
and institutional priors are rational (Altman, 2008).\medskip

In the real world there is a point of transition between the traditional
(neoclassical) approach to decision making, based on data and data
anaysis (logic and rational), by adding new parameters and arguments
that are outside rational conventional thinking but are also valid. For
example, that neoclassical theory makes use of the assumption that all
parties will behave rationally overlooks the fact that human nature is
vulnerable to other forces, which causes people to make irrational
choices.\medskip 

An essential ingredient of any model trying to understand trading
behavior is an assumption about investor preferences (Barberis and
Thaler, 2003), or how investors evaluate risky gambles. Investors
systematically deviate from rationality when making financial decisions,
yet as acknowledged by Kuhnen and Knutson (2005), the mechanisms
responsible for these deviations have not been fully identified. Some
errors in judgement suggest distinct mental operations promote different
types of financial choices that may lead to investing mistakes.
Deviations from the optimal investment strategy of a rational risk
neutral agent are viewed as risk-seeking mistakes and risk-aversion
mistakes (Kuhnen and Knutson, 2005).\medskip 

\subsection{Theoretical investigations for the quantification of moderm ORMF}

Kuhnen and Knutson (2005) explain that these risk-seeking choices (such
as gambling at a casino) and risk-averse choices (such as buying
insurance) may be driven by distinct
neural\footnote{As recent evidence from human brain imaging has shown [@kuhnen2005neural] linking neural states to risk-related behaviours [@paulus2003increased].}
phenomena, which when activated can lead to a shift in risk preferences.
Kuhnen and Knutson (2005) found that certain areas of the brain precede
risk-seeking mistakes or risky choices and other areas precede
risk-aversion mistakes or riskless choices. A risk-aversion mistake is
one where a gamble on a prospect of a gain is taken by a risk-averse
agent in the face of the chance of a prospective loss. The fear of
losing prohibits one's urge to gamble, but people engage in gambling
activity anyway. Barberis and Thaler (2003) show that people regularly
deviate from the traditional finance paradigm evidenced by the extensive
experimental results compiled by cognitive psycologists on how people
make decisions given their beliefs.\medskip 

Kahneman and Tversky (2013) maintains, preferences between prospects
which violate rational behaviour demonstrate that outcomes which are
obtained with certainty are overweighted relative to uncertain outcomes.
This will contribute to a risk-averse preference for a sure gain over a
larger gain that is merely probable or a risk-seeking preference for a
loss that is merely probable over a smaller loss that it certain. As a
psycological principle, overweighting of certainty favours risk-aversion
in the domain of gains and risk-seeking in the domain of losses.\medskip

The present discussion replicates the common behavioral pattern of risk
aversion, where people weigh losses more than equivalent gains.
Furthermore, neuroeconomic research shows that this pattern of behavior
is directly tied to the brain's greater sensitivity to potential losses
than gains (Tom, Fox, Trepel, and Poldrack, 2007). This provides a
target for investigating a more comprehensive theory of individual
decision-making rather than the rational actor model and thus yield new
insights relevant to economic
theory\footnote{Representing ability of FI's financial market models to characterise the repeated decision-making process that applies to loss aversion}
(Kuhnen and Knutson, 2005).\medskip  

If people are reasonably accurate in predicting their choices, the
presence of systematic violations of risk neutral behavior provides
presumptive evidence against this i.e., people systematically violate
EUT when choosing among risky gambles. This seeks to improve and adapt
to reality and advance different interpretations of economic behaviour;
viz., to propose a more adequately descriptive model, that can represent
the basis for an alternative to the way the traditional model is built
for decisions taken under uncertainty. This has led some influential
commentators to call for an entirely new economic paradigm to displace
conventional neoclassical theory with a psycologically more realistic
preference specification (List, 2004).

\textbf{\section{A new class of ORMF models approach}}
\label{sec:A new class of ORMF models approach}

A substantial body of evidence shows that decision makers systematically
violate EUT when choosing between risky prospects. Indeed, people would
rather satisfy their needs than maximise their utility, contravening the
normative model of rational choice (i.e., EUT) which has dominated the
analysis of decision making under risk. In recent work (Barberis and
Thaler, 2003) in behavioral finance, it has been argued that some of the
lessons learnt from violations of EUT are central to understanding a
number of financial phenomena. In response to this, there has been
several theories put forward advocating for the basis of a slightly
different intepretation which describes how individuals actually make
decisions under uncertainty/risk. Of all the non-EUT's, we focus on
Prospect Theory (PT) as this framework has had most success matching
most empirical
facts\footnote{OpRisk loss events in FI's are largely due to human failings that are exploitable e.g., fraudulent trading activity, and PT is based on the same behavioural element of how people make financial decisions about prospects}.\medskip 

Kahneman and Tversky (2013) list the key elements of PT, which are 1{]}
a value function, and 2{]} a non-linear transformation of the
probability scale, that factors in risk aversion of the participants.
According to Kahneman and Tversky (2013), the probability scale
overweights small probabilities and underweights high probabilities.
This feature is known as loss/risk aversion: This means that people have
a greater sensitivity to losses (around 2.5 times more times) than
gains, and are especially sensitive to small losses unless accompanied
by small
gains\footnote{Diminishing marginal utility for gains but opposite for losses.}.
Loss aversion is a strong differentiator when it comes to explaining
exceptions to the general risk patterns that characterize prospect
theory.\medskip 

\subsection{Prospect theory}
\label{ssec:Prospect theory}

By relaxation of the expectation principle in equation
\ref{sssec:Expected utility theory}, the over-all value
\(\mathbf{\bigvee}\) of the regular prospect \((x,p;y,q)\): In such a
prospect, one receives \(x\) with probability \(p\), \(y\) with
probability \(q\), and nothing with probability \(1-p-q\), is expressed
in terms of two scales, \(\pi(\cdot)\), and \(\nu(\cdot)\), where
\(\pi(\cdot)\) is a decision weight and \(\nu(\cdot)\) a number
reflecting the subjective value of the outcome. Then
\(\mathbf{\bigvee}\) is assigned the value:

\begin{equation}\label{eqn2}
\mathbf{\bigvee}=\pi(p)\nu(x)+\pi(q)\nu(y) \qquad\mbox{iff} \qquad p+q \leq 1
\end{equation}

The scale, \(\pi\), associates with each probability \(p\) a decision
weight which reflects the impact of \(p\) on the over-all value of the
prospect. The second scale, \(\nu\), assigns to each outcome \(x\) a
number \(\nu(x)\), which measures the value of deviations from a
reference point i.e., gains or losses. \(\pi\) is not a probability
measure and \(\pi(p) + \pi(1-p) < 1\). Through PT we add new parameters
and arguments to improve the mathematical modelling method for decisions
taken under risk/uncertainty, such that the value of each outcome is
multiplied by a decision weight, not by an additive probability.

PT looks for common attitudes in people (in FI's) with regard to their
behaviour toward taking financial risks or gambles that cannot be
captured by EUT. In light of this view, people are not fully invested in
either of the percieved outcomes \(x\) and \(y\), Which tells us that
\(p+q \leq 1\). In lieu of this, an FI using (internal) historical
OpRisk loss data to model future events; say a historical case of fraud
at the FI occurs and is incorporated in the model, the probability of
making the same error in future is provided for in the model versus risk
events that haven't happened. The modelled future should over-provide
for the loss events that have already occured, which fits normal
patterns around individuals psycological make up and is consistent with
risk-averse behavior. The idea at the basis of PT is that a better
modeling method can be obtained which leads to a closer approximation of
the over-all-value of OpRisk losses.

\subsection{Modeling}

In this study, an important new algorithm for ORMFs and is laid out
coupled with data intensive estimation techniques; viz. Generalised
Additive Models for locatin Scale \& Shape (GAMLSS), Generalized Linear
Models (GLDs), Artificial Neural Networks (ANNs), Random Forest (RF) \&
Decision Trees (DTs), which have capabilities to tease out the deep
hierarchies in the features of covariates irrespective of the challenges
associated with the non-linear or multi-dimensional nature of the
underlying problem, at the same time supporting the call from industry
for a new class of EBOR models that capture forward-looking aspects.
Machine Learning (ML) is used as a substitute tool for the traditional
model based Autoregressive Moving Average (ARMA) used for analysing and
representing stochastic processes. As opposed to the statistical tool,
ML does not impose a functional relationship between variables, the
functional relationship is determined by extracting the pattern of the
training set and by learning from the data observed.\medskip 

Using computationally intensive (using ML techniques on historical data
) OpRisk measurement techniques and mixing with a theory is not a new
approach for modeling, particularly in calculating OpRisk RC; as
evidenced through Agostini, Talamo, and Vecchione (2010) in a study
whereby the LDA model for forecasting OpRisk RC, via VaR, was
implemented in conjunction with the use of advanced credibility theory
(CT). The idea at the basis of their use of CT, is to advance the very
recent literature that a better estimation of the OpRisk RC measurement
can be obtained by integrating historical data and scenario analysis
i.e., combining the historical simulations with scenario assessments
through formulas that are weighted averages of the historical data
entries and scenario assessments, advocating for the combined use of
both experiences.\medskip 

However, applying ML is an original way of looking at the approximation
issue as opposed to advanced CT. The essential feature of PT are
assumptions which are more compatible with basic principles of
perception and judgement for decisions taken under uncertainty, whereas
ML will reveal additional chance probabilities determined through the
natural clusters of unknown data feature findings from which new
discoveries are made.\medskip

According to Kahneman and Tversky (2013), the decision maker, who is a
risk agent within the FI, constructs a representation of the losses and
outcomes that are relevant to the decision, then assesses the value of
each prospect and chooses according to the losses (changes in wealth),
not the overall financial state of the FI. We wish to bring the
prescribed model to equilibrium, by applying a method that tries to
establish what accurately ascribes to decision rules that people wish to
obey, in made predictions about what operational loss events might
result in the future, then use empirical data to test this idea in a way
that is falsifyable.

\textbf{\section{Problem statement}} \label{sec:Problem statement}

\subsection{Main problem}

The existing models of OpRisk VaR measurement frameworks assume FI's are
risk neutral, and do not learn from past losses/mistakes: We address
weaknesses in current OpRisk VaR measurement frameworks by assuming that
FI's are more risk averse. Furthermore, introducing exposure-based
operational risk modeling, we gain an understanding of how capturing
past losses and exposures of forward looking aspects affect risk
attitudes using machine learning techniques. As a consequence, projected
future losses are estimated through a learning algorithm adapting
capital estimates to changes in the risk profile, i.e.~in the
introduction of new products or changes in the business mix of the
portfolio (e.g.~mergers, trade terminatons, allocations or
disinvestments), providing sufficient incentives for OpRisk management
to mitigate risk.

\textbf{\section{Objectives of the study}}
\label{sec:Objectives of the study}

The research objectives are three-fold:

\subsection{Exposure-based OpRisk (EBOR) models}

To quantify OpRisk losses by introducing generalised additive models for
location, scale and shape (GAMLSS) in the framework for OpRisk
management, that captures exposures to forward-looking aspects of the
OpRisk loss prediction problem. EBOR treatments effectively replace
historical loss severity curves obtained from historical loss counts, by
looking into deep hierarchies in the features of covariates in
investment banking (IB), and by forward-looking measures using event
frequencies based on actual operational risk (OpRisk) exposures in the
business environment and internal control risk factors (BEICF) thereof.

\subsection{Modeling OpRisk depending on covariates}

To investigate the performance of several supervised learning classes of
data-intensive methodologies for the improved assessment of OpRisk
against current \emph{traditional} statistical estimation techniques.
Three different machine learning techniques viz., DTs, RFs, and ANNs,
are employed to approximate weights of input features (the risk factors)
of the model. A comprehensive list of user defined input variables with
associated root causes contribute to the \emph{frequency} of OpRisk
events of the underlying value-adding processes. Moreover, the
\emph{severity} of OpRisk is also borne out through loss impacts in the
dataset . As a consequence of theses new mwthodologies, capital
estimates should be able to adapt to changes in the risk profile of the
bank, i.e.~upon the addition of new products or varying the business mix
of the bank providing sufficient incentives for ORMF to mitigate risk
(Einemann et al., 2018).

\subsection{Interpretation Issues using cluster analysis}

To identify potential flaws in the mathematical framework for the loss
distribution approach (LDA) model of ORM, which is based the derivation
of OpRisk losses based on a risk-neutral measure \(\mathbb{Q}\), by
employing Cluster Analysis (CA). The study addresses weaknesses in the
current \emph{traditional} LDA model framework, by assuming managerial
risk-taking attitudes are more risk averse. More precisely, CA learns
the deep hierarchies of input
features\footnote{A typical approach taken in the literature is to use an unsupervised learning algorithm to train a model of the unlabeled data and then use the results to extract interesting features from the data [@coates2012learning]}
that constitute OpRisk event \emph{frequencies} \& \emph{severities} of
losses during banking operations. In theory, a risk manager who
experiences persistent/excessive losses due to particular risk events,
would over-compensate cover for these particular risk types. This would
show in reduced losses in those loss event types over time, subsequently
determining whether risk adverse techniques over-compensate for
persistent losses.

\textbf{\section{Significance of the study}}
\label{sec:Significance of the study}

This study fills a gap in that advancing OpRisk VaR measurement methods
beyond simplistic and traditional techniques, new data-intensive
techniques offer an important tool for ORMFs and at the same time
supporting the call from industry for a new class of EBOR models that
capture forward-looking aspects of ORM (Embrechts, Mizgier, and Chen,
2018). The current \emph{traditional} approach consists of a loss data
collection exercise (LDCE) which suffers from inadequate technologies at
times relying on spreadsheets and manual controls to pull numbers
together, and therefore do not support the use of data intensive
techniques for the management of financial risks. In this study, a new
dataset with unique feature characteristics is developed using an
automated LDCE, as defined by Committee and others (2011) for internal
data. The dataset in question is at the level of individual loss events,
it is fundamental as part of the study to know when they happened, and
be able to identify the root causes of losses arising from which OpRisk
loss events.\medskip 

This study will provide guidance on combining various supervised
learning techniques with extreme value theory (EVT) fitting, which is
very much based on the Dynamic EVT-POT model developed by
Chavez-Demoulin, Embrechts, and Hofert (2016). This can only happen due
to an abundance of larger and better quality datasets and which also
benefits the loss distribution approach (LDA) and other areas of OpRisk
modeling. In Chavez-Demoulin et al. (2016), they consider dynamic models
based on covariates and in particular concentrate on the influence of
internal root causes that prove to be useful from the proposed
methodology. Moreover, EBOR models are important due to wide
applicability beyond capital calculation and the potential to evolve
into an important tool for auditing process and early detection of
potential losses, culminating in structural and operational changes in
the FI, hence releasing human capital to focus on dilemmas that require
human judgement.

\textbf{\section{Organisation of the study}}
\label{sec:Organisation of the study}

This study is made up of seven chapters. The introductory chapter is to
the purpose, overview, research problem \& objectives, and the
significance of the study. The introductory chapter is succeded by a
general literaty review chapter (two) followed by three stand alone
chapters each focusing on the three research objectives regarding the
issues in OpRisk capital requirement estimation.\medskip

Chapter one begins with an account of significance and a commentary on
the nature and scope of the practical problem. It then provides a
background of current issues when dealing with OpRisk measurement, the
research problem and research questions thereof. Chapter two gives an
overview of the literature concerning the LDA, an AMA technique used in
the generation of \texttt{OpVaR}. It concludes by proposing the a
research methodology in which a combination of ML techniques and
statistical theory underlying ORMF's would benefit measurement of
capital requirements for OpVaR.\medskip

Chapter three looks at the methodological and empirical determinants of
OpRisk measurement. It explores the different dataset\ldots{}

\FloatBarrier

\newpage

\fancyhead[L]{Literature Review} \fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{LITERATURE REVIEW}

\doublespacing

\textbf{\section{Introduction}} \label{sec2:Introduction}

A look into literary sources for OpRisk indicates (Acharyya, 2012) that
there is insufficient academic literature that looks to characterize its
theoretical roots, as it is a relatively new discipline, choosing
instead to focus on proposing a solution to the quantification of
OpRisk. This chapter seeks to provide an overview of some of the
antecedents of OpRisk measurement and management in the banking
industry. As such, this chapter provides a discussion on why OpRisk is
not trivial to quantify and attempts to understand its properties in the
context of risk aversion with the thinking of practitioners and
academics in this field.\medskip

According to Cruz (2002), FI's wish to measure the impact of operational
events upon profit and loss (P\&L), these events depict the idea of
explaining the \emph{volatility of earnings} due to OpRisk data points
which are directly observed and recorded. By seeking to incorporate data
intensive statistical approaches to help understand the data, the
framework analyses response variables that are decidedly non-normal
(including categorical outcomes and discrete counts) which can shed
further light on the understanding of firm-level OpRisk RC. Lastly, a
synopsis of gaps in the literature is presented.

\textbf{\section{The theoretical foundation of OpRisk}}
\label{sec:The theoretical foundation of OpRisk}

Hemrit and Arab (2012) argue that common and systematic operational
errors in hypothetical situations poses presumtive evidence that OpRisk
events, assuming that the subjects have no reason to disguise their
preferences, are created sub-consciously. This study purports, supported
by experimental evidence, behavioural finance theories should take some
of this behaviour into account in trying to explain, in the context of a
model, how investors maximise a specific utility/value function.\medskip

Furthermore its argued by integrating OpRisk management into behavioral
finance
theory,\footnote{In behavioral finance, we investigate whether certain financial phenomena are the result of less than fully rational thinking [@barberis2003survey]},
that it may be possible to improve our understanding of firm level RC by
refining the resulting OpRisk models to account for these behavioral
traits - implying that people's economic preferences described in the
model, have an economic incentive to improve the OpRisk RC measure.
\medskip

Wiseman and Catanach Jr (1997) suggest that managerial risk-taking
attitudes are influenced by the decision (performance) context in which
they are taken. In essence, managerial risk-taking attitude is
considered as a proxy for measuring OpRisk (Acharyya, 2012). In so
doing, Wiseman and Catanach Jr (1997) investigate more comprehensive
economic theories, viz. prospect theory and the behavioural theory of
the firm, that prove relevant to complex organizations who present a
more fitting measure for OpRisk.\medskip 

In a theoretical paper, Wiseman and Catanach Jr (1997) discussed several
organizational and behavioural theories, such as PT, which influence
managerial risk-taking attitudes. Their findings demonstrate that
behavioural views, such as PT and the behavioural theory of the firm
explain risk seeking and risk averse behaviour in the context of OpRisk
even after agency based influences are controlled for. Furthermore, they
challenge arguments that behavioral influences are masking underlying
root causes due to agency effects. Instead they argue for mixing
behavioral models with agency based views to obtain more complete
explanations of risk preferences and risk taking behavior (Wiseman and
Catanach Jr, 1997). \medskip 

Despite the reality that OpRisk does not lend itself to scientific
analysis in the way that market risk and credit risk do, someone must do
the analysis, value the RC measurement and hope the market reflects
this. Besides, financial markets are not objectively scientific, a large
percentage of successful people have been lucky in their forecasts, it
is not an area which lends itself to scientific analysis.

\textbf{\section{Overview of operational risk management}}
\label{sec:Overview of operational risk management}

It is important to note how OpRisk manifests itself: King (2001) has
established the causes and sources of operational loss events as
observed phenomena associated with operational errors and are wide
ranging. By definition, the occurence of a loss event is due to P\&L
volatitlity from a payment, settlement or a negative court ruling within
the capital horizon over a time period (of usually one year) (Einemann
et al., 2018). As such, P\&L volatitlity is not only related to the way
firms finance their business, but also in the way they
\emph{operate}.\medskip 

In operating practice, one assumes that on observing or on following
instructions we are consciously analysing and accurately executing our
tasks based on the information. However, the occurence of operational
loss events indicates that there are sub-concious faults in information
processing, which we are not consciously aware of. These operational
loss events are almost always initiated at the dealing phase of a
trading process, which more often than not implicates front office (FO)
personnel to bear the responsibility for the losses e.g., during the
trading process in cases where OpRisk events occur as a result of a
mismatch between the trade booked (booking in trade feed) and the
details agreed by the trader. The middle office (MO) and back offices
(BO) conduct the OpRisk management, who undertake a broad view of P\&L
attribution carried out from deal origination to settlement within the
perspective of strategic management, and detects the interrelationships
between OpRisk factors with others to conceptualise the potential
overall consequences (Acharyya, 2012) e.g., in the afore-mentioned
example, human error (a sub-conscious phenomenon) is usually quoted as
the source of error, and the trade is fixed by \lq\lq amending\rq\rq~or
manually changing the trade details. \medskip

Furthermore, Acharyya (2012) recognised that organizations may hold
OpRisk due to external causes, such as failure of third parties or
vendors (either intentionally or unintentionally), in maintaining
promises or contracts. The criticism in the literature is that no amount
of capital is realistically reliable for the determination of RC as a
buffer to OpRisk, particularly the effectiveness of the approach of
capital adequacy from external events, as there is effectively no
control over them.\medskip

\section{The loss collection data exercise (LCDE)}
\label{sec:The loss collection data exercise (LCDE)}

The main challenge in OpRisk modeling is in poor loss data quantities,
and low data quality. There are usually very few data points and are
often characterised by high frequency low severity (HFLS) and low
frequency high severity (LFHS) losses. It is common knowledge that HFLS
losses at the lower end of the spectrum tend to be ignored and are
therefore less likely to be reported, whereas low frequency high
severity losses (LFHS) are well guarded, and therefore not very likely
to be made public.\medskip

In this study, a new dataset with unique feature characteristics is
developed using the official loss data collection exercise (LDCE), as
defined by Committee and others (2011) for internal data. The dataset in
question is at the level of individual loss events, it is fundamental as
part of the study to know when they happened, and be able to identify
the root causes of losses arising from which OpRisk loss events.\medskip

The LCDE is carried out drawing statistics directly from the trade
generation and settlement system, which consists of a tractable set of
documented trade detail extracted at the most granular level, i.e.~on a
trade-by-trade basis {[}as per number of events (frequencies) and
associated losses (severities){]}, and then aggregated daily. The
dataset is split into proportions and trained, validated and tested. The
afore-mentioned LDCE, is an improved reflection of the risk factors by
singling out the value-adding processes associated with individual
losses, on a trade-by-trade level.

\textbf{\section{Current operational risk measurement modeling framework}}
\label{sec:Current operational risk measurement modeling framework}

Historical severity curves obtained from historical loss counts have
been widely considered to be the most reliable models when used in
OpRisk loss estimation. However they have not been successfull when used
as measures capturing forward-looking aspects of the OpRisk loss
prediction problem.\medskip 

In this paper, we develop data intensive analysis techniques which yield
a more realistic estimation for underlying risk factors, through linking
risk factors to covariates based on internal control vulnerabilities
(ICV's). ICV's are selected as measures of trading risk exposure,
business environment and internal control factors (BEICF's) i.e., trade
characteristics and causal factors. For each loss event, information
such as unique trade identifier, trader identification, loss event
capture personnel, trade status and instrument type, loss event
description, loss amount, market variables, trading desk and business
line, beginning and ending date and time of the event, and settlement
time are given.\medskip  

AMA's allow banks to use their internally generated risk estimates Under
Basel II; a first attempt internal measurement approach (IMA) capital
charge calculation for OpRisk (i.e. ) is similar to the Basel II model
for credit risk, where a loss event is a default in the credit risk
jargon. There are generally seven event type categories (Risk, 2001) and
eight business lines. Potential losses are decomposed into several
(\(7 \times 8 = 56\)) sub-risks using event types and business line
combinations: e.g., execution, delivery \& process management is one
such category defined the risk that operational losses/problems would
take place in the banks transactions, given as:

\begin{eqnarray}
\mathcal{\Large{C}}_{OpRisk}^{IMA} &=& \sum_{i=1}^8 \sum_{k=1}^7 \gamma_{ik}\epsilon_{ik} \\
where \quad \epsilon_{ik}&:& \quad \mbox{expected loss for business line $i$, risk type $k$} \nonumber \\
      \gamma_{ik}&:& \quad \mbox{scaling factor} \nonumber
\end{eqnarray}

\subsection{The business line/ event type (BL/ET) matrix}

The 3-dimensional diagram, Figure \ref{BL/ET Matrix} depicts the
formation of the \(BL/ET\) matrix: Duration (time \(T+\tau\)) is
represented along the depth ordinate.

\begin{figure}
\begin{tikzpicture}[every node/.style={minimum size=0.5cm},on grid]
\begin{scope}[every node/.append style={yslant=-0.5},yslant=-0.5]
  \shade[right color=gray!10, left color=black!50](0,0) rectangle +(8,8);
  \node at (0.5,7.5) {$\textbf{\mbox{BL}1}$};
  \node at (0.5,6.5) {$\color{green}{{X^l}_{11}}$};   
  \node at (1.5,7.5) {$\textbf{\mbox{BL}2}$};
  \node at (2.5,7.5) {$\textbf{\mbox{BL}3}$};
  \node at (3.5,7.5) {$\textbf{\mbox{BL}4}$};
  \node at (4.5,7.5) {$\textbf{\mbox{BL}5}$};
  \node at (5.5,7.5) {$\textbf{\mbox{BL}6}$};
  \node at (6.5,7.5) {$\textbf{\mbox{BL}7}$};
  \node at (7.5,7.5) {$\textbf{\mbox{BL}8}$};
  \node at (7.5,6.5) {$\color{blue}{{X^l}_{18}}$};  
  \draw (0,0) grid (8,8);
\end{scope}   
\begin{scope}[every node/.append style={yslant=0.5},yslant=0.5]  
  \shade[right color=gray!70,left color=gray!10](8,-8) rectangle +(8,8);
  \node at (11.5,-0.5) {$\mbox{T}+\tau \quad \Huge{\color{magenta}\xrightarrow{\hspace*{4cm}}}$};
  \node at (8.5,-1.5) {\textbf{EL1}};
  \node at (8.5,-2.5) {\textbf{EL2}};
  \node at (8.5,-3.5) {\textbf{EL3}};
  \node at (8.5,-4.5) {\textbf{EL4}};
  \node at (8.5,-5.5) {\textbf{EL5}};
  \node at (8.5,-6.5) {\textbf{EL6}};
  \node at (8.5,-7.5) {\textbf{EL7}};
  \node at (10.5,-1.5) {Internal Fraud};
  \node at (10.5,-2.5) {External Fraud};
  \node at (12.5,-3.5) {Employment Practices and Workplace Safety};
  \node at (12.0,-4.5) {Client Products and Business Practices};
  \node at (11.5,-5.5) {Damage to Physical Assets};
  \node at (12.0,-6.5) {Business Disruption and System Failure};
  \node at (12.5,-7.5) {Execution, Delivery and Process Management};
\end{scope}   
\begin{scope}[every node/.append style={yslant=0.5,xslant=-1},yslant=0.5,xslant=-1]
  \node at (10.0,7.5) {Corporate Finance};
  \node at (10.0,6.5) {Trading and Sales};
  \node at (10.0,5.5) {Retail Banking};
  \node at (10.0,4.5) {Commercial Banking};
  \node at (10.0,3.5) {Payment and Settlement};
  \node at (10.0,2.5) {Agency Services};
  \node at (10.0,1.5) {Asset Management};
  \node at (10.0,0.5) {Retail Brokerage};
\end{scope}  
\end{tikzpicture}
\label{BL/ET Matrix}
\caption{The 3-Dimensional grid of the BL/ET matrix for 7 event types and 8 business lines}
\end{figure}

\textbf{\section{Loss Distribution Approach (LDA)}}
\label{sec:Loss Distribution Approach (LDA)}

The Loss Distribution Approach (LDA) is an AMA method whose main
objective is to provide realistic estimates to calculate VaR for OpRisk
RC in the banking sector and it's business units based on loss
distributions that accurately reflect the frequency and severity loss
distributions of the underlying data. Having calculated separately the
frequency and severity distributions, we need to combine them into one
aggregate loss distribution that allows us to produce a value for the
OpRisk VaR. \medskip

We begin by defining some concepts:

\begin{itemize}
\item In line with Basel II, and according to @frachot2001loss, we consider a matrix consisting of business lines $BL$ and (operational) event types $ET$. The bank estimates, for each business line/event type (BL/ET) cell, the probability functions of the single event impact and the event frequency for the next three months. More precisely, in each cell of the BL/ET matrix separate distributions for loss frequency and severity are modeled and aggregated to a loss distribution at the group level. The aggregated operational losses can be seen as a sum $S$ of a random number $N$ of individual operational losses \begin{math} (X_1, \ldots, X_N )\end{math}. This sum can be represented by:

\singlespacing
\begin{equation}\label{eqn1}
S = X_1, \ldots, X_N ,\quad N = 1, 2, \ldots 
\end{equation}
\doublespacing

\item Three month daily statistics are taken of the time series of internal processing errors (frequency data) and their associated severities and used in each cell of the BL/ET matrix. Frequency refers to the number of events that occur within the specified time period (daily buckets) $T$ and $T + \tau$ and severity refers to the P\&L impact resulting from the frequency of events. The time (1 day bucket) period is chosen in order to ensure that the number of data points is sufficient for statistical analysis.
\end{itemize}

\subsection{Computing the frequency distribution}

\begin{itemize}
\item Let $\mathbf{N}_{ij}$ be variable in random selection, representing \textbf{the number of times of process risk event failures} between times $T$ \& $T +\tau$. Suppose subscript $i$ refers to the $BL$ which ranges from \begin{math} 1, \ldots, k \end{math} and subscript $j$ to $ET$ ($j=1$ for process risk). We have taken a random sample implying that the observations \begin{math} N_{ij}\end{math}, {where} \begin{math}{i,j}= (1,1)\,,\;\ldots, (k,1)\end{math} are independent and identically distributed (i.i.d). 

\item The random variable $N_{i1}$\footnote{$N_{ij}$ \, where subscript $j=1$ since we are only dealing with $1$ event type i.e. process risk} has distribution function\footnote{The term distribution function is monotonic increasing function of $n$ which tends to $0$ as \begin{math} n \longrightarrow -\infty\end{math}, and to $1$ as \begin{math} n \longrightarrow \infty \end{math}} The random variable has distribution function (d.f.) \begin{math}\mathbf{P}_{i1}(n/\theta_0)  \end{math}, where $\theta_0$ is an unknown parameter of the estimated distribution.  The unknown parameter $\theta_0$ may be a scalar or a vector quantity \begin{math}\mathbf{\theta_0}\end{math}, for example, The Poisson distribution depends on one parameter called $\lambda$ whereas the univariate normal distribution depends on two parameters, $\mu$ and $\sigma ^2$, the mean and variance.  These parameters are to be estimated in some way. We use the Maximum Likelihood Estimate (m.l.e) which is that value of $\theta$ that makes the observed data \lq\lq most probable\rq\rq or \lq\lq most likely\rq\rq.\medskip

\item The d.f. \begin{math}\mathbf{P}_{i1}(n/\theta_0)  \end{math}, is the probability that $N_{i1}$ takes a value less than or equal to $n$, where $n$ is a small sample from the entire population of observed frequencies, i.e.
\singlespacing
\begin{equation}\label{eqn2}
\mathbf{P}_{ij}(n)=Pr \left(N_{ij}\leq n \right) \quad{i,j}= (1,1),\ldots, (k,1)
\end{equation}
\doublespacing

\item The probability density function (p.d.f) : A density function is a non--negative function $p(n)$ whose integral, extended over the entire $x$ axis, is equal to $1$ for a given continuous random variable $X$. i.e. it is the area under the probability density curve, of the discrete random variable $N_{i1}$ takes discrete values of $n$ with finite probabilities. In the discrete case the term for p.d.f. is the probability function (p.f.) also called the probability mass function, i.e. $N_{i1}$ is given by the probability that the variable takes the value $n$, i.e.

\singlespacing
\begin{equation}\label{eqn3}
p_{ij}(n)=Pr\left(N_{ij} = n\right), \quad{i,j}= (1,1),\ldots, (k,1) 
\end{equation} 
\doublespacing

\item The r.h.s of equation~(\ref{eqn2}) is the summation of the r.h.s of equation~(\ref{eqn3}), we derive a relation for the \textbf{loss frequency distribution} in terms of the (p.f): 

\singlespacing
\begin{equation}\label{eqn4} 
\mathbf{P}_{ij}(n)=\sum_{k=1}^{n_k} p_{ij}(n) \quad{i,j}= (1,1),\ldots, (k,1)
\end{equation}
\doublespacing

\end{itemize}

\subsection{Computing the severity distribution}

\begin{itemize}
\item Suppose $X_{ij}$ is a random variable representing \textbf{the amount of one loss event} in a cell of the BL/ET matrix. Define next period's loss in each cell $(i,j)$, where $i$ is the number of business line cells, \begin{math} {{L}^{T+1}}_{i,j}\end{math}: Operational loss for loss type $j=1$ (process risk). One models the amount of the total operational loss of type $j$ at a given time $T$ \& $T + 1$, over the future (say 3 months), as:

\singlespacing
\begin{equation}\label{eqn5}
{L}^{T+1}=\sum_{i=1}^{k}{L}^{T+1}_{i1}=\sum_{i=1}^{2}\sum_{l=1}^{{{N}_{i1}}^{T+1}}{{X}^{l}}_{i1} \quad l=1,2,\ldots, N_{i1} 
\end{equation}
\doublespacing

\item Let $ N_1, N_2,...,N_m $ (where $m$ in the number of combinations in the BL/ET matrix) be random variables that represent the loss frequencies. It is usually assumed that the random variables $X_{i1}$ are independently distributed and independent of the number of events $N_{m}$. A fixed number of a particular loss type would be denoted by ${{X}^{1}}_{i1}$, i.e the random variable \begin{math}{{X}^{l}}_{i1}\end{math}, represents random samples of the severity distribution [@aue2006lda].\medskip

The \textbf{loss severity distribution} is denoted by \begin{math}\mathbf{F}_{i1}\end{math}. Since loss severity variate $X$ is continuous (i.e. can take on any real value), we define a level of precision $\emph{h}$ such that the probability of $X$ being within $\pm\emph{h}$ of a given number $x$ tends to zero. The loss severity, $X_{i1}$ has a (d.f.) \begin{math}\mathbf{F}_{i1}(x/\theta_1) \end{math}, where $\theta_1$ is an unknown parameter and $x$ is a small sample from the entire population of loss severity.

\item We define probability density in the continuous case as follows:

\singlespacing
\begin{eqnarray}
f_{X}(x) &=& \lim_{h\rightarrow 0}\frac{Pr[x < X \leq x + h]}{h}\nonumber\\
&=& \lim_{h\rightarrow 0}\frac{F_{X}(x + h) - F_{X}(x)}{h}\nonumber\\
&=&\frac{dF_{X}(x)}{dx} \label{eqn6a}
\end{eqnarray}
\doublespacing

operate with $\int\,dx$ on both sides of \ref{eqn6a}

\singlespacing
\begin{equation}
\mathbf{F}_{X_{ij}}(x)=\int_{k=1}^{\infty} f_{X_{ij}}(x)dx \quad{i,j}= (1,1),\ldots, (k,1)\label{eqn6b}
\end{equation}
\doublespacing

where $f_{X_{ij}}(x)$ is the probability density function (p.d.f.). Once again, the subscript $X$ identifies the random variable for severity (P\&L impact) of one loss event while the argument $x$ is an arbitrary sample of the severity events.
\end{itemize}

\subsection{Formal Results}

Having calculated both the frequency and severity process we need now to
combine them in one aggregate loss distribution that allows us to
predict an amount for the operational losses to a degree of confidence.
There is no simple way of aggregating the frequency and severity
distribution. Numerical approximation techniques (computer algorithms)
successfully bridge the divide between theory and implementation for the
problems of mathematical analysis.\medskip

The aggregated losses at time \(t\) are given by
\(\vartheta(t) = \sum_{n=1}^{N(t)} X_{n}\) (where X represents
individual operational losses). Frequency and severity distributions are
estimated, e.g., the poisson distribution is a representation of a
discrete variable commonly used to model operational event frequency
(counts), and a selection from continuous distributions which can be
linear (e.g.~gamma distribution) or non-linear (e.g.~lognormal
distribution) for operational loss severity amounts. The compound loss
distribution \(\mathbf{G}(t)\) can now be derived. Taking the aggregated
losses we obtain:

\singlespacing 

\begin{equation}\label{Compound_losses}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}

\doublespacing

For most choices of \(N(t)\) and \(X_{n}\), the derivation of an
explicit formula for \(\mathbf{G}_{\vartheta(t)}(x)\) is, in most cases
impossible. \(\mathbf{G}(t)\) can only be obtained numerically using the
Monte Carlo method, Panjer's recursive approach, and the inverse of the
characteristic function {[}Frachot, Georges, and Roncalli (2001); Aue
and Kalkbrener (2006); Panjer (2006); \& others{]}. \medskip

\begin{itemize}
\item We now introduce the aggregate loss variable at time $t$ given by $\vartheta(t)$. This new variable represents \textbf{the loss for business line $i$ and event type $j$}. The aggregate loss is defined by \begin{math} \vartheta(t) = \sum_{n=1}^{N(t)} X_{n} \end{math} (where X represents individual operational losses). Once frequency and severity distributions are estimated, the compound loss distribution \begin{math} \mathbf{G}(t)\end{math} can be derived.  Taking the aggregated losses we obtain:

\singlespacing
\begin{equation}\label{eqn6}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}
\doublespacing

\item The derivation of an explicit formula for \begin{math}\mathbf{G}_{\vartheta(t)}(x) \end{math} is, in most cases impossible. Again we implicitly assume that the processes \{$N(t)$\} and $\{X_{n}\}$ are independent and identically distributed (i.i.d).  Deriving the analytical expression for \begin{math}\mathbf{G}_{\vartheta(t)}(x) \end{math}, we see a fundamental relation corroborated by @frachot2001loss, @cruz2002modeling, @embrechts2013modelling, \& others:

\singlespacing
\begin{equation}\label{eqn7}
\mathbf {G}_{\vartheta(t)}(x)=\left\{\begin{array}{rcl}
                 &\sum_{n,k=0,1}^{\infty} p_{k}(n)\mathbf{F}_{X}^{k\star}(x) &x>0\\ &p_{k}(0) &x=0
                 \end{array}\right\}
\end{equation}
\doublespacing

where $\star $ is the \emph{convolution} operator on d.f.'s, \begin{math}\mathbf{F}^{k\star} \end{math} is the k-fold convolution of \begin{math}\mathbf{F} \end{math} with itself. The convolution of two functions $f(x)$ and $g(x)$ is the function
\singlespacing
\begin{equation}
\int_{0}^{x}f(t)g(x-t)dt
\end{equation}
\doublespacing , i.e. \begin{math} \mathbf{F}_{X}^{k\star}(x)=Pr(X_1 + \ldots + X_k \leq x) \end{math}, the d.f. of the sum of $k$ independent random variables with the same distribution as $X$. 

\item The aggregate loss distribution \begin{math} \mathbf {G}_{\vartheta(t)}(x) \end{math} cannot be represented in analytic form, hence approximations, expansions, recursions of numerical algorithms are proposed to overcome this problem.  For purposes of our study, an approximation method will do. One such method consists of taking a set \begin{math} \langle \vartheta_1, \ldots , \vartheta_s \rangle \end{math}, otherwise known as the ideal generated by elements \begin{math} \vartheta_1, \ldots , \vartheta_s \end{math} which are $s$ simulated values of the random variable $\vartheta_{ij}$  for $s = 1,\ldots, S$ [@fraleigh2003first].\medskip

This method is popularly known as Monte Carlo simulation coined by physicists in the 1940's, it derives its name and afore--mentioned popularity to its similarities to games of chance. The way it works in layman's terms is; in place of simulating scenario's based on a base case, any possible scenario through the use of a probability distribution (not just a fixed value) is used to simulate a model many times. In the LDA separate distributions of frequency and severity are derived from loss data then combined by Monte Carlo simulation. 
\end{itemize}

\subsection{Dependence Effects (Copulae)}

The standard assumption in the LDA is that frequency and severity
distributions in a cell are independent and the severity samples are
i.i.d. According to Basel II, dependence effects in OpRisk are not
considered. Economic capital allocation however, could benefit if it
were determined in a way that recognises the risk-reducing impact of
correlation effects between the risks of the BL/ET combinations.
Concluding remarks from a study by Urbina and Guillén (2014) allude that
failure to account for correlation may lead to risk management practices
that are unfair, as evidenced in an example using data from the banking
sector. \medskip

One of the main issues we are confronted with in OpRisk measurement is
the aggregation of individual risks (in each BL/ET element). A powerful
concept to aggregate the risks -- the \emph{copula} function -- has been
introduced in finance by Embrechts, McNeil, and Straumann (2002).
Copulas have been used extensively in finance theory lately and are
sometimes held accountable for recent global financial failures,
e.g.~the global credit crunch of 2008 - 2009. They are nevertheless
still applicable and in use for OpRisk as operational risk models follow
a different stochastic process to other areas of risk, e.g.~operational
VaR is subject to more jumps than market VaR and is thought to be
discrete whereby market VaR is continuous. \medskip

Copulas are functions which conveniently incorporate correlation into a
function that combines each of the frequency (marginal) distributions to
produce a single bivariate cumulative distribution function. Our model
is used to determine the aggregate (bivariate) distribution of a number
of correlated random variables through the use a Clayton copula.
Dependence matters due to the effect of the addition of risk measures
over different risk classes (cells in the BL/ET matrix). \medskip

More precisely, the frequency distributions of the individual cells of
the BL/ET matrix are correlated through a Clayton copula in order to
replicate observed correlations in the observed data. Let \(m\) be the
number of cells, \(\mathbf{G_1}, \mathbf{G_2},...,\mathbf{G_m}\) the
distribution functions of the frequency distributions in the individual
cells and \(\mathbf{C}\) the so--called copula. Abe Sklar proved in 1959
through his theorem (Sklar's Theorem) that for any joint distribution
\(\mathbf{G}\) the copula \(\mathbf{C}\) is unique. \(\mathbf{C}\) is a
distribution function on \([0,1]^{m}\) with uniform marginals. We refer
to a recent article by Chavez-Demoulin, Embrechts, and Nešlehová (2006)
for further information: It is sufficient to note that \(\mathbf{C}\) is
unique if the marginal distributions are continuous.

\singlespacing

\begin{equation}\label{eqn7a}
\mathbf{G}(x_1, \ldots, x_m) = \mathbf{C}\left(\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m) \right)
\end{equation}

\doublespacing

Conversely, for any copula \(\mathbf{C}\) and any distribution functions
\(\mathbf{G_1}, \mathbf{G_2},...,\mathbf{G_m}\), the functions
\(\mathbf{C}\left(\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m) \right)\)
is a joint distribution function with marginals
\(\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m)\). Moreover, combining
given marginals with a chosen copula through Equation \ref{eqn7a} always
yields a multivariate distribution with those marginals. The copula
function has then a great influence on the aggregation of risk.

\textbf{\section{LDA model shortcomings}}
\label{ssec:LDA model shortcomings}

After most complex banks adopted the LDA for accounting for RC,
significant biases and delimitations in loss data remain when trying to
attribute capital requirements to OpRisk losses (Frachot et al., 2001).
OpRisk is related to the internal processes of the FI, hence the quality
and quantity of internal data (optimally combined with external data)
are of greater concern as the available data could be rare and/or of
poor quality. Such expositions are unsatisfactory if OpRisk, as Cruz
(2002) professes, represents the next frontier in reducing the riskiness
associated with earnings.

Opdyke (2014) advanced studies intending on eliminating bias apparently
due to heavy tailed distributions to further provide insight on new
techniques to deal with the issues that arise in LDA modeling, keeping
practitioners and academics at breadth with latest research in OpRisk
VaR theory. Recent work in LDA modeling has been found wanting (Badescu,
Lan, Lin, and Tang, 2015), due to the very complex characteristics of
data sets in OpRisk VaR modeling, and even when studies used quality
data and adequate historical data points, as pointed out in a recent
paper by Hoohlo (2015), there is a qualitative aspect in OpRisk modeling
that is often ignored, but whose validity should not be overlooked.
\medskip

Opdyke (2014), Agostini et al. (2010), Jongh, De Wet, Raubenheimer, and
Venter (2015), Galloppo and Previati (2014), and others explicate how
greater accuracy, precision and robustness uphold a valid and reliable
estimate for OpRisk capital as defined by Basel II/III. Transforming
this basic knowledge into \lq\lq risk culture\rq\rq~or firm-wide
knowledge for the effective management of OpRisk, serves as a starting
point for a control function providing attribution and accounting
support within a framework, methodology and theory for understanding
OpRisk measurement. FI's are beginning to implement sophisticated risk
management systems similar to those for market and credit risk, linking
theories which govern how these risk types are controlled to theories
that govern financial losses resulting from OpRisk events. \medskip

Jongh et al. (2015) and Galloppo and Previati (2014) sought to address
the shortcomings of Frachot et al. (2001) by finding possible ways to
improve the problems of bias and data delimitation in operational risk
management. They follow the recent literature in finding a
statistical-based model for integrating internal data and external data
as well as scenario assessments in on endeavor to improve on accuracy of
the capital estimate.

\textbf{\section{A new class of models capturing forward-looking aspects}}
\label{sec:A new class of models capturing forward-looking aspects}

Agostini et al. (2010) also argued that banks should adopt an integrated
model by combining a forward-looking component (scenario analysis) to
the historical operational VaR, further adding to the literature through
their integration model which is based on the idea of estimating the
parameters of the historical and subjective distributions and then
combining them by using the advanced CT. \medskip

The idea at the basis of CT is that a better estimation of the OpRisk
measure can be obtained by combining the two sources of information: The
historical loss data and expert's judgements, advocating for the
combined use of both experiences. Agostini et al. (2010) seek to explain
through a weight called the credibility, the amount of credence given to
two components (historical and subjective) determined by statistical
uncertainty of information sources, as opposed to a weighted average
approach chosen on the basis of qualitative judgements.\medskip

Thus generating a more predictable and forward looking capital estimate.
He deemed the integration method as advantageous as it is self contained
and independent of any arbitrary choice in the weight of the historical
or subjective components of the model.

\textbf{\section{Applicability of EBOR methodology for capturing forward-looking aspects of ORM}}
\label{sec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM}

Einemann et al. (2018), in a theoretical paper, construct a mathematical
framework for an EBOR model to quantify OpRisk for a portfolio of
pending litigations. Their work unearths an invaluable contribution to
the literature, discussing a strategy on how to integrate EBOR and LDA
models by building hybrid frameworks which facilitate the migration of
OpRisk types from a classical to an exposure-based treatment through a
quantitative framework, capturing forward looking aspects of BEICF's
(Einemann et al., 2018).\medskip

The fundamental premise of the tricky nature behind ORMF, is to provide
an exposure-based treatment of OpRisk losses which caters to modeling
capital estimates for forward-looking aspects of ORM due to the lag in
the loss data. By the very nature of OpRisk, there is usually a
significant lag between the moment the OpRisk event is conceived to the
moment the event is observed and accounted.i.e., there is a gap in time
between the moment the risk is conceived and the realised losses. This
timing paradox often results in questionable capital estimates,
especially for those near misses, pending and realised losses that need
to be captured in the model.\medskip

\subsection{Definition of exposure}
\label{ssec:Definition of exposure}

Exposure is residual risk, or the risk that remains after risk
treatments have been applied. In the ORMF context, it is defined as:

\begin{definition}
The  \textbf{exposure} of risk type $i$, $d_{i}$ is the time interval, expressed in units of time, from the initial moment when the event happened, until the occurrence of a risk correction.
\end{definition}

\subsection{Definition of rate}
\label{ssec:Definition of rate}

The \textbf{rate}, \(R\) is defined as:\medskip

\begin{definition}
the \textbf{rate} is the mean count per unit exposure
\end{definition}

i.e.,

\singlespacing

\begin{eqnarray}
R &=& \frac{\mu}{\tau} \qquad \mbox{where} \qquad R = \mbox{rate,} \quad \tau = \mbox{exposure},d_{i}\quad \mbox{and}\nonumber\\
\mu &=& \mbox{mean count over an exposure duration of} \quad T+\tau \nonumber
\end{eqnarray}

\doublespacing

\textbf{\section{Intepretation}} \label{sec:Intepretation}

In turn, with reference to
\ref{ssec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
the fundamental premise behind the LDA is that each firm's OpRisk losses
are a reflection of it's underlying Oprisk exposure. In particular, the
assumption behind the use of the poisson model to estimate the frequency
of losses, is that both the the intensity (or rate) of occurrence and
the opportunity (or exposure) for counting are constant for all
available observations.\medskip

The measure of exposure we need to use depends specifically on
projecting the number of Oprisk event types (frequency of losses) and is
different to the measure if the target variable were the severity of the
losses. We need historical exposure for experience rating because we
need to be able to compare the loss experience of different years on a
like-for-like basis and to adjust it to current exposure levels(Parodi,
2014).\medskip 

When observed counts all have the same exposure, modeling the mean count
\(\mu\) as a function of explanatory variables \(x_{1},\ldots,x_{p}\) is
the same as modeling the rate \(R\).

\textbf{\section{Benefits and Limitations}}
\label{sec:Benefits and Limitations}

These approaches in
\ref{sec:Current operational risk measurement modeling framework}, were
found to have significant advantages over conventional LDA methods,
proposing that an optimal mix of the two modeling elements could more
accurately predict OpRisk VaR over traditional methods. Particularly
Agostini et al. (2010), whose integration model represents a benchmark
in OpRisk measurement by including a component in the AMA model that is
not obtained by a direct average of historical and subjective
VaR.\medskip

Instead, the basic idea of the integration methodology in
\ref{sec:A new class of models capturing forward-looking aspects} is to
estimate the parameters of the frequency and severity distributions
based on the historical losses and correct them; via a statistical
theory, to include information coming from the scenario analysis. The
method has the advantage of being completely self contained and
independent of any arbitrary choice in the weight of the historical or
subjective component of the model, made by the analyst. The components
weights are derived in an objective and robust way, based on the
statistical uncertainty of information sources, rather than through risk
managers choices based on qualitative motivations. \medskip

However, they could not explain the prerequisite coherence between the
historical and subjective distribution function needed in order for the
model to work; particularly when a number of papers (Chau, 2014),
propose using mixtures of (heavy tailed) distributions commonly used in
the setting of OpRisk capital estimation (Opdyke, 2014).\medskip

In
\ref{sec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
their model (Einemann et al., 2018) is particularly well-suited to the
specific risk type dealt with in their paper i.e., the portfolio of
litigation events, due to better usage of existing information and more
plausible model behavior over the litigation life cycle, but is bound to
under-perform for many other OpRisk event types, since these EBOR models
are typically designed to quantify specific aspects of OpRisk -
litigation risk have rather concentrated risk profiles. However, EBOR
models are important due to wide applicability beyond capital
calculation and its potential to evolve into an important tool for
auditing process and early detection of potential losses.\medskip

\textbf{\section{Gap in the Literature}}
\label{sec:Gap in the Literature}

There is cognitive pressure which seeks to remove information which we
are largely unaware of, because they are undetectable to human senses
that no one could ever see them. We seek to remove this pressure,
effectively lowering uncertainty and allowing us to position ourselves
to develop a defense against our cognitive biases. It is through
patterns in that information that we are largely unaware of that
predictions could arise; or that, OpRisk management incorporates rather
than dismiss the many alternatives that were not imagined, the
possibility of market inefficiencies or finding value in unusual places.

\textbf{\section{Conclusion}} \label{sec:Conclusion}

A substantial body of evidence suggests that loss aversion, the tendency
to be more sensitive to losses than to gains plays an important role in
determining how people evaluate risky gambles. In this paper we evidence
that human choice behavoir can substantially deviate from neoclassical
norms.\medskip

PT takes into account the loss avoidance agents and common attitudes
toward risk or chance that cannot be captured by EUT; which is not
testing for that inherent bias, so as to expect the probability of
making the same operational error in future to be overcompensated for
i.e., If an institution suffers from an OpRisk event and survives, it's
highly unlikely to suffer the same loss in the future because they will
over-provide for particular operational loss due to their natural risk
aversion. This is a testable proposition which fits normal behavioral
patterns and is consistent with risk averse behaviour.

\singlespacing

\FloatBarrier

\newpage

\fancyhead[L]{Exposure-based Operational Risk Analysis}
\fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{EXPOSURE-BASED OPERATIONAL RISK ANALYSIS}

\doublespacing

\textbf{\section{Introduction}} \label{sec3:Introduction}

The fundamental premise in the nature behind ORMFs, is to provide an
exposure-based treatment of OpRisk losses which caters to modeling
capital estimates for forward-looking aspects of ORM. This proves tricky
due to the lag between the time the loss event occurs and the actual
realised loss, i.e.~by the very nature of OpRisk, there is a significant
lag between the moment the OpRisk event is conceived to the moment the
event is observed and accounted for. There is a gap in time between
\(\tau\) the moment the risk is conceived and the time \(\tau+1\) when
impact of the loss is realised. This timing paradox often results in
questionable capital estimates, especially for those near misses,
pending and realised losses that need to be captured in the model

\section{EBOR methodology for capturing forward-looking aspects of ORM}
\label{sec:EBOR methodology for capturing forward-looking aspects of ORM}

Einemann et al. (2018), in a theoretical paper, construct a mathematical
framework for an EBOR model to quantify OpRisk for a portfolio of
pending litigations. Their work unearths an invaluable contribution to
the literature, discussing a strategy on how to integrate EBOR and LDA
models by building hybrid frameworks which facilitate the migration of
OpRisk types from a \emph{classical} to an exposure-based treatment
through a quantitative framework, capturing forward looking aspects of
BEICF's (Einemann et al., 2018), a key source of the OpRisk data.

The measure of exposure we need to use depends specifically on
projecting the number of OpRisk event types (frequency of losses) as the
target variable in the model and is different to the measure if the
target variable were the severity of the losses. As per definition
\ref{ssec:Definition of exposure}, the lag represents exposure; we need
historical exposure for experience rating because we need to be able to
compare the loss experience of different years on a like-for-like basis
and to adjust it to current exposure levels (Parodi, 2014).\medskip 

With reference to the current section, Section
\ref{sec:EBOR methodology for capturing forward-looking aspects of ORM},
the fundamental premise behind the LDA is that each firm's OpRisk losses
are a reflection of it's underlying Oprisk exposure. In particular, the
assumption behind the use of the Poisson distribution in the model to
estimate the frequency of losses, is that both the the intensity (or
rate) of occurrence and the opportunity (or exposure) for counting are
constant for all available observations.\medskip

\subsection{Limitations of the EBOR model}

In their model (Einemann et al., 2018), the definition of exposure,
Definition \ref{ssec:Definition of exposure}, is particularly
well-suited to the specific risk type dealt with in their paper i.e.,
the portfolio of litigation events, due to better usage of existing
information and more plausible model behavior over the litigation life
cycle. However, it is bound to under-perform for many other OpRisk event
types since these EBOR models are typically designed to quantify
specific aspects of OpRisk i.e., litigation risk have rather
concentrated risk profiles. Furthermore, EBOR models are important due
to wide applicability beyond capital calculation and its potential to
evolve into an important tool for auditing process and early detection
of potential losses.

\textbf{\section{Generalised Linear Models (GLM's)}}
\label{sec:Generalised Linear Models}

Operational riskiness in FIs grows as trading transactions grow in
complexity, i.e.~the more complex and numerous trading activity builds
the higher the rate at which new cases of OpRisk events occur.
Therefore, it is likely that the rate of operational hazard may be
increasing exponentially over time. The scientifically interesting
question is whether the data provides any evidence that the increase in
the underlying operational hazard generation is slowing.\medskip

The aforementioned postulate provides a plausible model to start
investigating this question. The model assumes that if \(\mu_i\) is the
(rate) number of expected new OpRisk events per time interval
\([\tau, \tau+1]\) since the beginning of the data, then \(\mu_i\)
increases according to:

\singlespacing

\begin{eqnarray}
\mu_i = \gamma \mbox{exp}({\delta\tau_i}) \nonumber
\end{eqnarray}

\doublespacing

where \(\gamma\) and \(\delta\) are unknown parameters. Taking a log
link turns the model into Generalised Linear Model (GLM) form so that:

\singlespacing

\begin{eqnarray}\label{eqn:simplepoisson}
\mbox{log}(\mu_i) = \mbox{log}(\gamma) + \delta\tau_i = \beta_0 + \tau_i\beta_1
\end{eqnarray}

\doublespacing

Where the LHS is the observed number of new cases over time \(\tau\) and
\(\tau+1\), and the RHS is a linear in the parameters \(\beta_0 =\)
\mbox{log}(\(\gamma\)) and \(\beta_1 = \delta\).\medskip

We define a binary variable LossIndicator, which takes on the value
\(1\) for realised losses, and the value \(0\) for pending losses,
and/or near misses. The choice of the poisson distribution shows the
number of relevant Oprisk event's incidence in a specified time interval
\(\tau\) and \(\tau+1\). A quadratic term (\(\beta_2t_i^2\)) could be
added to the model so that its residual values increase monotonically
with time as with the fitted (modelled) values, which usefully
approximates other situations which may influence the counts adapted to
the poisson case. Amending the RHS of Equation \ref{eqn:simplepoisson}
with the quadratic term so other situations other than the unrestricted
spread of OpRisk hazards are represented yields:

\singlespacing

\begin{eqnarray}\label{eqn:adaptedpoisson}
\mu = d_i\mbox{exp}(\beta_0 + \beta_1\tau_i + \beta_2{\tau_i}^2) 
\end{eqnarray}

\doublespacing

\subsection{GLM for count data}

The LHS of a GLM formula is the model's random component i.e.,
observations of the number of OpRisk transactions over the trading
transaction period in an FI's portfolio; given by the independent random
variables \(y_1, y_2,\ldots, y_n\), not i.i.d (Wood, 2017, Covrig et al.
(2015)). \(Y\) takes a (exponential) family argument, depending on
parameters \(ln\lambda\), where \(\lambda\) which represents the average
frequency of the OpRisk transactions. It is worth distinguishing between
the response data \(y_i\) which is an observation of \(Y\).\medskip

\textbf{\section{A poisson regression operational hazard model}}
\label{sec:A poisson regression operational hazard model}

The target variable (LossIndicator) which shows the number of relevant
Oprisk event's incidence in a specified time interval \(\tau\) and
\(\tau+1\), justifying the choice of poisson distribution as a
reasonable model for is count data. It's probability mass function (pdf)
is:

\singlespacing

\begin{eqnarray}\label{eqn:Poisson}
Y &\sim& \mbox{poisson}(\lambda), \quad f(y;\lambda) = \frac{\lambda^y}{y!}\dot\exp{-y}\\
 &\mbox{where}& \quad y \in  \mathbb{N}, \mbox{and} \quad \lambda > 0 \nonumber
\end{eqnarray}

\doublespacing

the expectation and variance
\(E[Y] = \mbox{VaR}[Y] = \lambda\)\footnote{If you were to guess an independent $Y_i$ from a random sample, the best guess is given by this expression},
are both equal to parameter \(\lambda\) simultaneously. The RHS of
Equation \ref{eqn:Poisson} is the model's systematic component, and it
specifies the linear predictor. It builds on equation
\ref{eqn:adaptedpoisson} with \(p+1\) parameters,
\(\beta = (\beta_0\ldots,\beta_p)^t\), and \(p\) explanatory variables:

\singlespacing

\begin{eqnarray}
\nu = \beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}, \qquad \mbox{where} \quad i = 1,\ldots,n
\end{eqnarray}

\doublespacing

If sample variables \(Y_i \sim \mbox{Poisson}(\lambda_i)\), then
\(\mu = E[Y_i] = \lambda_i\); the link function between the random and
systematic components, viz. a tranformation by the model by some
function \(g()\), which does not change features essential to to
fitting, but rather a scaling in magnitude so that:

\singlespacing

\begin{eqnarray}\label{eqn:linkfcn }
\nu_i &=& g(\lambda_i) = \mbox{ln}\lambda_i, \qquad \mbox{that is} \nonumber \\
\nu &=& \beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}
\end{eqnarray}

\doublespacing

so the mean frequency or otherwise the rate \(R\), will be predicted by
the model\ldots

\singlespacing

\begin{eqnarray}\label{eqn:multmodel}
\lambda_i &=& d_i\mbox{exp}(\beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}) \quad \mbox{or} \nonumber \\
\lambda_i &=& d_i\cdot e^{\beta_0}\cdot e^{\beta_1x_{i1}}\cdot e^{\beta_2x_{i2}} \ldots e^{\beta_px_{ip}}
\end{eqnarray}

\doublespacing

Where \(d_i\) represents the risk exposure for transaction \(i\). Taking
logs on both sides of equation \ref{eqn:multmodel}, the regression model
for the estimation of loss frequency is:

\singlespacing

\begin{eqnarray}
\mbox{ln}\lambda_i =  \mbox{ln}d_i + \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \ldots + \beta_px_{ip}
\end{eqnarray}

\doublespacing

where \(\mbox{ln}d_i\) is the natural log of risk exposure, called the
``offset variable''.

\subsection{Interpretation}

Table \ref{tab_int} presents the various units produced for the various
GLM links.

\begin{table}[tb]
\centering
\caption{The generalized linear model link functions with their associated units of interpretation.} 
\label{tab_int}
\begin{tabular}{lc}
\toprule
Link Function & Target variable Effect \\ 
\midrule
Identity & Original Continuous Unit \\ 
  Log & Original Continuous Unit \\ 
  Logit & Risk \\ 
  Probit & Risk \\ 
  Poisson & Count \\ 
  Gamma & Count \\ 
  Negative Binomial & Count \\ 
\bottomrule
\end{tabular}
\end{table}

The poisson distribution is restrictive when applied to approximate
counts, due to the assumption made about it that the mean and variance
of the number of events are equal. However, in models for count data
where means are low so that the number of zeros and ones in the data is
exessive are well adapted to the poisson case (Wood, 2017). These cases
are characteristic of scenarios in OpRisk other than those modeling
situations when the unchecked spreading of negligent behaviour may
result in an operational hazard. For example, the negative binomial
and/or quasipoisson regression models ascribe to data that exhibits
\emph{overdispersion}, wherein the variance is much larger than the mean
for basic count data, therefore they have been eliminated in this paper.

\textbf{\section{Research Objective 1}} \label{sec:Research Objective 1}

To introduce the generalised additive model for location, scale and
shape (GAMLSS) framework for OpRisk management, that captures exposures
to forward-looking aspects in the OpRisk loss prediction problem, due to
deep hierarchies in the features of covariates in the investment banking
(IB) business environment, and internal control risk factors (BEICF)
thereof.

\textbf{\section{Exploratory data analysis}}
\label{sec:Exploratory data analysis}

The main source of the analysis dataset is primary data, a collection of
internal OpRisk losses for the period between 1 January 2013 and 31st
March 2013 at an investment bank in SA. The method of data generation
and collection is at the level of the individual trade deal, wherein
deal information is drawn directly from the trade generation and and
settlement system (TGSS) and edit detail from attribution reports
generated in middle office profit \& loss (MOPL). The raw source
consists of two separate datasets on a trade-by-trade basis of daily
frequencies (number of events) and associated loss severities.\medskip

The raw frequency data consists of 58,953 observations of 15 variables,
within the dataset there are 50,437 unique trades. The raw severity data
consists of 6,766 observations of 20 variables; within the severity
dataset there are 2,537 unique trades. The intersection between the
frequency and severity datasets consists of 2,330 individual
transactions which represent realised losses, pending and/or near
misses. This dataset is comprised of 3-month risk correction detail, in
the interval between 01 January 2013 and 31 March 2013. \medskip

\begin{table}[ht]
\centering
\caption{The contents of the traded transactions of the associated risk correction events.}
\begin{tabular}{lcc}
\toprule
  & \multicolumn{2}{c}{Storage} \\
Covariate     & Levels   & Type \\ 
\midrule
 Trade       &          & numeric \\
 UpdateTime  &          & numeric \\
 UpdatedDay  &          & numeric \\
 UpdatedTime &          & numeric \\
 TradeTime   &          & numeric \\
 TradedDay   &          & numeric \\
 TradedTime  &          & numeric \\
 Desk        &  10      & categorical \\
 CapturedBy  &  5       & categorical \\
 TradeStatus &  4       & categorical \\
 TraderId    &  7       & categorical \\
 Instrument  &  23      & categorical \\
 Reason      &  19      & categorical \\
 Loss        &          & numeric \\
 EventTypeCategoryLevel & 5  & categorical \\
 BusinessLineLevel      & 8  & categorical \\
 LossIndicator          & 2  & binary \\
 exposure               &    & numeric \\
 \bottomrule
\end{tabular}\label{tab_contents}
\end{table}

Two new variables are derived from the data; a target variable
(LossIndicator) is a binary variable whereupon, a \(1\) signifies a
realised loss, and \(0\) for those pending losses, or near misses. The
\emph{exposure} variable is computed by deducting the time between the
trade amendment (UpdateTime) and the time when the trade was booked
(TradeTime). It is a measure that is meant to be rougly proportional to
the risk of the transaction or a group of transactions. The idea is that
if the exposure (e.g.~the duration of a trade, the number of
allocation(trade splits), etc.) doubles whilst everything else (e.g.~the
rate, nominal of the splits, and others) remains the same, then the risk
also doubles.

In R, the GLM function works with two types of covariates/explanatory
variables: numeric (continuous) and categorical (factor) variables as
depicted in table \ref{tab_contents}. Multi-level categorical variables
are recoded by building dummy variables corresponding to each level.
This is achieved through an implemented algorithm in R, through a
transformation as recommended for the estimation of the GLM,
particularly in the estimation of the poisson regression model for count
data.

The model revolves around the fact that for each categorical variable
(covariate), previously transformed into a dummy variable, one must
specify a reference category from which the corresponding observations
under the same covariate are estimated and assigned a weight against in
the model (Covrig et al., 2015). By default in the GLM, the first level
of the categorical variable is taken as the reference level. As best
practice, De Jong, Heller, and others (2008), Frees and Sun (2010),
Denuit, Maréchal, Pitrebois, and Walhin (2007), Cameron and Trivedi
(2013) and others recommend that for each categorical variable one
should specify the modal class as the reference level; as this variable
corresponds to the level with the highes order of predictability,
excluding the dummy variable corrresponding to (weight coefficient =
\(0\)) the biggest absolute frequency.

\textbf{\section{Description of the dataset}}
\label{sec:Description of the dataset}

In this section, section \ref{sec:Description of the dataset}, the
dataset called \emph{OpRiskDataSet\_exposure}, provides data on the
increase in the numbers of operational events over a three month period,
beginning 01 January 2013 to end of 20 March 2013. For each transaction,
there is information about: trading risk exposure, trading
characteristics, causal factor characteristics and their cost.

\begin{figure}
\begin{subfigure}[b]{0.55\textwidth}
   \begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Intra-day Trend of Loss Severity} & \textbf{Trends of Loss Severities per Trader} \\
        \includegraphics[width=7.5cm]{IntraDayUpdatedTime.eps}
         &
         \includegraphics[width=7cm]{TrendTraderId.eps}
         \end{tabular}
    \end{frame}
\subcaption{Scatterplots}
   \label{Intra_Day_Trends} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
   \begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Loss per month} & \textbf{Trading frequency} \\
        \includegraphics[width=7.5cm]{UpdatedDayFreq.eps}
         &
         \includegraphics[width=7cm]{TradedDayFreq.eps}
         \end{tabular}
    \end{frame}
\subcaption{Histograms}
   \label{Hist_Loss_Freq}
\end{subfigure}
\caption[Numerical grid display]{(a) Scatterplots of intra-day trend analysis for logs of severities of operational events and trends incident activity for identifying the role of the trader originating the incidents. (b) As for (a) but in the form of histograms showing the frequency distrbution of the number daily operational indicents and the number of trades over a monthly period.} 
\end{figure}

\subsection{Characteristics of exposure}

The exposure of risk of type \(i\), \(d_i\) shows the daily duration,
from when the trade was booked to the moment the operational risk event
was observed and ended. This measure is defined this way when
specifically applied to projecting the number of loss events
(frequencies) and can be plotted as follows depicted in graphs depicted
in Figure \ref{Exploration_analysis_exposure}.\medskip

\begin{figure}
\begin{frame}
      \centering
       \begin{tabular}{ccc}
        \textbf{Distribution} & \textbf{Density} & \textbf{Digital Analysis} \\
        \includegraphics[width=5cm]{Exposure_cdf.eps}
         &
         \includegraphics[width=5cm]{Dist_exposure.eps}
         &
         \includegraphics[width=5cm]{Benford.eps}
         \end{tabular}
    \end{frame}
        \captionof{figure}{A simple comparison of the Sigmoidal like features of the fat-tailed, right skewed distribution for exposure, and first-digit frequency distribution from the exposure data with the expected distribution according to Benford's Law}
    \label{Exploration_analysis_exposure}
\end{figure}

The variable follows a logistic trend on \([0,1]\), implying an FIs
operational risk portfolio rises like a sigmoid function throughout the
period of observation, typically starting from \(0\), which then
observes a plateau in growth. The average exposure is 389.99 or about 1
year.\medskip

Grid plots \ref{Exploration_analysis_exposure} portray the logistic
function, together with a simple comparison of first-digit frequency
distribution analysis, according to Benford's Law, with exposure data
distribution. The close fitting nature implies the data are uniformly
distributed across several orders of magnitude, especially within the 1
year period.\medskip

\subsection{Characteristics of the covariates}

The characteristics of the operational risk portfolio are given by the
following covariates: \emph{UpdatedDay}, \emph{UpdatedTime} - the day of
the month and time of day the OpRisk incident occurs respectively;
\emph{TradedDay}, \emph{TradedTime} - the day in the month and time of
day the deal was originated respectively; The \emph{LossIndicator} as
indicated before is a binary variable consisting of two values: A \(0\),
which indicates pending or near misses, and \(1\), if the incident
results in a realised loss, meaning that there is significant p\&L
impact due to the OpRisk incident.\medskip

the \emph{Desk} is the location in the portfolio tree the incident
originated, it is a factor variable conisting of 10 categories;
\emph{CapturedBy}, the designated analyst who actions the incident, a
factor variable consisting of 5 categories; \emph{TraderId}, the trader
who originates the deal, a factor variable with 7 categories;
\emph{TradeStatus}, the live status of the deal, a factor variable with
4 categories; \emph{Instrument}, the type of deal, a factor variable
with 23 categories; \emph{Reason}, a description of the cause of the
OpRisk incident, a factor variable with 19 levels;
\emph{EventTypeCategoryLevel}, 7 OpRisk event types as per Risk (2001),
a factor variable with 5 categories; \emph{BusinessLineLevel}, 8 OpRisk
business lines as per Risk (2001), a factor variable with 8
categories.\medskip

The factor variables were transformed into dummy variables using the
following commands: \singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Remap factor variables and transform into numeric variables.}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_Desk"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"Desk"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_CapturedBy"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset}
\NormalTok{                                              [[}\StringTok{"CapturedBy"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_TraderId"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TraderId"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_Instrument"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset}
\NormalTok{                                              [[}\StringTok{"Instrument"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_Reason"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"Reason"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_EventTypeCategoryLevel1"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset}
\NormalTok{                                        [[}\StringTok{"EventTypeCategoryLevel1"}\NormalTok{]])}
\NormalTok{crs}\OperatorTok{$}\NormalTok{dataset[[}\StringTok{"TNM_BusinessLineLevel1"}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(crs}\OperatorTok{$}\NormalTok{dataset}
\NormalTok{                                             [[}\StringTok{"BusinessLineLevel1"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\doublespacing

The continuous numerical variable \emph{Loss}, shows the financial
impact (severity) of the OpRisk incident in Rands. For the most part
(i.e.~96.1\% of the time) OpRisk incidents result in pending losses
and/or near misses, most realised losses (2.3\%) lie within the
{[}\textbf{R$200,00$}, \textbf{R$300,000$}{]} range. In the current
portfolio there are also five p\&L impacts higher than
\textbf{R$2.5$ million}.\medskip

\subsection{Characteristics of daily operational activity}

The distribution of daily losses and/or pending/near misses by
operational activities are represented in
\ref{Exploratory_Time_Day_Frequency3plot}. Figure
\ref{Exploratory_UpdateTime_Frequency3plot} shows that most operational
events occur in times leading up to midday (i.e.~10:50AM to 11:50AM),
the observed median is 11:39AM, and of these potential loss events, most
realised losses occur closest to mid-day. The frequencies of the loss
incidents in the analysed portfolio sharply decreases during the
following period, i.e.~from 12:10PM to 13:10PM, during which the least
realised losses occur.\medskip

Figure \ref{Exploratory_UpdateDay_Frequency3plot} shows that operational
activity increases in intensity in the days leading up to the middle of
the month, i.e. \(10^{th}\) - \(15^{th}\); the observed mean is
\(14.49\) days, and of these potential loss events, realised losses
especially impact on the portfolio during these days.

\singlespacing

\doublespacing

\begin{figure}
\centering
\begin{subfigure}[b]{0.55\textwidth}
   \includegraphics[width=1\linewidth]{Exploratory_UpdateTime_Frequency3plot.eps}
   \subcaption{Frequency distributions of operational incidents by the time in the day}
   \label{Exploratory_UpdateTime_Frequency3plot} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
   \includegraphics[width=1\linewidth]{Exploratory_UpdateDay_Frequency3plot.eps}
   \subcaption{Frequency distributions of operational incidents by the day in the month}
   \label{Exploratory_UpdateDay_Frequency3plot}
\end{subfigure}

\caption[Two numerical solutions: Histograms showing the distribution of UpdatedTime \& UpdatedDay by LossIndicator.]{The frequency distributions of All the losses, the realised losses, and pending/near misses of operational incidents by the day in the month when the indidents' occurred}
\label{Exploratory_Time_Day_Frequency3plot}
\end{figure}

Similarly, the influence of trading desk's on the frequency of
operational events can be analysed on the basis of the portfolio's
bidimensional distribution by variables \emph{Desk} and
\emph{LossIndicator}, which shows the proportions realised losses vs
pending and/or near misses for each particular desk. The bidimensional
distribution of \emph{Desk} and \emph{LossIndicator} is presented in a
contingency table, Table \ref{tab_Desk_Prop}, in which it's considered
useful to calculate proportions for each desk category.

\begin{figure}
\centering
\includegraphics[width=20cm,height=5cm]{Density_UpdateDay_TradedDay.eps}
\caption[Density plots showing a comparison of realised vs pending losses and/near misses over a month for the day in the month the OpRisk incident was updated to the day in the month trades were traded/booked]{Density plots showing a comparison of realised vs pending losses and/near misses over a month for the day in the month the OpRisk incident was updated to the day in the month trades were traded/booked}
\label{Desk_Proportions}
\end{figure}

\singlespacing

\doublespacing

\begin{table}[ht]
\centering
\caption{Occurence of realised losses: proportions on desk categories}
\begin{tabular}{lccr}
\toprule
  & \multicolumn{3}{c}{No. of transactions} \\
Desk   & no Loss   & Loss & Total\\ 
\midrule
  Africa            &  49 & 10 &  59 \\
  Bonds/Repos       & 113 & 31 & 144 \\
  Commodities       & 282 & 45 & 327 \\
  Derivatives       & 205 & 24 & 229 \\
  Equity            & 269 & 66 & 335 \\
  Management/Other  &  41 &  2 &  43 \\
  Money Market      & 169 & 52 & 221 \\
  Prime Services    & 220 & 62 & 282 \\
  Rates             & 336 & 53 & 389 \\
  Structured Notes  & 275 & 26 & 301 \\
 \bottomrule
\end{tabular}\label{tab_Desk_Prop}
\end{table}

Thus, as illustratred in figure \ref{Desk_Proportions}, from 23,5\%; the
highest proportion of realised losses per desk is the Money Market (MM)
desk, the figures are decreasing, followed by Prime Services (22\%);
Bonds/Repos (21,5\%); Equity (19,7\%); Africa (16,9\%); Commodities
(13,8\%); Rates (13,6\%); Derivatives (10,5\%); Structured Notes (SND)
(8.6\%), to the least proportion in the Management/Other, a category
where only 4,7\% of operations activities were realised as losses.

\singlespacing

\doublespacing

\begin{figure}
\centering
\includegraphics[width=20cm,height=5cm]{Exploratory_Desk_Proportions.eps}
\caption[Desk category by realised losses]{Histograms showing the proportions of realised losses vs all losses including pending and/or near misses by desk category}
\label{Desk_Proportions}
\end{figure}

This behaviour can be extended beyond the trading desk, as represented
in Figure \ref{Mosaic_Instr_Trd_Tec}, a mosaic plot grid presenting the
structure of the OpRisk portfolio by Instrument, TraderId, CapturedBy
\footnote{i.e. the type of financial instrument, the trader who originated the incident on the deal, and the role of the technical support personnel who is involved in the query resolution.}
and the operational losses.

\singlespacing

\doublespacing

\begin{figure}
\begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Type of instrument traded} & \textbf{Role identification} \\
        \includegraphics[width=7.5cm]{Single_Instr.eps}
         &
         \includegraphics[width=7.5cm]{Stacked_TrId_TechSup.eps}
         \end{tabular}
    \end{frame}
    \caption{Mosaic grid plots for the bidimensional distribution by traded instrument, the trader originating the operational event, and by the technical support personnel involved in query resolution, against the dummy variable showing if a realised loss was reported.}
    \label{Mosaic_Instr_Trd_Tec}
\end{figure}

One can notice that the width of the bars corresponding to the different
categories, i.e.~Instrument, TraderId, CapturedBy, is given by their
proportion in the sample. In particular, for the category `at least one
realised loss', in the top right mosaic of Figure
\ref{Mosaic_Instr_Trd_Tec} portrays a increase in ``riskiness'' trending
up from Associate to AMBA, Analyst, Vice Principal, Managing Director,
Director, up to the risky ATS category, which are automated trading
system generated trades.\medskip

Figure \ref{Mosaic_Instr_Trd_Tec} bottom right mosaic plot for technical
support personnel for the category `at least one realised loss',
portrays a downward trend, slowing in riskiness from Unauthorised User
downward to Tech Support, Mid Office, Prod Controller down to the least
risky Prod Accountant. This intepretation makes sense given unauthorised
users are more likely to make impactful operational errors, technical
support personnel would also be accountable for large impacts albiet for
contrasting reasons, they are mandated to perform these deal adjustments
which have unavoidable impacts associated with them, whereas the former
group are unauthorised to perform adjustments therefore may lack the
skill, or be criminally minded insiders acting on their own or in unison
to enable their underhanded practices and intentions without raising any
suspicion.\medskip   

In another mosaic plot, Figure \ref{Mosaic_Contingency}, the
bidimensional distribution of transactions by trader and realised vs
pending losses, conditional on the trade status is presented and
analysed. Here, and in the contingency table, Table
\ref{tab:Mosaic_Contingency}, we can clearly see the following trends:
In BO-BO confirmed status - an increase in realised losses from the
leftmost TraderID (i.e.~AMBA) to right, and the opposite for
transactions performed in BO Confirmed status (both with two
exceptions). In particular, the biggest number of realised losses in
both BO and BO-BO Confirmed statuses occur due to automated trading
systems (ATS) who also give rise to the exceptions mentioned.\medskip

Table \ref{tab:Mosaic_Contingency} and Figure \ref{Mosaic_Contingency}
are obtained with the following commands:

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(vcd)}
\NormalTok{STD <-}\StringTok{ }\KeywordTok{structable}\NormalTok{(}\OperatorTok{~}\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{LossIndicator}
\NormalTok{                                        , }\DataTypeTok{data =}\NormalTok{ projdata)}
\NormalTok{MS01 <-}\StringTok{ }\KeywordTok{mosaic}\NormalTok{(STD, }\DataTypeTok{condvars =} \StringTok{'TradeStatus'}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{20}\NormalTok{),}
                  \DataTypeTok{split_horizontal =} \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\singlespacing

\begin{figure}
\centering
\textbf{Mosasic plot for trader identification and loss indicator, by trade status}
\includegraphics[width=\linewidth,height=0.75\linewidth]{Mosaic_Contingency.eps}
\caption[Portfolio structure by trader, trade status and number of realised losses]{A mosaic plot representing the structure of the operational risk portfolio by trader identification (TraderId), the status ofthe trade (TradeStatus) and the number of realised losses vs pending or near misses}
\label{Mosaic_Contingency}
\end{figure}

\doublespacing

Table \ref{tab:Crosstab_covariate} presents the most frequent category
in the operational risk dataset for each possible covariate.

\begin{table}[htbp]
        \centering
        \textbf{Crosstab of trader identification and loss indicator, by trade status}
\singlespacing        
        \small
        \setlength\tabcolsep{2pt}
            \begin{tabular}{|p{2cm}|p{2cm}|l|l|l|l|l|l|p{2cm}|p{2cm}|} \hline
            & & \multicolumn{7}{|c|}{Trader Identification} \\ \hline
            TradeStatus & Loss Indicator & Amba & Analyst & Associate & ATS & Director & Mng Director & Vice Principal \\\hline
            \multirow{2}{*}{BO-BO Confirmed} & 0 & 24 & 136 & 320 & 0 & 282 & 52 & 49 \\ \cline{2-9}
                                   & 1 & 2  &  15 & 43 & 0 & 50 & 18 & 16 \\\cline{2-9}
            \multirow{2}{*}{BO Confirmed} & 0 & 17  & 299 & 153 & 13 & 257 & 102 & 153 \\ \cline{2-9}
                                   & 1 &  3 &  71 & 12 & 8 &  62 & 23 & 30 \\ \cline{2-9}
            \multirow{2}{*}{Terminated}       & 0 & 83 & 9 & 1 & 0 & 0 & 2 & 1 \\ \cline{2-9}
                                  & 1 & 17 & 1 & 0 & 0 & 0 & 0 & 0 \\ \cline{2-9}
            \multirow{2}{*}{Terminated/Void}  & 0 & 2 & 0 & 0 & 0 & 2 & 1 & 1 \\ \cline{2-9}
                                   & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
            \end{tabular}
            \caption{A contingency table showing the bidimensional distribution of transactions by trader identification vs realised and/or pending losses, conditional on the trade status}
            \label{tab:Crosstab_covariate}
\end{table}

\doublespacing

\begin{table}[htbp]
        \centering
        \textbf{Modal classes for the categorical variables} 
\singlespacing        
        \small
        \setlength\tabcolsep{2pt}
            \begin{tabular}{|l|l|p{4cm}|} \hline
            Variable & Modal class or category & Name of modal class \\\hline
            Desk & Rates & DeskRates \\ \cline{1-3}
            CapturedBy & TECHSUPPORT & CapturedBy\_TECHSUPPORT \\ \cline{1-3}
            TradeStatus & BO confirmed & TradeStatus\_BO confirmed \\ \cline{1-3}
            TraderId & DIRECTOR & TraderId\_DIRECTOR \\ \cline{1-3}
            Instrument & Swap & Instrument\_Swap \\ \cline{1-3}
            Reason & Trade enrichment for system flow  & Reason\_Trade enrichment for system flow \\ \cline{1-3}
            EventTypeCategoryLevel & EL7 & EventTypeCategoryLevel\_EL7 \\ \cline{1-3}
            BusinessLineLevel & BL2 & BusinessLineLevel\_BL2 \\ \cline{1-3} 
            \end{tabular}
            \caption{A contingency table showing the bidimensional distribution of transactions by trader identification vs realised and/or pending losses, conditional on the trade status}
            \label{tab:Mosaic_Contingency}
\end{table}

\doublespacing

\textbf{\section{The estimation of some poisson regression generalised linear models (GLM's)}}
\label{sec:The estimation of some poisson regression generalised linear models (GLM's)}

Section \ref{sec:Generalised Linear Models} introduced a GLM for the
start of the expected number of operational events in the early stages.
We aim to estimate the mean OpRisk frequency through a poisson
classification model given by equation \ref{eqn:Poisson} using the glm
function. The mean daily loss frequency in the risk correction
statistics is estimated through the poisson regression model. Let us
consider a model where the \emph{LossIndicator} is the target variable:
The following fits the model (the log link is canonical for the poisson
distribution, and hence the R default) and checks it.\medskip

\singlespacing

\doublespacing

In calling the GLM we specify the target variable \emph{LossIndicator};
the explanatory variables are composed of of numeric, continuous and
categorical variables. Where the variable in the argument of a GLM is
categorical , one chose to specify the modal class as the reference
level. A user defined function ``getmode'' has been created; it selects
the modal observation in each factor, and the dataset is reordered using
the \emph{relevel} function in
\includegraphics[width=0.05000\textwidth]{C:/Users/User/Documents/OpRiskPHDGitHub/OpRisk_PHD_Dissertate/OpRisk_PHD_Dissertation/figures/smallorb_90.eps}.
These specifications were achieved in the following code chunk:

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create function "getmode" which finds the modal class in}
\CommentTok{# the categorical variables}
\NormalTok{getmode <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  u <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(x)}
  \KeywordTok{as.integer}\NormalTok{(u[}\KeywordTok{which.max}\NormalTok{(}\KeywordTok{tabulate}\NormalTok{(}\KeywordTok{match}\NormalTok{(x,u)))])}
\NormalTok{\}}
\CommentTok{# Reorder the categorical variables so that the modal class }
\CommentTok{# is specified as the reference level}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{5}\OperatorTok{:}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(d1) }\OperatorTok{-}\StringTok{ }\DecValTok{3}\NormalTok{))\{}
\NormalTok{     d1[[i]] <-}\StringTok{ }\KeywordTok{relevel}\NormalTok{(d1[[i]], }\KeywordTok{getmode}\NormalTok{(d1[[i]]))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\doublespacing

Other GLM arguments are: The afore-mentioned link function
poisson(link=``log''); a data frame containing the OpRisk dataset,
data=d1; and the r offset=log(exposure), i.e.~the variable representing
a component known apriori, coefficient= \(1\), introduced in the linear
predictor (Covrig et al., 2015).\medskip

Firstly, consider a GLM in which is introduced two explanatory
variables, one numerical variable, \emph{UpdatedTime}, and another
categorical variable \emph{Desk}. This will be our global model. We will
use \emph{LossesIndicator} as the target variable, while these two
unique variables will be explanatory variables:

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freqfit1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(LossesIndicator }\OperatorTok{~}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk, }\DataTypeTok{data=}\NormalTok{d1, }
               \DataTypeTok{family=}\KeywordTok{poisson}\NormalTok{(}\DataTypeTok{link =} \StringTok{'log'}\NormalTok{), }\DataTypeTok{offset =} \KeywordTok{log}\NormalTok{(exposure))}
\end{Highlighting}
\end{Shaded}

\doublespacing

The output result of the estimation is presented below, where variables
who were found to be significant predictors are indicated. The
coefficients of the categorical variable \emph{Desk} are reordered and
weighted against the modal class: \emph{DeskRates}. Interestingly the
modal class does not show up in the results section (as the coefficient
of the modal class = \(0\)), given that the remaining classes are
weighted against it.

\singlespacing

\begin{verbatim}
## 
## Call:
## glm(formula = LossesIndicator ~ UpdatedTime + Desk, family = poisson(link = "log"), 
##     data = d1, offset = log(exposure))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8450  -0.5587  -0.2438  -0.0536   4.2780  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(>|z|)    
## (Intercept)           -9.2147     0.2855 -32.275  < 2e-16 ***
## UpdatedTime            1.7972     0.4749   3.784 0.000154 ***
## DeskAfrica             1.2515     0.3449   3.629 0.000285 ***
## DeskBonds/Repos        1.7758     0.2263   7.846 4.30e-15 ***
## DeskCommodities        0.8274     0.2027   4.082 4.47e-05 ***
## DeskDerivatives       -0.2071     0.2468  -0.839 0.401446    
## DeskEquity             1.3687     0.1849   7.403 1.33e-13 ***
## DeskManagement/Other  -1.2208     0.7204  -1.695 0.090135 .  
## DeskMM                 0.3910     0.1954   2.001 0.045431 *  
## DeskPrime Services     2.1217     0.1875  11.316  < 2e-16 ***
## DeskSND               -0.7055     0.2397  -2.943 0.003250 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 2767.9  on 2329  degrees of freedom
## Residual deviance: 2461.7  on 2319  degrees of freedom
## AIC: 3225.7
## 
## Number of Fisher Scoring iterations: 8
\end{verbatim}

\doublespacing
Using this bivariate model, the estimated quarterly OpRisk
(LossIndicators) frequency of realised losses for each \emph{Desk}
category (excluding the insignificant ones) are:

\begin{list}{*}{}
\item $0,002099618  = e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.2515}$, for the combination of the \textbf{UpdateTime} and \textbf{DeskAfrica} category, which implies that frequency of realised losses for this combination of preditor variables is $3.4955824(=\cdot e^{1.2515})$ fold (times) higher than the realised loss frequency of OpRisk causes in the reference desk category, viz. the \textbf{Rates} desk. 
\item $0,003546834 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.7758}$, for the combination of the \textbf{UpdateTime} and \textbf{DeskBonds/Repos} category, which implies that frequency of realised losses for this combination of preditor variables is $5,90500325(=\cdot e^{1.7758})$ fold higher than causes in the reference desk category.
\item $0,001373903 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{0.8274}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $2,287363856(=\cdot e^{0.8274})$ fold higher than the causes in the reference desk category.
\item $0,002360693= e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.3687}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $3,930238063(=\cdot e^{1.3687})$ fold higher than the causes in the reference desk category.
\item $0,001373903 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{0.8274}$, for the combination with \textbf{DeskMM},an increase of $39\%)$ w.r.t the baseline (the \textbf{Rates} desk)
\item $0,005012603= e^{-9.2147}\cdot e^{1.7972}\cdot e^{2.1217}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $8,345312467(=\cdot e^{2.1217})$ fold higher than the causes in the reference desk category.
\end{list}

The predicted mean frequency of realised losses for OpRisk incident
\(i\), for the model \textbf{freqfit1}, is given by:

\singlespacing

\begin{eqnarray}
\mu_{i}& = &\mbox{exposure}_i\cdot e^{-9.2147\cdot \mbox{Intercept}_i}\cdot e^{1.7972\cdot \mbox{UpdatedTime}_i}\cdot e^{1.2515\cdot \mbox{DeskAfrica}_i}\nonumber\\
&\cdot&e^{1.7758\cdot \mbox{DeskBonds/Repos}_i}\cdot e^{0.8274\cdot \mbox{DeskCommodities}_i}\cdot e^{1.3687\cdot \mbox{DeskEquity}_i}\nonumber\\
&\cdot& e^{0.3910\cdot \mbox{DeskMM}_i}\cdot e^{2.1217\cdot \mbox{DeskPrime Services}_i}\cdot e^{-0.7055\cdot \mbox{DeskSND}_i}
\end{eqnarray}

\doublespacing

We now fit a more comprehensive model where we introduce more variables,
in which show realised losses for quarterly OpRisk incidents for an all
inclusive case. We will use ``LossesIndicator'' as the dependent
variable, while the other variables will be predictor variables.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freqfit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(LossesIndicator }\OperatorTok{~}\StringTok{ }\NormalTok{UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}
\StringTok{                 }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}
\StringTok{                 }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason}
               \OperatorTok{+}\StringTok{ }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1,}
\DataTypeTok{data=}\NormalTok{d1, }\DataTypeTok{family=}\KeywordTok{poisson}\NormalTok{(}\DataTypeTok{link =} \StringTok{'log'}\NormalTok{), }\DataTypeTok{offset =} \KeywordTok{log}\NormalTok{(exposure))}
\end{Highlighting}
\end{Shaded}

\doublespacing

Which yields output (in summarised form):

\singlespacing

\begin{verbatim}
Call:
glm(formula = LossesIndicator ~ UpdatedDay + UpdatedTime + TradedDay + 
    TradedTime + Desk + CapturedBy + TradeStatus + TraderId + 
    Instrument + Reason + EventTypeCategoryLevel1 + BusinessLineLevel1, 
    family = poisson(link = "log"), data = d1, offset = log(exposure))

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-4.6205  -0.3700  -0.1056  -0.0295   4.0726  

Coefficients:
                                                        Estimate Std. Error z value             Pr(>|z|)    
(Intercept)            -8.953252   0.604562 -14.809 < 0.0000000000000002 ***
UpdatedDay             -0.006976   0.008140  -0.857             0.391428    
UpdatedTime             1.113913   0.564165   1.974             0.048331 *  
TradedDay              -0.012303   0.006368  -1.932             0.053382 .  
TradedTime              0.101378   0.637529   0.159             0.873656    
DeskAfrica              1.899956   0.446050   4.260   0.0000204875303586 ***
DeskBonds/Repos         2.803220   0.334324   8.385 < 0.0000000000000002 ***
DeskCommodities         0.747527   0.364630   2.050             0.040355 *  
DeskDerivatives         0.683199   0.374174   1.826             0.067867 .  
DeskEquity              1.507079   0.321232   4.692   0.0000027113532659 ***
DeskManagement/Other   -2.054697   1.082815  -1.898             0.057755 .  
DeskMM                  1.544054   0.453315   3.406             0.000659 ***
DeskPrime Services     -0.028783   0.960227  -0.030             0.976087    
DeskSND                 0.766563   0.573844   1.336             0.181602  
\vdots                  \vdots     \vdots     \vdots            \vdots  

BusinessLineLevel1BL1   1.537103   0.636829   2.414             0.015792 *  
BusinessLineLevel1BL3  -0.359123   0.514434  -0.698             0.485119    
BusinessLineLevel1BL4  -1.384293   0.391691  -3.534             0.000409 ***
BusinessLineLevel1BL5  -1.169766   0.394350  -2.966             0.003014 ** 
BusinessLineLevel1BL6   1.250498   1.002141   1.248             0.212095    
BusinessLineLevel1BL7   0.875839   1.746369   0.502             0.616005    
BusinessLineLevel1BL9   4.214689   1.598598   2.636             0.008377 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 2767.9  on 2329  degrees of freedom
Residual deviance: 1821.2  on 2252  degrees of freedom
AIC: 2719.2

Number of Fisher Scoring iterations: 13
\end{verbatim}

\doublespacing

\singlespacing

\doublespacing

\subsubsection{Model selection and multimodel inference}

The selection of the best-fit model from the list of possible
combinations of predictor variables traditionally follows of a process
removing/adding each variable progressively after each estimation, and
propagating backward/forward, comparing goodnes of fit tests at each
stage. For example, if we compare the values of the Aikaike information
criteria (AIC) for the bivariate model \textbf{freqfit1} and the
multivariate model \textbf{freqfit}, by AICs; we see that for the first
model the AIC value is 3225.7 and 2719.2 for the second model, which
suggests that the second model, \textbf{freqfit}, the model in which we
considered an all inclusive list of 15 predictor variables is a better
fit since the AIC reduces in magnitude the first, hence \textbf{freqfit}
is prefereble to the first. \medskip

In a similar way, we can estimate the models comparing each one which
enables one to choose the most appropriate or ``best'' fit one, by first
checking if the model is significant, i.e.~if the Residual deviance and
the corresponding number of degrees of freedom doesn't have a value
significantly bigger than 1: In the latter model
\(\frac{1821.2}{2252} = 0.808703374\), and then retaining the one with
the smaller AIC value.\medskip 

Burnham and Anderson (2002) introduces information-theoretic approach
that allows a data-based selection of a ``best'' model in the anaysis of
our dataset, and a ranking and weighting of the remaining models. These
approaches allow traditional (formal) statistical inference to be based
on the selected ``best'' model, which is now based on more than one
model (multimodel inference). To do this we are required to load the
``MuMIn'' package in
\includegraphics[width=0.05000\textwidth]{C:/Users/User/Documents/OpRiskPHDGitHub/OpRisk_PHD_Dissertate/OpRisk_PHD_Dissertation/figures/smallorb_90.eps}.
\medskip

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(MuMIn)}
\end{Highlighting}
\end{Shaded}

\doublespacing

Then, we use ``dredge'' function to generate models using combinations
of the terms in the global model. The function will also calculate AICc
values and rank models according to it. Note that AICc is AIC corrected
for finite sample sizes. The process of analyzing data where the
experimentalist has few or no a priori information, thus ``all possible
models'' are considered by subjectively ad iteratively searching the
data for patterns and ``significance'', is often called ``data mining'',
``data snooping'' or the term ``data dredging''.

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{na.action=}\NormalTok{na.fail)}
\NormalTok{freqfits <-}\StringTok{ }\KeywordTok{dredge}\NormalTok{(freqfit)}
\end{Highlighting}
\end{Shaded}

\doublespacing

The function ``MuMLn::dredge'' returns a list of \(4096\) models, which
is every combination of predictor variable in the global model freqfit.
Model number 2942 is the best model and shows that all predictor
variables included in the model have a positive effect on the target
variable except for the preditor TrddD (\textbf{TradedDay}) which has a
negative effect on the likelihood og a realised loss (target variable
\emph{LossIndicator}). Additionally, from the delta (=delta AIC) one
cannot distinguish model 2942 from 3966 and 2878 since (using the common
thumb rule) they have AIC \textless{} 2.\medskip

The top three models, models \(2942, 3966\) \& \(2878\) each include
nine, ten and seven predictor variables respectively, and where a
variable doesn't have a value, it means that it was not included in the
model, not that it does not have and effect. For example model \(2942\)
returns a combination of the seven variables \(1/2/3/4/5/6/7/8/10\),
corresponding to the following output predictor variables (abbreviated
in the header row) below:

\singlespacing

\begin{verbatim}
Model selection table 
     (Intrc) BsLL1 Desk ETCL1 Instr Reasn TrddD   TrdrI TrdSt UpdtT 
2942  -9.107     +    +     +     +     + -0.011660   +     + 1.2760000 
\end{verbatim}

\doublespacing

Information from the AICc's values suggest, that of the top three models
have similar support, and their Akaike weights are not high relative to
the \([0,1]\) weight range; This is characteristic of the endemic nature
of data dredging, as the literature suggests (Burnham and Anderson,
2002), and should generally be avoided to curb attendant inferential
problems if a single model is chosen, e.g the risk of finding spurious
effects, overfitting, etc. .Burnham and Anderson (2002) advises that
model averaging is useful in finding a confirmatory result as estimates
of precision should include model selection uncertainty. Even so, one
can rule out many models on a priori grounds.\medskip    

We now use ``get.models'' function to generate a list in which its
objects are the fitted models. We will also use the ``model.avg''
function to do a model averaging based on AICc. Note that
``subset=TRUE'' will make the function calculate the average model (or
mean model) using all models. However, we want to get only the models
that have delta AICc \textless{} 2; we threfore use
``subset=delta\textless{}2''

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{adelmodel <-}\StringTok{ }\NormalTok{(}\KeywordTok{model.avg}\NormalTok{(}\KeywordTok{get.models}\NormalTok{(freqfits, }\DataTypeTok{subset=}\NormalTok{delta}\OperatorTok{<}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\doublespacing

Now we have AICc values for our models and we have the average (mean)
model.\medskip

Multimodel inference leads to more robust inferences, especially in the
point of view that the selection of the model used to estimate the mean
frequency must, at the same time, serve the ultimate root cause analysis
objective of OpRisk control, that decide calculating capital
requirement, in OpVaR measures, taking into account as many
characteristics of the trading OpRisk dataset as possible, as well
considering how the variables interact with each other.

\textbf{\subsection{Modelling population size of the OpRisk events}}

We have gained initial insights through data exploration in Section
\ref{sec:Exploratory data analysis} and then built models. The next
critical step is to evaluate our model. For this we need to use a
testing dataset whose function is to provide error estimates of the
final result. The testing dataset is not used in building or even fine
tuning the models that we build, for the sake of model building define a
training dataset and a validation dataset to test different parameter
setings or different choices of variables in the data mining part of the
project.\medskip

We have a population of \(K = 2330\) OpRisk events over the first
quarter Q12013, and of these events we have a number \(N = 371\) of
realised losses. \(N\) is a discrete random variable modelled as a
Poisson variable with rate \(\lambda\). Each loss \(X_i\) is another
random variable with an underlying sverity distribution. How does the
size \(K\) of the population enter the risk model?. It doesn't appear
explicitly in the model (Parodi, 2014), however, it is taken into
account during the creation of the model. Intuitively, the poisson rate
\(\lambda\) is likely to be proportional to the current OpRisk sample
size, or more specifically, it is the rate of some expected operational
event over per specified time interval. Predicting test set results and
evaluating the parameter \(\lambda\)

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{av.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(adelmodel, test_set)}

\NormalTok{MASS}\OperatorTok{::}\KeywordTok{fitdistr}\NormalTok{(}\KeywordTok{exp}\NormalTok{(av.pred), }\StringTok{"Poisson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\doublespacing

yields a daily rate of \(\lambda = 0.001840831\) or 0.1840831\% per
day.\medskip 

By a simple growth formula, five years of data (20 quarters) i.e., 3
months * 20 = 5 years:

\singlespacing

\begin{eqnarray}
5yr_Population &=& Initial_Population * (1 + \lambda)^n \nonumber \\
5yr_Population &=& 2330*(1+0.18009498)^20 \nonumber \\
5yr_Population &=& 63929
\end{eqnarray}

\doublespacing

corresponds to a 5yr population of \(63929\) observations. What remains
is to use the extrapolation script to generate the simulated dataset.

\textbf{\section{The estimation of some  generalised additive models for location scale and shape (GAMLSS) for severity of loss}}
\label{sec:The estimation of some  generalised additive models for location scale and shape (GAMLSS) for severity of loss}

We introduce a Box-Cox Power Exponential distribution (BCPE), which is a
four parameter distribution, for fitting a GAMLSS to estimate the
(non-linear nature) mean OpRisk loss severity using the gamlss function.
The mean daily loss severities in the risk correction statistics is
estimated through the BCPE gamlss model.\medskip

The pdf of the BCPE distribution is defined as: \singlespacing

\begin{eqnarray}
f(y|\mu,\sigma,\nu,\tau)&=&(y^{(\nu-1)/\mu^nu)}\cdot{\frac{\tau}{\sigma}}\cdot \frac{e^(-0.5\cdot|\frac{z}{c}|^\tau)}{(c\cdot 2^(1+\frac{1}{tau})}\cdot \Gamma(\frac{1}{\tau}))\nonumber\\
\mbox{where} \quad c&=&[2^(\frac{-2}{\tau})\cdot\frac{\Gamma(\frac{1}{\tau})}{\Gamma(\frac{3}{\tau})}]^{0.5},\quad \mbox{where if}\quad \nu!=0, \quad \mbox{then} \nonumber\\
Z&=&\frac{(\frac{y}{\mu})^\nu-1}{\nu\cdot \sigma},\quad \mbox{else} \quad z=\frac{log\frac{y}{\mu}}{\sigma},\nonumber\\
\mbox{for} \quad y>0 &,& \mu>0, \sigma>0, \nu=(\mbox{-Inf,+Inf})\quad \mbox{and}\quad \tau>0.
\end{eqnarray}

\doublespacing

The BCPE adjusts the obove density \(f(y|\mu,\sigma,\nu,\tau)\),
resulting from the condition \(y>0\). See Stasinopoulos, Rigby, Heller,
Voudouris, and De Bastiani (2017) . We now consider a model where the
\emph{Loss} is the target variable: The following fits the model and
checks it.\medskip

\singlespacing

\doublespacing

\singlespacing

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gamlss)}
\NormalTok{sf <-}\StringTok{ }\KeywordTok{gamlss}\NormalTok{(Losses}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}
\StringTok{                }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }
               \OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }
\StringTok{                 }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1), }
\DataTypeTok{sigma.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }
\StringTok{                }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }
               \OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }
\StringTok{               }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1),}
\DataTypeTok{nu.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime}
               \OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }
\StringTok{              }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }
\StringTok{              }\NormalTok{BusinessLineLevel1),}
 \DataTypeTok{tau.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime}
                \OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}
\StringTok{                }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}
\StringTok{                }\NormalTok{BusinessLineLevel1),}
\DataTypeTok{data=}\NormalTok{D1, }\DataTypeTok{mu.start =} \OtherTok{NULL}\NormalTok{,  }\DataTypeTok{sigma.start =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{nu.start =} \OtherTok{NULL}\NormalTok{,}
                              \DataTypeTok{tau.start =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{family=}\NormalTok{BCPE)}
\end{Highlighting}
\end{Shaded}

\doublespacing

\singlespacing

\FloatBarrier
\newpage
\fancyhead[L]{Methods for modeling OpRisk depending on covariates}
\fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{METHODS FOR MODELING OPRISK DEPENDING ON COVARIATES}

\doublespacing

\textbf{\section{Introduction}} \label{sec:Introduction}

This section of the paper concentrates on combining various supervised
learning techniques with extreme value theory (EVT) fitting, which is
very much based on the Dynamic EVT-POT model developed by
Chavez-Demoulin et al. (2016). This can only happen due to an abundance
of larger and better quality datasets and which also benefits the loss
distribution approach (LDA) and other areas of OpRisk modeling. In
Chavez-Demoulin et al. (2016), they consider dynamic models based on
covariates and in particular concentrate on the influence of internal
root causes that prove to be useful from the proposed methodology.

Motivated by the abundance of data and better data quality, these new
data-intensive techniques offer an important tool for ORM and at the
same time supporting the call from industry for a new class of EBOR
models that capture forward-looking aspects of ORM (Embrechts et al.,
2018). Three different machine learning techniques viz., decision trees,
random forest, and neural networks, will be employed using R. A
comprehensive list of user defined variables associated with root causes
that contribute to the accumulation of OpRisk events (frequency) has
been provided, moreover, a lot can be gained from this dataset as it
also bears the impacts of these covariates on the severity of OpRisk.

\textbf{\section{Modeling Oprisk: The loss distribution approach (LDA)}}
\label{sec:Modeling Oprisk: The loss distribution approach (LDA)}

Twenty-one key risk indicators (kri's) with eight feature groups
including person identification, trade origination, root causes and
market value sensitivities are in the chosen covariates. For each risk
event there is information about: trading risk exposure, trading
characteristics, causal factor characteristics and the losses created by
these factors. The development, training and validation of the machine
learning (ML) models lends itself to this new type of data and requires
a higher degree of involvement across operations. Moreover, at this
level of granularity the different types of data is particularly suited
to exposure-based treatment, and other forward-looking aspects within
the OpRisk framework, for improved forecasts of OpRisk losses.\medskip

The aggregated operational losses can be seen as a sum \(S\) of a random
number \(N\) individual operational losses \[(X_1, \ldots, X_N )\]. The
total required capital is the sum of VaR of each BL/ET combination
calibrated through the underlying mathematical model whose analytic
expression is given by:

\singlespacing

\begin{equation}\label{eqn4}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right), \qquad \mbox{where} \quad \vartheta(t) = \sum_{n=1}^{N(t)} X_{n}.
\end{equation}

\doublespacing

\(\mathbf{G}(t)\) can only be obtained numerically using the Monte Carlo
method, Panjer's recursive approach, and the inverse of the
characteristic function (Frachot et al. (2001); Aue and Kalkbrener
(2006); Panjer (2006); \& others).

\subsection{Research Objective 2}

To test the accuracy of several classes of data-intensive techniques in
approximating the weights of the risk factors; i.e., the input features
of the model viz., TraderID, UpdatedDay, Desk, etc. of the underlying
value-adding processes, against traditional statistical techniques, in
order to separately estimate the frequency and severity distribution of
the OpRisk losses from historical data. As a consequence, capital
estimates should be able to adapt to changes in the risk profile e.g.,
upon the addition of new products or varying the business mix of the
bank (e.g., terminations, voids, allocations, etc.) to provide
sufficient incentives for ORM to mitigate risk (Einemann et al., 2018).

\section{Theoretical investigations for the quantification of modern ORM}

Within the variety of relations among risk preferences, people have
difficulty in grasping the concept of risk-neutrality. In a market where
securities are traded, risk-neutral probabilities are the cornerstone of
trade, due to their importance in the law of no arbitrage for securities
pricing. Mathematical finance is concerned with pricing of securities,
and makes use of this idea: That is, assuming that arbitrage activities
do not exist, two positions with the same pay-off must also have an
identical market value (Gisiger, 2010). A position (normally a primary
security) can be replicated through a construction consisting of a
linear combination of long, as well as short positions of traded
securities. It is a relative pricing concept which removes risk-free
profits due to the no-arbitrage condition.\medskip

This idea seems quite intuitive from an OpRisk management perspective.
The fact that one can take internal historical loss data and use this to
make a statement on the \texttt{OpRisk} VaR measure for the population,
is based on the underlying assumption of risk neutrality. Consider a
series of disjoint risky events occurring at times \(\tau\) to
\(\tau + 1\). We can explore the concept of a two state economy in which
value is assigned to gains and losses, rather than to final assets, such
that an incremental gain or loss can be realised at state \(\tau + 1\),
contingent on the probability which positively impacts on the event
happening. \medskip

\subsection{Risk-neutral measure $\mathbb{Q}$}

Risk-neutral probabilities simply enforce a linear consistency for views
on equivalent losses/gains, with regard to the shape of the value
function. The shape the graph depicts a linear relationship based on
responses to gains/losses and value. The risk neutral probability is not
the real probability of an event happening, but should be interpreted as
(a functional mapping) of the number of loss events (frequency).\medskip

Suppose we have: \(\Theta = \mbox{Gain/Loss}\);
\(\nu(x) = \mbox{risk event happening}\); and
\(X = \mbox{Individual gain/loss (or both)}\), then; \singlespacing

\begin{eqnarray}\label{eqn3}
\Theta = &\sum_{i=1}^{n}\mbox{Pr}[\nu (x_{i})]*X_i & \\
 \mbox{where} \nonumber\\
&\sum_{i=1}^{n}\mbox{Pr}[\nu (x_{i})] = 1 &\qquad \mbox{and} \qquad \mbox{Pr}[\nu (x_{i})] \geq 0 \quad \forall i\nonumber
\end{eqnarray}

\doublespacing

Note that the random variable \(\Theta\) is the sum of the products of
frequency and severity for losses (in \texttt{OpRisk} there are no
gains).\medskip

This formula is used extensively in actuarial practices, for decisions
relating to quantifying different types of risk, in particular in the
quantification of value-at-risk (VaR) (a risk measure used to determine
capital adequacy requirements, commonly adopted in the banking
industry). A quantile of the distribution of the aggregate losses is the
level of exposure to risk, expressed as VaR.\medskip

People exhibit a specific four-fold behaviour pattern when facing risk
(Shefrin, 2016). There are four combinations of gain/loss and
moderate/extreme probabilities, with two choices of risk attitude per
combination. OpRisk measurement focuses on only those casual factors
that create losses with random uncertainty, for the value adding
processes of the business unit.

\singlespacing

\FloatBarrier
\newpage
\fancyhead[L]{Theoretical investigations for the quantification of modern ORMF's}
\fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{THEORETICAL INVESTIGATIONS INTO THE QUANTIFICATION OF MODERN ORMF'S}

\doublespacing

\section{Theoretical investigations for the quantification of modern ORM}

Within the variety of relations among risk preferences, people have
difficulty in grasping the concept of risk-neutrality. In a market where
securities are traded, risk-neutral probabilities are the cornerstone of
trade, due to their importance in the law of no arbitrage for securities
pricing. Mathematical finance is concerned with pricing of securities,
and makes use of this idea.\medskip

That is, assuming that arbitrage activities do not exist, two positions
with the same pay-off must also have an identical market value (Gisiger,
2010). A position (normally a primary security) can be replicated
through a construction consisting of a linear combination of long, as
well as short positions of traded securities. It is a relative pricing
concept which removes risk-free profits due to the no-arbitrage
condition.\medskip

This idea seems quite intuitive from an OpRisk management perspective.
The fact that one can take internal historical loss data and use this to
make a statement on the \texttt{OpRisk} VaR measure for the population,
is based on the underlying assumption of risk neutrality. Consider a
series of disjoint risky events occurring at times \(\tau\) to
\(\tau + 1\). We can explore the concept of a two state economy in which
value is assigned to gains and losses, rather than to final assets, such
that an incremental gain or loss can be realised at state \(\tau + 1\),
contingent on the probability which positively impacts on the event
happening.\medskip

\subsection{Risk-neutral measure $\mathbb{Q}$}

Risk-neutral probabilities simply enforce a linear consistency for views
on equivalent losses/gains, with regard to the shape of the value
function. The shape the graph depicts a linear relationship based on
responses to gains/losses and value. The risk neutral probability is not
the real probability of an event happening, but should be interpreted as
(a functional mapping) of the number of loss events (frequency).\medskip

Suppose we have: \(\Theta = \mbox{Gain/Loss}\);
\(\nu(x) = \mbox{risk event happening}\); and
\(X = \mbox{Individual gain/loss (or both)}\), then

\begin{eqnarray}\label{eqn3}
\Theta = &\sum_{i=1}^{n}\mbox{Pr}[\nu (x_{i})]*X_i & \\
 \mbox{where} \nonumber\\
&\sum_{i=1}^{n}\mbox{Pr}[\nu (x_{i})] = 1 &\qquad \mbox{and} \qquad \mbox{Pr}[\nu (x_{i})] \geq 0 \quad \forall i\nonumber
\end{eqnarray}

Note that the random variable \(\Theta\) is the sum of the products of
frequency and severity for losses (in \texttt{OpRisk} there are no
gains).\medskip

This formula is used extensively in actuarial practices, for decisions
relating to quantifying different types of risk, in particular in the
quantification of value-at-risk (VaR) (a risk measure used to determine
capital adequacy requirements, commonly adopted in the banking
industry).\medskip

A quantile of the distribution of the aggregate losses is the level of
exposure to risk, expressed as VaR. People exhibit a specific four-fold
behaviour pattern when facing risk (Shefrin, 2016). There are four
combinations of gain/loss and moderate/extreme probabilities, with two
choices of risk attitude per combination. OpRisk measurement focuses on
only those casual factors that create losses with random uncertainty,
for the value adding processes of the business unit.

\subsection{Cluster analysis}

Cluster analysis (CA) is an unsupervised machine learning technique,
which sets out to group combinations of covariates according to levels
of similarity into clusters. The CA algorithm attempts to optimise
homogeneity within data groups, and heterogeneity between groups of
observations. Thus, in the context of ORM, CA regroups these
combinations of covariates into clusters (so that features within each
group are similar to one another, and different from features in other
groups), ordering and prioritising the root causes of losses.\medskip

A new and challenging argument can be demonstrated through clustering
correlated data objects in the OpRisk dataset, by asserting that
clustering should show more than one distinct group. In addition, the
more groups of distinct clusters, losses are expected to drop, and
losses in distinct clusters should also show a decreasing trend over
time, with intensifying exposure. Ultimately, subtle patterns of
frequencies and associated severities of losses in the OpRisk data can
be revealed.\medskip  

The OpRisk dataset is subdivided for training patterns, validated and
tested with the \emph{k}-means clustering algorithm. To achieve this the
\emph{k}-means algorithm randomly subdivides the data in k groups.
Firstly, each groups mean is found by clustering the centers in the
input variable-space of the training patterns. In each cluster within
each group, the significant variables' coefficients which determine
cluster have set centers closest to the cluster centers generated by the
\emph{k}-means clustering algorithm applied to the input vectors of the
training data (Flake, 1998). These clusters have centers closest:- as
defined by a differential metric i.e., the Euclidean distance, to a
relationship (e.g.~a linear combination of coefficients and variables)
which most accurately predicts the target variable.

\subsection{Research Objective 3}

To identify potential flaws in the loss distribution approach (LDA)
model of ORM by employing CA. The \textit{classical} LDA model, through
a mathematical framework derives a negative pay-off function (loss)
based on a risk-neutral measure \(\mathbb{Q}\). The study addresses
weaknesses in the current LDA model framework, by assuming managerial
risk-taking attitudes are more risk averse.\medskip

More precisely, the goal is to use CA to learn deep hierarchies of
features\footnote{A typical approach taken in the literature is to use an unsupervised learning algorithm to train a model of the unlabeled data and then use the results to extract interesting features from the data [@coates2012learning]}
found during operations, to then determine whether risk adverse
techniques over-compensate for persistent loss event types over time.

\textbf{\section{Description of the dataset}}

The characteristics of the traded transactions or of the associated risk
correction event are given by the following variables: Trade,
UpdateTime, UpdatedDay, TradedTime, TradedDay, Desk, CapturedBy,
TradeStatus, TraderId, Instrument, Reason behind the risk correction
event, Nominal, FloatRef floating rate reference for fixed income
products, ResetDate and ResetRate, Theta, Loss severity, four
EventTypeCategoryLevel viz., EL1 - IF, EL4 - CPBP, EL6 - BDSF, and EL7 -
EDPM \& all seven associated BusinessLineLevel, and the LossIndicator.
The exposure variable shows the length of the time interval from the
initial moment when the risk event happened, until the occurrence of a
risk correction.\medskip

The data is limited to the training dataset over the interval 01 January
- 31 March 2013, in Figure \ref{Fig4}, portrays detail of the trend of
OpRisk losses against exposures for each of the 1631 observations and 16
variables. In the first plot, transactions with small exposures are
concentrated in the first quadrant where HFLS losses persist. This is in
line with the sentiment in risk management circles, that small exposures
are not actively managed and hence risk mitigation is not a priority. As
a result many of the unforeseeable LFHS losses occur here, as they are
not anticipated and therefore slip through OpRisk defences, who more
often than not, do not mitigate against these events.\medskip

Loss severities decrease with increasing exposures, as seen by the
lowering variabilities (and colour concentration of the exposure)
between loses and exposures. This support the view that more impactful
past losses invoke active risk management and mitigation, as risk
managers overcompensate for these severities in their management
practices i.e., they are more risk averse. In addition there are
graphically displayed correlations (which work for numerical explanatory
variables only), which are ordered by their strengths. There is a weak
positive relationship between exposure and UpdatedDay, TradedTime \&
TradedDay; a weak negative relationship with UpdatedTime.

\textbf{\section{Exploratory data analysis}}

\begin{figure}
\begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{OpRisk loss severities vs exposure} & \textbf{Ordered correlations by strength}\\
        \includegraphics[width=7cm]{Loss_vs_Exposure(1).eps}
         &
         \includegraphics[width=7cm]{CorrPlot.eps}
         \end{tabular}
    \end{frame}
    \caption{Graphically displayed correlations by strength and a plot of OpRisk loss severities vs exposure}
    \label{Fig4}
\end{figure}

\subsection{The estimation of \emph{k}-means clustering algorithm}

A cluster analysis will identify groups within a dataset. The target
variable is LossIndicator, a binary variable indicating a \(1\) if a
realised loss occurs and \(0\) for those pending or near misses. The
\emph{K}-means clustering algorithm will search for K clusters
(specified by the user). The resulting \emph{k} clusters are represented
by the mean or average values of each of the variables. Let us consider
a model where the LossIndicator is the target variable: The user whose
task it is to specify \emph{k}, may guess right or in practice they may
obtain a priori, the knowledge of how to select the appropriate \emph{k}
in advance.\medskip

Rather than the trial and error method which involves guessing \emph{k}
values and successively computing minimum separation between centers,
there are several data mining techniques found in the literature, that
can be used to determine the optimal \emph{k} (Rousseeuw, 1987). The
output plot for the estimation of the optimal \emph{k} is presented in
Figure \ref{Fig5} below. We have iterated over cluster sizes from 2 to
10 clusters. The program KMeans resets the random number seed to obtain
the same results each time. where the optimal \emph{k} found to be
significant close to \(\emph{k} = 10\).\medskip

The plot displays the `sum(withinss)' for each clustering and the change
in this value from the previous clustering. The Sum(WithinSS) (blue
line) as a performance metric indicates that beyond \emph{k} = 4
clusters the model overfits: Its computes the absolute error which is
initially large, then monotonicaly decreases to the point \emph{k} = 4,
it then begins to increase subsequent to the point where the
Diffprevious Sum(WithinSS) (red line) intersects viz., at \emph{k} = 4
clusters, which means \emph{k} = 4 is the local optimal number of
clusters i.e., beyond which the iterative relative errors converges
faster than the absolute errors and successively reduces as \emph{k}
increases from 4 to 10.

\begin{figure}
\centering
\includegraphics[width=15cm, height=7.5cm]{IterateKmeans.eps}
\caption{Finding the optimal number of \emph{k} groups by the Silhouette Statistic SS: Sum is a  measure to approximate the optimal number of \emph{k} groups by the Silhouette Statistic SS}
\label{IterateKmeans}
\end{figure}

\subsubsection{Rattle program code}

\subsubsection{Results}\begin{verbatim}
Cluster sizes:

[1] "478 404 570 179"

Data means:

      Trade  UpdatedDay UpdatedTime   TradedDay  TradedTime 
0.762016409 0.448559166 0.486589314 0.487369712 0.601539912 
       Loss    exposure 
0.003232348 0.121083376 

Cluster centers:

      Trade UpdatedDay UpdatedTime TradedDay TradedTime        Loss
1 0.8106844  0.3943515   0.4123358 0.2912134  0.8556825 0.004692829
2 0.8716248  0.4900990   0.5409218 0.7948845  0.8270263 0.002132631
3 0.8378683  0.4493567   0.5264944 0.4160234  0.2165842 0.002308103
4 0.1431301  0.4970205   0.4351758 0.5443203  0.6397973 0.004757466
    exposure
1 0.08060460
2 0.06359981
3 0.07134609
4 0.51729829

Within cluster sum of squares:

[1]  84.88017  89.27845 148.89661  59.37208

Time taken: 1.86 secs

Rattle timestamp: 2018-12-13 07:22:48 User
\end{verbatim}

\begin{sidewaysfigure}
\centering
\includegraphics[width=22.5cm, height=15cm]{CA14MeansPlot.eps}
\caption{A scatterplot matrix for the \emph{k}-means clustering of size 4, and the covariates of frequency loss events consisting of 369 loss event frequencies amounting to R 61 534 745 P\&L severity of loss impact.}
\label{CA14MeansPlot}
\end{sidewaysfigure}

\singlespacing

\FloatBarrier

\newpage

\fancyhead[L]{Results} \fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{RESULTS}

\begin{quote}
\emph{The power of intuitive understanding will protect you from harm until the end of your days.}
--- Lao Tzu
\end{quote}

\doublespacing

\section{Introduction}\label{introduction-1}

\section{Results}\label{results-2}

\begin{verbatim}
## Error in eval(expr, envir, enclos): object 'da36361.0001' not found
\end{verbatim}

\begin{verbatim}
## Error in .f(.x[[i]], ...): object 'cigrec' not found
\end{verbatim}

\begin{verbatim}
## Error: Column indexes must be at most 15 if positive, not 16, 17, 18
\end{verbatim}

\begin{verbatim}
## Error: Column indexes must be at most 15 if positive, not 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36
\end{verbatim}

\begin{verbatim}
## Error in mutate_impl(.data, dots): Evaluation error: object 'cigrec' not found.
\end{verbatim}

\begin{verbatim}
## Error in library(here): there is no package called 'here'
\end{verbatim}

\begin{verbatim}
## Error in here("Data/NSDUH_2014_Results.rda"): could not find function "here"
\end{verbatim}

\begin{verbatim}
## Error in library(survey): there is no package called 'survey'
\end{verbatim}

\begin{verbatim}
## Error in svydesign(ids = ~1, strata = ~vestr, weights = ~analwt_c, data = d1): could not find function "svydesign"
\end{verbatim}

\begin{verbatim}
## Error in svyglm(self ~ religious + age2 + irsex + newrace2 + irfamin3 + : could not find function "svyglm"
\end{verbatim}

\begin{verbatim}
## Error in svyglm(peer ~ religious + age2 + irsex + newrace2 + irfamin3 + : could not find function "svyglm"
\end{verbatim}

\begin{verbatim}
## Error in svyglm(dep ~ religious + age2 + irsex + newrace2 + irfamin3 + : could not find function "svyglm"
\end{verbatim}

\begin{verbatim}
## Error in coef(obj): object 'svy_a1' not found
\end{verbatim}

\begin{verbatim}
## Error in rownames(est1) = c("Respondent", "Peer", "Depression"): object 'est1' not found
\end{verbatim}

\begin{verbatim}
## Error in data.frame(est1): object 'est1' not found
\end{verbatim}

\begin{verbatim}
## Error in svyglm(model, design = design, family = "quasibinomial"): could not find function "svyglm"
\end{verbatim}

\begin{verbatim}
## Error in vcov.default(object): object does not have variance-covariance matrix
\end{verbatim}

\begin{verbatim}
## Error in rownames(est2) = c("Tobacco", "Rx", "Marijuana", "Illicit"): object 'est2' not found
\end{verbatim}

\begin{verbatim}
## Error in data.frame(est2): object 'est2' not found
\end{verbatim}

\begin{verbatim}
## Error in loadNamespace(name): there is no package called 'anteo'
\end{verbatim}

\section{Conclusions}\label{conclusions}

\singlespacing

\FloatBarrier

\newpage

\fancyhead[L]{Discussion} \fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter{DISCUSSION}

\begin{quote}
\emph{A model is a simplification or approximation of reality and hence will not reflect all of reality. ... While a model can never be ``truth," a model might be ranked from very useful, to useful, to somewhat useful, to, finally, essentially useless.}
--- Burnham and Anderson, 2002
\end{quote}

\doublespacing

\section{General Discussion}\label{general-discussion}

\subsection{Findings from the Three
Chapters}\label{findings-from-the-three-chapters}

\section{Limitations}\label{limitations}

\section{Future Research}\label{future-research}

\section{Conclusions}\label{conclusions-1}

\singlespacing

\FloatBarrier

\newpage

\fancyhead[L]{References} \fancyhead[R]{\thepage} \fancyfoot[C]{}

\chapter*{REFERENCES}

\setlength{\parindent}{-0.5in} \setlength{\leftskip}{0.4in}
\setlength{\parskip}{6pt} \noindent

\hypertarget{refs}{}
\hypertarget{ref-acharyya2012current}{}
Acharyya, M. (2012). Why the current practice of operational risk
management in insurance is fundamentally flawed: Evidence from the
field. In \emph{ERM symposium, april} (pp. 18--20).

\hypertarget{ref-agostini2010combining}{}
Agostini, A., Talamo, P., and Vecchione, V. (2010). Combining
operational loss data with expert opinions through advanced credibility
theory. \emph{The Journal of Operational Risk}, \emph{5}(1), 3.

\hypertarget{ref-altman2008behavioral}{}
Altman, M. (2008). Behavioral economics, economic theory and public
policy.

\hypertarget{ref-aue2006lda}{}
Aue, F., and Kalkbrener, M. (2006). LDA at work: Deutsche bank's
approach to quantifying operational risk. \emph{Journal of Operational
Risk}, \emph{1}(4), 49--93.

\hypertarget{ref-badescu2015modeling}{}
Badescu, A. L., Lan, G., Lin, X. S., and Tang, D. (2015). Modeling
correlated frequencies with application in operational risk management.

\hypertarget{ref-barberis2003survey}{}
Barberis, N., and Thaler, R. (2003). A survey of behavioral finance.
\emph{Handbook of the Economics of Finance}, \emph{1}, 1053--1128.

\hypertarget{ref-Burnham2002}{}
Burnham, K., and Anderson, D. (2002). \emph{Model selection and
multimodel inference: A practical information-theoretic approach}.
Springer-Verlag.

\hypertarget{ref-cameron2013regression}{}
Cameron, A. C., and Trivedi, P. K. (2013). \emph{Regression analysis of
count data} (Vol. 53). Cambridge university press.

\hypertarget{ref-chau2014robust}{}
Chau, V. (2014). \emph{Robust estimation in operational risk modeling}
(Master's thesis).

\hypertarget{ref-chavez2016extreme}{}
Chavez-Demoulin, V., Embrechts, P., and Hofert, M. (2016). An extreme
value approach for modeling operational risk losses depending on
covariates. \emph{Journal of Risk and Insurance}, \emph{83}(3),
735--776.

\hypertarget{ref-chavez2006quantitative}{}
Chavez-Demoulin, V., Embrechts, P., and Nešlehová, J. (2006).
Quantitative models for operational risk: Extremes, dependence and
aggregation. \emph{Journal of Banking \& Finance}, \emph{30}(10),
2635--2658.

\hypertarget{ref-basel2010basel}{}
Committee, B., and others. (2010). Basel iii: A global regulatory
framework for more resilient banks and banking systems. \emph{Basel
Committee on Banking Supervision, Basel}.

\hypertarget{ref-basel2011operational}{}
Committee, B., and others. (2011). Operational risk--Supervisory
guidelines for the advanced measurement approaches. \emph{Basel: Bank
for International Settlements}.

\hypertarget{ref-covrig2015using}{}
Covrig, M., Mircea, I., Zbaganu, G., Coser, A., Tindeche, A., and
others. (2015). Using r in generalized linear models. \emph{Romanian
Statistical Review}, \emph{63}(3), 33--45.

\hypertarget{ref-cruz2002modeling}{}
Cruz, M. G. (2002). \emph{Modeling, measuring and hedging operational
risk}. John Wiley \& Sons New York,

\hypertarget{ref-de2008generalized}{}
De Jong, P., Heller, G. Z., and others. (2008). Generalized linear
models for insurance data. \emph{Cambridge Books}.

\hypertarget{ref-denuit2007actuarial}{}
Denuit, M., Maréchal, X., Pitrebois, S., and Walhin, J.-F. (2007).
\emph{Actuarial modelling of claim counts: Risk classification,
credibility and bonus-malus systems}. John Wiley \& Sons.

\hypertarget{ref-mysis2013}{}
Dorval, M. (2013). \emph{Achieving Basel III compliance: how to tackle
it and business issues} (pp. 1--12). Retrieved from
\url{http://www.risktech-forum.com/research/achieving-basel-iii-compliance-how-to-tackle-it-and-business-issues}

\hypertarget{ref-einemann2018operational}{}
Einemann, M., Fritscher, J., and Kalkbrener, M. (2018). Operational risk
measurement beyond the loss distribution approach: An exposure-based
methodology.

\hypertarget{ref-embrechts2002correlation}{}
Embrechts, P., McNeil, A., and Straumann, D. (2002). Correlation and
dependence in risk management: Properties and pitfalls. \emph{Risk
Management: Value at Risk and Beyond}, \emph{1}, 176--223.

\hypertarget{ref-embrechts2018modeling}{}
Embrechts, P., Mizgier, K. J., and Chen, X. (2018). Modeling operational
risk depending on covariates. an empirical investigation.

\hypertarget{ref-flake1998square}{}
Flake, G. W. (1998). Square unit augmented radially extended multilayer
perceptrons. In \emph{Neural networks: Tricks of the trade} (pp.
145--163). Springer.

\hypertarget{ref-frachot2001loss}{}
Frachot, A., Georges, P., and Roncalli, T. (2001). Loss distribution
approach for operational risk.

\hypertarget{ref-frees2010household}{}
Frees, E. W., and Sun, Y. (2010). Household life insurance demand: A
multivariate two-part model. \emph{North American Actuarial Journal},
\emph{14}(3), 338--354.

\hypertarget{ref-friedman1948utility}{}
Friedman, M., and Savage, L. J. (1948). The utility analysis of choices
involving risk. \emph{Journal of Political Economy}, \emph{56}(4),
279--304.

\hypertarget{ref-galloppo2014review}{}
Galloppo, G., and Previati, D. (2014). A review of methods for combining
internal and external data.

\hypertarget{ref-gigerenzer2009homo}{}
Gigerenzer, G., and Brighton, H. (2009). Homo heuristicus: Why biased
minds make better inferences. \emph{Topics in Cognitive Science},
\emph{1}(1), 107--143.

\hypertarget{ref-gisiger2010risk}{}
Gisiger, N. (2010). Risk-neutral probabilities explained.

\hypertarget{ref-hemrit2012major}{}
Hemrit, W., and Arab, M. B. (2012). The major sources of operational
risk and the potential benefits of its management. \emph{The Journal of
Operational Risk}, \emph{7}(3), 71--92.

\hypertarget{ref-hoohlo2015new}{}
Hoohlo, M. (2015). \emph{A new internal data measure for operational
risk: A case study of a south african bank} (PhD thesis).

\hypertarget{ref-de2015combining}{}
Jongh, R. de, De Wet, T., Raubenheimer, H., and Venter, J. H. (2015).
Combining scenario and historical data in the loss distribution
approach: A new procedure that incorporates measures of agreement
between scenarios and historical data.

\hypertarget{ref-kahneman2003perspective}{}
Kahneman, D. (2003). A perspective on judgment and choice: Mapping
bounded rationality. \emph{American Psychologist}, \emph{58}(9), 697.

\hypertarget{ref-kahneman2013prospect}{}
Kahneman, D., and Tversky, A. (2013). Prospect theory: An analysis of
decision under risk. In \emph{Handbook of the fundamentals of financial
decision making: Part i} (pp. 99--127). World Scientific.

\hypertarget{ref-king2001operational}{}
King, J. L. (2001). Operational risk: Measurement and modelling (the
wiley finance series).

\hypertarget{ref-kuhnen2005neural}{}
Kuhnen, C. M., and Knutson, B. (2005). The neural basis of financial
risk taking. \emph{Neuron}, \emph{47}(5), 763--770.

\hypertarget{ref-list2004neoclassical}{}
List, J. A. (2004). Neoclassical theory versus prospect theory: Evidence
from the marketplace. \emph{Econometrica}, \emph{72}(2), 615--625.

\hypertarget{ref-mignola2016comments}{}
Mignola, G., Ugoccioni, R., and Cope, E. (2016). Comments on the basel
committee on banking supervision proposal for a new standardized
approach for operational risk.

\hypertarget{ref-morgenstern1953theory}{}
Morgenstern, O., and Von Neumann, J. (1953). \emph{Theory of games and
economic behavior}. Princeton university press.

\hypertarget{ref-opdyke2014estimating}{}
Opdyke, J. D. (2014). Estimating operational risk capital with greater
accuracy, precision, and robustness. \emph{arXiv Preprint
arXiv:1406.0389}.

\hypertarget{ref-panjer2006operational}{}
Panjer, H. H. (2006). \emph{Operational risk: Modeling analytics} (Vol.
620). John Wiley \& Sons.

\hypertarget{ref-parodi2014pricing}{}
Parodi, P. (2014). \emph{Pricing in general insurance}. CRC Press.

\hypertarget{ref-peters2016should}{}
Peters, G., Shevchenko, P. V., Hassani, B., and Chapelle, A. (2016).
Should the advanced measurement approach be replaced with the
standardized measurement approach for operational risk?

\hypertarget{ref-risk2001supporting}{}
Risk, B. O. (2001). Supporting document to the new basel capital accord.
\emph{Consultative Document, January}, \emph{200}.

\hypertarget{ref-risk2016supporting}{}
Risk, B. O. (2016). Standardised measurement approach for operational
risk. \emph{Consultative Document, June}.

\hypertarget{ref-rousseeuw1987silhouettes}{}
Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the
interpretation and validation of cluster analysis. \emph{Journal of
Computational and Applied Mathematics}, \emph{20}, 53--65.

\hypertarget{ref-shefrin2016behavioral}{}
Shefrin, H. (2016). \emph{Behavioral risk management: Managing the
psychology that drives decisions and influences operational risk}.
Springer.

\hypertarget{ref-stasinopoulos2017flexible}{}
Stasinopoulos, M. D., Rigby, R. A., Heller, G. Z., Voudouris, V., and De
Bastiani, F. (2017). \emph{Flexible regression and smoothing: Using
gamlss in r}. Chapman; Hall/CRC.

\hypertarget{ref-tom2007neural}{}
Tom, S. M., Fox, C. R., Trepel, C., and Poldrack, R. A. (2007). The
neural basis of loss aversion in decision-making under risk.
\emph{Science}, \emph{315}(5811), 515--518.

\hypertarget{ref-urbina2014application}{}
Urbina, J., and Guillén, M. (2014). An application of capital allocation
principles to operational risk and the cost of fraud. \emph{Expert
Systems with Applications}, \emph{41}(16), 7023--7031.

\hypertarget{ref-wiseman1997longitudinal}{}
Wiseman, R. M., and Catanach Jr, C. (1997). A longitudinal
disaggregation of operational risk under changing regulations: Evidence
from the savings and loan industry. \emph{Academy of Management
Journal}, \emph{40}(4), 799--830.

\hypertarget{ref-wood2017generalized}{}
Wood, S. N. (2017). \emph{Generalized additive models: An introduction
with r}. Chapman; Hall/CRC.

\clearpage
\addcontentsline{toc}{chapter}{APPENDICES} \fancyhead[L]{Appendices}
\fancyhead[R]{\thepage} \fancyfoot[C]{}

\vspace*{\fill}

\begin{center}
    APPENDICES 
  \end{center}

\vspace*{\fill}

\clearpage

--\textgreater{}

\doublespacing

\section*{Appendix A: R Code for Chapter
3}\label{appendix-a-r-code-for-chapter-3}
\addcontentsline{toc}{section}{Appendix A: R Code for Chapter 3}

\singlespace

Required: R Packages from CRAN

\small

\normalsize

Required: R Packages from GitHub

\small

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(MarginalMediation))\{}
\NormalTok{  devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"tysonstanley/MarginalMediation"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(MarginalMediation)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\subsection*{Exploratory Data Analyses}\label{exploratory-data-analyses}
\addcontentsline{toc}{subsection}{Exploratory Data Analyses}

Data Preparations for tables and figures around page

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_loc <-}\StringTok{ "C:/Users/User/Documents/OpRiskPHDGitHub/OpRisk_PHD_Dissertate}
\StringTok{/OpRisk_PHD_Dissertation"}
\KeywordTok{setwd}\NormalTok{(file_loc)}
\KeywordTok{list.files}\NormalTok{(file_loc)}

\NormalTok{frequency <-}\StringTok{ }\NormalTok{openxlsx}\OperatorTok{::}\KeywordTok{read.xlsx}\NormalTok{(}\StringTok{"Raw_Formatted_Data.xlsx"}\NormalTok{, }
                                 \DataTypeTok{check.names =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sheet =} \StringTok{"Frequency"}\NormalTok{)}
\NormalTok{severity <-}\StringTok{ }\NormalTok{openxlsx}\OperatorTok{::}\KeywordTok{read.xlsx}\NormalTok{(}\StringTok{"Raw_Formatted_Data.xlsx"}\NormalTok{, }
                                \DataTypeTok{check.names =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sheet =} \StringTok{"Severity"}\NormalTok{)}
\NormalTok{projdata <-}\StringTok{ }\NormalTok{openxlsx}\OperatorTok{::}\KeywordTok{read.xlsx}\NormalTok{(}\StringTok{"OPriskDataSet_exposure.xlsx"}\NormalTok{, }
                                \DataTypeTok{check.names =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sheet =} \StringTok{"CleanedData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pander}\OperatorTok{::}\KeywordTok{pander}\NormalTok{(}\KeywordTok{tapply}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Loss, }\DataTypeTok{INDEX =}\NormalTok{ projdata}\OperatorTok{$}\NormalTok{Instrument, }\ControlFlowTok{function}\NormalTok{(x)}
  \KeywordTok{c}\NormalTok{(}\DataTypeTok{N =} \KeywordTok{length}\NormalTok{(x), }\DataTypeTok{Mean =} \KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{SD =} \KeywordTok{sd}\NormalTok{(x), }\DataTypeTok{Min =} \KeywordTok{min}\NormalTok{(x), }\DataTypeTok{Max =} \KeywordTok{max}\NormalTok{(x))))}

\NormalTok{tablex <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(}\StringTok{"rbind"}\NormalTok{, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Loss, projdata}\OperatorTok{$}\NormalTok{Instrument),}
\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{c}\NormalTok{(}\DataTypeTok{N =} \KeywordTok{length}\NormalTok{(x), }\DataTypeTok{Mean =} \KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{SD =} \KeywordTok{sd}\NormalTok{(x), }\DataTypeTok{Min =} \KeywordTok{min}\NormalTok{(x),}
              \DataTypeTok{Max =} \KeywordTok{max}\NormalTok{(x))))}
\NormalTok{tablex <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{Instrument =} \KeywordTok{rownames}\NormalTok{(tablex), tablex)}
\NormalTok{tablex <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(tablex)}
\NormalTok{tablex}\OperatorTok{$}\NormalTok{Mean <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(tablex}\OperatorTok{$}\NormalTok{Mean))}

\NormalTok{tablex <-}\StringTok{ }\NormalTok{tablex[}\KeywordTok{order}\NormalTok{(tablex}\OperatorTok{$}\NormalTok{Mean), ]}

\NormalTok{stargazer}\OperatorTok{::}\KeywordTok{stargazer}\NormalTok{(tablex)}

\NormalTok{openxlsx}\OperatorTok{::}\KeywordTok{write.xlsx}\NormalTok{(tablex, }\StringTok{"tablex.xlsx"}\NormalTok{, }\DataTypeTok{rownames =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

Figure on page

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Exploratory data analysis for Update Time}
\NormalTok{### summary statistics}
\KeywordTok{summary}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{UpdatedTime)}
\NormalTok{### Histograms}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)) }
\NormalTok{### ALL Losses}
\KeywordTok{hist}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{UpdatedTime, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"All losses"}\NormalTok{, }\DataTypeTok{xlab =} 
       \StringTok{"Update Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\NormalTok{### Near Misses/Pending Losses}
\KeywordTok{hist}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{UpdatedTime[projdata}\OperatorTok{$}\NormalTok{LossIndicator }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{], }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{main =}
       \StringTok{"Near Misses"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Update Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\NormalTok{### Realised losses}
\KeywordTok{hist}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{UpdatedTime[projdata}\OperatorTok{$}\NormalTok{LossIndicator }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{], }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{, main}
\NormalTok{     =}\StringTok{ "Realised losses"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Update Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{### }
\KeywordTok{plot}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{UpdatedTime, }\KeywordTok{log}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Loss}\OperatorTok{+}\FloatTok{0.000000001}\NormalTok{), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{18}\NormalTok{),}
     \DataTypeTok{col =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Updated Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Log. Loss"}\NormalTok{)}
\KeywordTok{do.call}\NormalTok{(}\StringTok{"rbind"}\NormalTok{, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Loss, projdata}\OperatorTok{$}\NormalTok{UpdatedTime), }
\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{c}\NormalTok{(}\DataTypeTok{N =} \KeywordTok{length}\NormalTok{(x), }\DataTypeTok{Mean =} \KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{SD =} \KeywordTok{sd}\NormalTok{(x), }\DataTypeTok{Min =} \KeywordTok{min}\NormalTok{(x), }\DataTypeTok{Max =} \KeywordTok{max}\NormalTok{(x))))}
\end{Highlighting}
\end{Shaded}

\normalsize

Figures , , and on pages , and , respectively.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Instrument}
\KeywordTok{unique}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Instrument)}
\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{LossIndicator, projdata}\OperatorTok{$}\NormalTok{Instrument)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{LossIndicator, projdata}\OperatorTok{$}\NormalTok{Instrument), }
    \DataTypeTok{main=}\StringTok{"By Instrument"}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{20}\NormalTok{), }\DataTypeTok{las=}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex.axis=}\FloatTok{1.0}\NormalTok{)}

\CommentTok{# Trader}
\KeywordTok{unique}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{TraderId)}
\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{TraderId, projdata}\OperatorTok{$}\NormalTok{LossIndicator)}
\KeywordTok{round}\NormalTok{(}\KeywordTok{addmargins}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{TraderId, projdata}\OperatorTok{$}\NormalTok{LossIndicator)}
\NormalTok{                                                            , }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{LossIndicator, projdata}\OperatorTok{$}\NormalTok{TraderId), }\DataTypeTok{main=}\StringTok{"By Trader"}\NormalTok{,}
                                      \DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{20}\NormalTok{), }\DataTypeTok{las=}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex.axis=}\FloatTok{1.0}\NormalTok{)}

\CommentTok{# Captured By}
\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{CapturedBy, projdata}\OperatorTok{$}\NormalTok{LossIndicator)}
\KeywordTok{round}\NormalTok{(}\KeywordTok{addmargins}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{CapturedBy, projdata}\OperatorTok{$}\NormalTok{LossIndicator)}
\NormalTok{                                                              , }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{table}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{LossIndicator, projdata}\OperatorTok{$}\NormalTok{CapturedBy), }\DataTypeTok{main=}\StringTok{"By Tech Support"}
\NormalTok{                                    , }\DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{20}\NormalTok{), }\DataTypeTok{las=}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex.axis=}\FloatTok{1.0}\NormalTok{)}

\KeywordTok{do.call}\NormalTok{(}\StringTok{"rbind"}\NormalTok{, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(projdata}\OperatorTok{$}\NormalTok{Loss, projdata}\OperatorTok{$}\NormalTok{CapturedBy), }\ControlFlowTok{function}\NormalTok{(x)}
  \KeywordTok{c}\NormalTok{(}\DataTypeTok{N =} \KeywordTok{length}\NormalTok{(x), }\DataTypeTok{Mean =} \KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{SD =} \KeywordTok{sd}\NormalTok{(x), }\DataTypeTok{Min =} \KeywordTok{min}\NormalTok{(x), }\DataTypeTok{Max =} \KeywordTok{max}\NormalTok{(x))))}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\subsection*{Examples from Chapter 3}\label{examples-from-chapter-3}
\addcontentsline{toc}{subsection}{Examples from Chapter 3}

\subsection*{Data Preparation}\label{data-preparation}
\addcontentsline{toc}{subsection}{Data Preparation}

Data preparation using the OpRisk Loss Collection Data Exercise (LCDE),
as described in Chapter 2.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set parameter values}
\NormalTok{crv}\OperatorTok{$}\NormalTok{seed <-}\StringTok{ }\DecValTok{42} \CommentTok{# set random seed to make your partition reproducible}
\NormalTok{crv}\OperatorTok{$}\NormalTok{taining.proportion <-}\StringTok{ }\FloatTok{0.7} \CommentTok{# proportion of data used for training}
\NormalTok{crv}\OperatorTok{$}\NormalTok{validation.proportion <-}\StringTok{ }\FloatTok{0.15} \CommentTok{# proportion of data used for validation}

\CommentTok{# Load data for frequency of LossIndicator analysis}
\NormalTok{d <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"OPriskDataSet_exposure.csv"}\NormalTok{,}
              \DataTypeTok{sep=}\StringTok{";"}\NormalTok{,}
              \DataTypeTok{dec=}\StringTok{","}\NormalTok{,}
              \DataTypeTok{na.strings=}\KeywordTok{c}\NormalTok{(}\StringTok{"."}\NormalTok{, }\StringTok{"NA"}\NormalTok{, }\StringTok{""}\NormalTok{, }\StringTok{"?"}\NormalTok{),}
              \DataTypeTok{strip.white=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{encoding=}\StringTok{"UTF-8"}\NormalTok{)}

\NormalTok{exposure <-}\StringTok{ }\NormalTok{d[,}\KeywordTok{ncol}\NormalTok{(d)] }
\KeywordTok{class}\NormalTok{(exposure)}
\KeywordTok{length}\NormalTok{(exposure)}

\KeywordTok{summary}\NormalTok{(d)}

\NormalTok{d1 <-}\StringTok{ }\NormalTok{d }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(UpdatedDay,}
\NormalTok{           UpdatedTime,}
\NormalTok{           TradedDay,}
\NormalTok{           TradedTime,}
\NormalTok{           Desk,}
\NormalTok{           CapturedBy,}
\NormalTok{           TradeStatus,}
\NormalTok{           TraderId,}
\NormalTok{           Instrument,}
\NormalTok{           Reason,}
\NormalTok{           EventTypeCategoryLevel1,}
\NormalTok{           BusinessLineLevel1) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{transmute}\NormalTok{(}\DataTypeTok{LossesIndicator =}\NormalTok{ LossIndicator,}
            \DataTypeTok{Losses =}\NormalTok{ Loss,}
            \DataTypeTok{exposure =}\NormalTok{ exposure)}

\CommentTok{# Load data for severity of losses analysis}

\NormalTok{D <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"OPriskDataSet_exposure_severity.csv"}\NormalTok{,}
              \DataTypeTok{sep=}\StringTok{";"}\NormalTok{,}
              \DataTypeTok{dec=}\StringTok{","}\NormalTok{,}
              \DataTypeTok{na.strings=}\KeywordTok{c}\NormalTok{(}\StringTok{"."}\NormalTok{, }\StringTok{"NA"}\NormalTok{, }\StringTok{""}\NormalTok{, }\StringTok{"?"}\NormalTok{),}
              \DataTypeTok{strip.white=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{encoding=}\StringTok{"UTF-8"}\NormalTok{)}

\NormalTok{exposure <-}\StringTok{ }\NormalTok{D[,}\KeywordTok{ncol}\NormalTok{(D)] }

\NormalTok{D1 <-}\StringTok{ }\NormalTok{D }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(UpdatedDay,}
\NormalTok{           UpdatedTime,}
\NormalTok{           TradedDay,}
\NormalTok{           TradedTime,}
\NormalTok{           Desk,}
\NormalTok{           CapturedBy,}
\NormalTok{           TradeStatus,}
\NormalTok{           TraderId,}
\NormalTok{           Instrument,}
\NormalTok{           Reason,}
\NormalTok{           EventTypeCategoryLevel1,}
\NormalTok{           BusinessLineLevel1) }\OperatorTok{%>%}\StringTok{ }
\KeywordTok{transmute}\NormalTok{(}\DataTypeTok{LossesIndicator =}\NormalTok{ LossIndicator,}
            \DataTypeTok{Losses =}\NormalTok{ Loss,}
            \DataTypeTok{exposure =}\NormalTok{ exposure)}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\subsection*{GLM Models}\label{glm-models}
\addcontentsline{toc}{subsection}{GLM Models}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{getmode <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  u <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(x)}
  \KeywordTok{as.integer}\NormalTok{(u[}\KeywordTok{which.max}\NormalTok{(}\KeywordTok{tabulate}\NormalTok{(}\KeywordTok{match}\NormalTok{(x,u)))])}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{5}\OperatorTok{:}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(d1) }\OperatorTok{-}\StringTok{ }\DecValTok{3}\NormalTok{))\{}
\NormalTok{     d1[[i]] <-}\StringTok{ }\KeywordTok{relevel}\NormalTok{(d1[[i]], }\KeywordTok{getmode}\NormalTok{(d1[[i]]))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freqfit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(LossesIndicator }\OperatorTok{~}\StringTok{ }\NormalTok{UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay}
               \OperatorTok{+}\StringTok{ }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}
\StringTok{               }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }
\StringTok{               }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1, }\DataTypeTok{data=}\NormalTok{d1,}
               \DataTypeTok{family=}\KeywordTok{poisson}\NormalTok{(}\DataTypeTok{link =} \StringTok{'log'}\NormalTok{), }\DataTypeTok{offset =} \KeywordTok{log}\NormalTok{(exposure))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{na.action=}\NormalTok{na.fail)}
\NormalTok{freqfits <-}\StringTok{ }\KeywordTok{dredge}\NormalTok{(freqfit)}
\NormalTok{adelmodel <-}\StringTok{ }\NormalTok{(}\KeywordTok{model.avg}\NormalTok{(}\KeywordTok{get.models}\NormalTok{(freqfits, }\DataTypeTok{subset=}\NormalTok{delta}\OperatorTok{<}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\subsection*{GAMLSS Model}\label{gamlss-model}
\addcontentsline{toc}{subsection}{GAMLSS Model}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf <-}\StringTok{ }\KeywordTok{gamlss}\NormalTok{(Losses}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime}
      \OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason}
      \OperatorTok{+}\StringTok{ }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1), }
\DataTypeTok{sigma.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk}
      \OperatorTok{+}\StringTok{ }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}
\StringTok{        }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1),}
\DataTypeTok{nu.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}\StringTok{ }
\StringTok{      }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}\StringTok{ }
\StringTok{      }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1),}
 \DataTypeTok{tau.formula=}\OperatorTok{~}\KeywordTok{cs}\NormalTok{(UpdatedDay }\OperatorTok{+}\StringTok{ }\NormalTok{UpdatedTime }\OperatorTok{+}\StringTok{ }\NormalTok{TradedDay }\OperatorTok{+}\StringTok{ }\NormalTok{TradedTime }\OperatorTok{+}\StringTok{ }\NormalTok{Desk }\OperatorTok{+}
\StringTok{      }\NormalTok{CapturedBy }\OperatorTok{+}\StringTok{ }\NormalTok{TradeStatus }\OperatorTok{+}\StringTok{ }\NormalTok{TraderId }\OperatorTok{+}\StringTok{ }\NormalTok{Instrument }\OperatorTok{+}\StringTok{ }\NormalTok{Reason }\OperatorTok{+}
\StringTok{      }\NormalTok{EventTypeCategoryLevel1 }\OperatorTok{+}\StringTok{ }\NormalTok{BusinessLineLevel1),}
\DataTypeTok{data=}\NormalTok{D1, }\DataTypeTok{mu.start =} \OtherTok{NULL}\NormalTok{,  }\DataTypeTok{sigma.start =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{nu.start =} \OtherTok{NULL}\NormalTok{,}
                                            \DataTypeTok{tau.start =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{family=}\NormalTok{BCPE)}
\end{Highlighting}
\end{Shaded}

\normalsize
\clearpage

\doublespacing

\section*{Appendix B: R Code for Chapter
6}\label{appendix-b-r-code-for-chapter-6}
\addcontentsline{toc}{section}{Appendix B: R Code for Chapter 6}

\singlespace

Required: R Packages from CRAN

\small

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(tidyverse))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(furniture))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"furniture"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(furniture)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(here))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"here"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(here)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(devtools))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(devtools)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(survey))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"survey"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(survey)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Required: R Packages from GitHub

\small

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(MarginalMediation))\{}
\NormalTok{  devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"tysonstanley/MarginalMediation"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(MarginalMediation)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\subsection*{Extrapolation code for simulation in
matlab}\label{extrapolation-code-for-simulation-in-matlab}
\addcontentsline{toc}{subsection}{Extrapolation code for simulation in
matlab}

Notably, the code for the predict condition was run via the Matlab
Terminal:

\small

\begin{Shaded}
\begin{Highlighting}[]

\ExtensionTok{%}\NormalTok{ Updated Time}
\ExtensionTok{%}\NormalTok{ generate the vector DD}

\ExtensionTok{DDD}\NormalTok{ = 1:31}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ generate the vector VVV}

\ExtensionTok{VVV}\NormalTok{ = 1:12}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ generate the vector UUU}

\ExtensionTok{UUU}\NormalTok{ = 2013 : -1 : 2006}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ making the full thrity five days vector}
\ExtensionTok{%}\NormalTok{ Years}
\ExtensionTok{Thirty_five_days}\NormalTok{ = [UUU}\StringTok{';UUU'}\KeywordTok{;}\ExtensionTok{UUU}\StringTok{';UUU(1:end-1)'}\NormalTok{]}\KeywordTok{;}
\ExtensionTok{%Months}
\ExtensionTok{Thirty_five_days2}\NormalTok{ = [VVV}\StringTok{';VVV'}\KeywordTok{;}\ExtensionTok{VVV}\NormalTok{(1:7)}\StringTok{'];}
\StringTok{% Days}
\StringTok{Thirty_five_days3 = DDD'}\NormalTok{;}

\ExtensionTok{%}\NormalTok{ The updated time algorithm}

\ExtensionTok{%}\NormalTok{ initializing the time matrix}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUU)}
    
    \ExtensionTok{UUU_trans}\DataTypeTok{\{i\}}\NormalTok{ = num2cell(zeros(1,12));}
    
\ExtensionTok{end}

\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUU)}
    \KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(UUU_trans}\DataTypeTok{\{1,1\}}\NormalTok{)}
        
        \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{ = num2cell(zeros(31,4));}
    \ExtensionTok{end}
    
\ExtensionTok{end}

\ExtensionTok{%}\NormalTok{ The number of random numbers}
\ExtensionTok{H}\NormalTok{ = 1000}\KeywordTok{;}
\ExtensionTok{UPD}\NormalTok{ =.789155092592539}\KeywordTok{;}
\ExtensionTok{format}\NormalTok{ long}
\ExtensionTok{%}\NormalTok{ filling in the time matrix}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUU)}
    \KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(UUU_trans}\DataTypeTok{\{1,1\}}\NormalTok{)}
        
        \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(:,end) = }\ExtensionTok{num2cell}\NormalTok{((1:31)}\StringTok{');}
\StringTok{        UUU_TRANS\{1,i\}\{1,j\}(:,end-1) = num2cell(VVV(j));}
\StringTok{        }
\StringTok{        }
\StringTok{        }
\StringTok{    end}
\StringTok{    }
\StringTok{end}

\StringTok{UUU = num2cell(UUU);}
\StringTok{%         UUU = sortrows(UUU,2);}


\StringTok{for i = 1 : length(UUU)}
\StringTok{    for j = 1 : length(UUU_trans\{1,1\})}
\StringTok{        for k = 1 : length(UUU_TRANS\{1,5\}\{1,1\})}
\StringTok{            % PART 1}
\StringTok{            UUU\{i,j\} =num2cell((((i)^(0)).*((j)^(0)).*rand(1,31)));}
\StringTok{            UUU\{i,j\} = UUU\{i,j\}'}\NormalTok{;}
            \ExtensionTok{UUU}\DataTypeTok{\{i,j\}}\NormalTok{(:,2) = }\ExtensionTok{num2cell}\NormalTok{(Thirty_five_days(:,1));}
            \ExtensionTok{UUU}\DataTypeTok{\{i,j\}}\NormalTok{ = sortrows(UUU}\DataTypeTok{\{i,j\}}\NormalTok{,1);}
            
            \ExtensionTok{%PART}\NormalTok{ 2}
            \ExtensionTok{UUU2}\DataTypeTok{\{i,j\}}\NormalTok{ =num2cell((((i)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*}\NormalTok{((j)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*rand}\NormalTok{(1,31)));}
            \ExtensionTok{UUU2}\DataTypeTok{\{i,j\}}\NormalTok{ = UUU2}\DataTypeTok{\{i,j\}}\StringTok{';}
\StringTok{            UUU2\{i,j\}(:,2) = num2cell(Thirty_five_days2(:,1));}
\StringTok{            UUU2\{i,j\} = sortrows(UUU2\{i,j\},1);}
\StringTok{            % PART 3}
\StringTok{            UUU3\{i,j\} =num2cell((((i)^(0)).*((j)^(0)).*rand(1,31)));}
\StringTok{            UUU3\{i,j\} = UUU3\{i,j\}'}\KeywordTok{;}
            \ExtensionTok{UUU3}\DataTypeTok{\{i,j\}}\NormalTok{(:,2) = }\ExtensionTok{num2cell}\NormalTok{(Thirty_five_days3(:,1));}
            \ExtensionTok{UUU3}\DataTypeTok{\{i,j\}}\NormalTok{ = sortrows(UUU3}\DataTypeTok{\{i,j\}}\NormalTok{,1);}
            \ExtensionTok{%}\NormalTok{ PART 1}
            \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-3) = }\ExtensionTok{UUU}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            
            \ExtensionTok{%}\NormalTok{ PART 2}
            \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-2) = }\ExtensionTok{UUU2}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            \ExtensionTok{%PART3}
            \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-1) = }\ExtensionTok{UUU3}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            \ExtensionTok{rH}\DataTypeTok{\{i,j\}}\NormalTok{ = num2cell(((i)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\ExtensionTok{.*}\NormalTok{(j)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*rand}\NormalTok{(H,1));}
            \ExtensionTok{yH}\DataTypeTok{\{i,j\}}\NormalTok{ = rH}\DataTypeTok{\{i,j\}}\NormalTok{(cell2mat( rH}\DataTypeTok{\{i,j\}}\NormalTok{) }\OperatorTok{<}\NormalTok{= }\ExtensionTok{UPD}\NormalTok{);}
            \ExtensionTok{gH}\DataTypeTok{\{i,j\}}\NormalTok{ = num2cell(cell2mat( yH}\DataTypeTok{\{i,j\}}\NormalTok{(1:31)));}
            \ExtensionTok{UUU_TRANS}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end) = }\ExtensionTok{gH}\DataTypeTok{\{i,j\}}\NormalTok{(k,1);}
        \ExtensionTok{end}
    \ExtensionTok{end}
    
\ExtensionTok{end}

\ExtensionTok{UPDATED_TIME}\NormalTok{ = UUU_TRANS}\KeywordTok{;}


\ExtensionTok{%%}\NormalTok{ Traded time}

\ExtensionTok{%}\NormalTok{ generate the vector DD}

\ExtensionTok{DDDT}\NormalTok{ = 1:31}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ generate the vector VVV}

\ExtensionTok{VVVT}\NormalTok{ = 1:12}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ generate the vector UUU}

\ExtensionTok{UUUT}\NormalTok{ = 2013 : -1 : 2006}\KeywordTok{;}

\ExtensionTok{%}\NormalTok{ making the full thrity five days vector}
\ExtensionTok{%}\NormalTok{ Years}
\ExtensionTok{Thirty_five_daysT}\NormalTok{ = [UUUT}\StringTok{';UUUT'}\KeywordTok{;}\ExtensionTok{UUUT}\StringTok{';UUUT(1:end-1)'}\NormalTok{]}\KeywordTok{;}
\ExtensionTok{%Months}
\ExtensionTok{Thirty_five_days2T}\NormalTok{ = [VVVT}\StringTok{';VVVT'}\KeywordTok{;}\ExtensionTok{VVVT}\NormalTok{(1:7)}\StringTok{'];}
\StringTok{% Days}
\StringTok{Thirty_five_days3T = DDDT'}\NormalTok{;}

\ExtensionTok{%}\NormalTok{ The updated time algorithm}

\ExtensionTok{%}\NormalTok{ initializing the time matrix}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUUT)}
    
    \ExtensionTok{UUU_transT}\DataTypeTok{\{i\}}\NormalTok{ = num2cell(zeros(1,12));}
    
\ExtensionTok{end}

\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUUT)}
    \KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(UUU_transT}\DataTypeTok{\{1,1\}}\NormalTok{)}
        
        \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{ = num2cell(zeros(31,4));}
    \ExtensionTok{end}
    
\ExtensionTok{end}

\ExtensionTok{%}\NormalTok{ The number of random numbers}
\ExtensionTok{HT}\NormalTok{ = 1000}\KeywordTok{;}
\ExtensionTok{UPDT}\NormalTok{ =.789155092592539}\KeywordTok{;}
\ExtensionTok{format}\NormalTok{ long}
\ExtensionTok{%}\NormalTok{ filling in the time matrix}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : length(UUUT)}
    \KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(UUU_transT}\DataTypeTok{\{1,1\}}\NormalTok{)}
        
        \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(:,end) = }\ExtensionTok{num2cell}\NormalTok{((1:31)}\StringTok{');}
\StringTok{        UUU_TRANST\{1,i\}\{1,j\}(:,end-1) = num2cell(VVVT(j));}
\StringTok{        }
\StringTok{        }
\StringTok{        }
\StringTok{    end}
\StringTok{    }
\StringTok{end}

\StringTok{UUUT = num2cell(UUUT);}
\StringTok{%         UUU = sortrows(UUU,2);}


\StringTok{for i = 1 : length(UUUT)}
\StringTok{    for j = 1 : length(UUU_transT\{1,1\})}
\StringTok{        for k = 1 : length(UUU_TRANST\{1,5\}\{1,1\})}
\StringTok{            % PART 1}
\StringTok{            UUUT\{i,j\} =num2cell((((i)^(0)).*((j)^(0)).*rand(1,31)));}
\StringTok{            UUUT\{i,j\} = UUUT\{i,j\}'}\NormalTok{;}
            \ExtensionTok{UUUT}\DataTypeTok{\{i,j\}}\NormalTok{(:,2) = }\ExtensionTok{num2cell}\NormalTok{(Thirty_five_daysT(:,1));}
            \ExtensionTok{UUUT}\DataTypeTok{\{i,j\}}\NormalTok{ = sortrows(UUUT}\DataTypeTok{\{i,j\}}\NormalTok{,1);}
            
            \ExtensionTok{%PART}\NormalTok{ 2}
            \ExtensionTok{UUU2T}\DataTypeTok{\{i,j\}}\NormalTok{ =num2cell((((i)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*}\NormalTok{((j)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*rand}\NormalTok{(1,31)));}
            \ExtensionTok{UUU2T}\DataTypeTok{\{i,j\}}\NormalTok{ = UUU2T}\DataTypeTok{\{i,j\}}\StringTok{';}
\StringTok{            UUU2T\{i,j\}(:,2) = num2cell(Thirty_five_days2T(:,1));}
\StringTok{            UUU2T\{i,j\} = sortrows(UUU2T\{i,j\},1);}
\StringTok{            % PART 3}
\StringTok{            UUU3T\{i,j\} =num2cell((((i)^(0)).*((j)^(0)).*rand(1,31)));}
\StringTok{            UUU3T\{i,j\} = UUU3T\{i,j\}'}\KeywordTok{;}
            \ExtensionTok{UUU3T}\DataTypeTok{\{i,j\}}\NormalTok{(:,2) = }\ExtensionTok{num2cell}\NormalTok{(Thirty_five_days3T(:,1));}
            \ExtensionTok{UUU3T}\DataTypeTok{\{i,j\}}\NormalTok{ = sortrows(UUU3T}\DataTypeTok{\{i,j\}}\NormalTok{,1);}
            \ExtensionTok{%}\NormalTok{ PART 1}
            \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-3) = }\ExtensionTok{UUUT}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            
            \ExtensionTok{%}\NormalTok{ PART 2}
            \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-2) = }\ExtensionTok{UUU2T}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            \ExtensionTok{%PART3}
            \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end-1) = }\ExtensionTok{UUU3T}\DataTypeTok{\{i,j\}}\NormalTok{(k,2);}
            \ExtensionTok{rHT}\DataTypeTok{\{i,j\}}\NormalTok{ = num2cell(((i)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\ExtensionTok{.*}\NormalTok{(j)^}\KeywordTok{(}\ExtensionTok{0}\KeywordTok{)}\NormalTok{)}\ExtensionTok{.*rand}\NormalTok{(HT,1));}
            \ExtensionTok{yHT}\DataTypeTok{\{i,j\}}\NormalTok{ = rHT}\DataTypeTok{\{i,j\}}\NormalTok{(cell2mat( rHT}\DataTypeTok{\{i,j\}}\NormalTok{) }\OperatorTok{<}\NormalTok{= }\ExtensionTok{UPDT}\NormalTok{);}
            \ExtensionTok{gHT}\DataTypeTok{\{i,j\}}\NormalTok{ = num2cell(cell2mat( yHT}\DataTypeTok{\{i,j\}}\NormalTok{(1:31)));}
            \ExtensionTok{UUU_TRANST}\DataTypeTok{\{1,i\}\{1,j\}}\NormalTok{(k,end) = }\ExtensionTok{gHT}\DataTypeTok{\{i,j\}}\NormalTok{(k,1);}
        \ExtensionTok{end}
    \ExtensionTok{end}
    
\ExtensionTok{end}

\ExtensionTok{TRADED_TIME}\NormalTok{ = UUU_TRANST}\KeywordTok{;}

\ExtensionTok{%%}\NormalTok{ RULE for correcting the traded time}
\ExtensionTok{SIZZZE}\NormalTok{ = size(UUU_TRANS}\DataTypeTok{\{1,3\}\{1,2\}}\NormalTok{);}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : 8}
    \KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(UUU_transT}\DataTypeTok{\{1,1\}}\NormalTok{)}
        \KeywordTok{for} \ExtensionTok{k}\NormalTok{ = 1 : length(UUU_TRANST}\DataTypeTok{\{1,5\}\{1,1\}}\NormalTok{)}
            
            
            \KeywordTok{if}  \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}
                
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\KeywordTok{;}
            \ExtensionTok{end}
                
            \KeywordTok{if} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}
                
                
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\KeywordTok{;}
            \ExtensionTok{end}
                
                
                
            \KeywordTok{if} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}
                
                
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}\KeywordTok{;}
            \ExtensionTok{end}
                
                
            \KeywordTok{if} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}\NormalTok{...}
                    \KeywordTok{&&} \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,4\}} \OperatorTok{>}\NormalTok{= UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,4\}}
                
                
                
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,1\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,2\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,3\}}\KeywordTok{;}
                \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,4\}}\NormalTok{ = UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,j\}\{k,4\}}\KeywordTok{;}
                
            \ExtensionTok{end}
        \ExtensionTok{end}
    \ExtensionTok{end}
    
\ExtensionTok{end}

\ExtensionTok{%%}\NormalTok{ The traded time table}
\ExtensionTok{%}\NormalTok{ Fill the updated time}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : 8}
    
   \ExtensionTok{TABLE_TRADED_TIME}\DataTypeTok{\{1,i\}}\NormalTok{ = vertcat(TRADED_TIME}\DataTypeTok{\{1,i\}\{1,1\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,2\}}\NormalTok{, TRADED_TIME}\DataTypeTok{\{1,i\}\{1,3\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,4\}}\NormalTok{, TRADED_TIME}\DataTypeTok{\{1,i\}\{1,5\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,6\}}\NormalTok{, TRADED_TIME}\DataTypeTok{\{1,i\}\{1,7\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,8\}}\NormalTok{, TRADED_TIME}\DataTypeTok{\{1,i\}\{1,9\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,10\}}\NormalTok{, TRADED_TIME}\DataTypeTok{\{1,i\}\{1,11\}}\NormalTok{,...}
       \ExtensionTok{TRADED_TIME}\DataTypeTok{\{1,i\}\{1,12\}}\NormalTok{);}
    
\ExtensionTok{end}

\ExtensionTok{%}\NormalTok{ The final concatenation}
\ExtensionTok{FINAL_TABLE_TRADED_TIME}\NormalTok{ = vertcat(TABLE_TRADED_TIME}\DataTypeTok{\{1,1\}}\NormalTok{,...}
    \ExtensionTok{TABLE_TRADED_TIME}\DataTypeTok{\{1,2\},TABLE_TRADED_TIME\{1,3\}}\NormalTok{,...}
    \ExtensionTok{TABLE_TRADED_TIME}\DataTypeTok{\{1,4\},TABLE_TRADED_TIME\{1,5\}}\NormalTok{,...}
    \ExtensionTok{TABLE_TRADED_TIME}\DataTypeTok{\{1,6\},TABLE_TRADED_TIME\{1,7\}}\NormalTok{,...}
    \ExtensionTok{TABLE_TRADED_TIME}\DataTypeTok{\{1,8\}}\NormalTok{);}


\ExtensionTok{%%}\NormalTok{ The updated time table}
\ExtensionTok{%}\NormalTok{ Fill the updated time}
\KeywordTok{for} \ExtensionTok{i}\NormalTok{ = 1 : 8}
    
   \ExtensionTok{TABLE_UPDATED_TIME}\DataTypeTok{\{1,i\}}\NormalTok{ = vertcat(UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,1\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,2\}}\NormalTok{, UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,3\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,4\}}\NormalTok{, UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,5\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,6\}}\NormalTok{, UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,7\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,8\}}\NormalTok{, UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,9\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,10\}}\NormalTok{, UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,11\}}\NormalTok{,...}
       \ExtensionTok{UPDATED_TIME}\DataTypeTok{\{1,i\}\{1,12\}}\NormalTok{);}
    
\ExtensionTok{end}

\ExtensionTok{%}\NormalTok{ The final concatenation}
\ExtensionTok{FINAL_TABLE_UPDATED_TIME}\NormalTok{ = vertcat(TABLE_UPDATED_TIME}\DataTypeTok{\{1,1\}}\NormalTok{,...}
    \ExtensionTok{TABLE_UPDATED_TIME}\DataTypeTok{\{1,2\},TABLE_UPDATED_TIME\{1,3\}}\NormalTok{,...}
    \ExtensionTok{TABLE_UPDATED_TIME}\DataTypeTok{\{1,4\},TABLE_UPDATED_TIME\{1,5\}}\NormalTok{,...}
    \ExtensionTok{TABLE_UPDATED_TIME}\DataTypeTok{\{1,6\},TABLE_UPDATED_TIME\{1,7\}}\NormalTok{,...}
    \ExtensionTok{TABLE_UPDATED_TIME}\DataTypeTok{\{1,8\}}\NormalTok{);}

\ExtensionTok{%}\NormalTok{ Find the unique traded times, Sort the traded times and assign}
\ExtensionTok{%}\NormalTok{ unique trade number to each in ascending order}
\ExtensionTok{UNIQ}\NormalTok{ = sortrows(unique(cell2mat(FINAL_TABLE_TRADED_TIME),}\StringTok{'rows'}\NormalTok{),[}\ExtensionTok{1}\NormalTok{ 2 3 4]);}
\ExtensionTok{UNIQO}\NormalTok{ = sortrows(unique(cell2mat(FINAL_TABLE_TRADED_TIME),}\StringTok{'rows'}\NormalTok{),[}\ExtensionTok{1}\NormalTok{ 2 3 4]);}

\ExtensionTok{%}\NormalTok{ generate a random number}
\ExtensionTok{raNDgen1}\NormalTok{ = (324434 : 26835144)}\StringTok{';}
\StringTok{raNDgen2 = rand(length(raNDgen1),1);}

\StringTok{% merge the two vectors}
\StringTok{raNDgen = [raNDgen1, raNDgen2];}
\StringTok{% sort according to the second column}
\StringTok{Sort_raNDgen = sortrows(raNDgen,2);}

\StringTok{% Cut at 2976 and sort according to the first column}
\StringTok{Sort_raNDgen1 = Sort_raNDgen(1: length(FINAL_TABLE_TRADED_TIME), :);}
\StringTok{Sort_raNDgen2 = sortrows(Sort_raNDgen1,1);}

\StringTok{% assign the computed trade numbers to the corresponding trade times}
\StringTok{UNIQO(:,5) = Sort_raNDgen2(:,1);}
\StringTok{% size of UNIQO}
\StringTok{SASS = size(UNIQO);}
\StringTok{% finding the position of the sorted traded times in the original times}
\StringTok{for i = 1 : length(FINAL_TABLE_TRADED_TIME)}
\StringTok{POS\{i,1\} = num2cell(find(ismember(cell2mat(FINAL_TABLE_TRADED_TIME(:,1:end)),UNIQO(i,1:4),'}\ExtensionTok{rows}\StringTok{')~=0));}
\StringTok{end}


\StringTok{%% THE_FINAL_TRADED_TIME}
\StringTok{THE_FINAL_TRADED_TIME = zeros(length(FINAL_TABLE_TRADED_TIME), SASS(2));}

\StringTok{for i = 1 : length(FINAL_TABLE_TRADED_TIME)}
\StringTok{    }
\StringTok{THE_FINAL_TRADED_TIME(cell2mat(POS\{i,1\}),:) = UNIQO(i,:);   }
\StringTok{       }
\StringTok{end}

\StringTok{ Headers = OPriskDataSetexposure(1,:);}

\StringTok{MATRIX = zeros(length(FINAL_TABLE_TRADED_TIME),length(Headers));}

\StringTok{%% The traded time}

\StringTok{% converting time into usal time formats}
\StringTok{% there are 24 hours in the a day , to fins the hour}
\StringTok{rt = 24.*THE_FINAL_TRADED_TIME(:,end-1);}
\StringTok{hh = round(rt);}

\StringTok{% the minutes}
\StringTok{rr = 60.*abs(rt - hh);}

\StringTok{mm = round(rr);}

\StringTok{% the seconds}
\StringTok{rg = 60.*abs(rr - mm);}

\StringTok{ss = round(rg);}

\StringTok{% Updated time as a vectors}
\StringTok{Vec_tedTime = [THE_FINAL_TRADED_TIME(:,1:end-2), hh, mm, ss];}

\StringTok{% converting the date back to string}
\StringTok{formatOut = '}\NormalTok{yyyy-mm-dd HH:MM:SS PM}\StringTok{';}
\StringTok{Vec_tradedTimeSTRING = datestr(Vec_tedTime(:, 1:end),formatOut);}

\StringTok{%% The updated time}
\StringTok{% converting time into usal time formats}
\StringTok{% there are 24 hours in the a day , to fins the hour}
\StringTok{rt = 24.*cell2mat(FINAL_TABLE_UPDATED_TIME(:,end));}
\StringTok{hh = round(rt);}

\StringTok{% the minutes}
\StringTok{rr = 60.*abs(rt - hh);}

\StringTok{mm = round(rr);}

\StringTok{% the seconds}
\StringTok{rg = 60.*abs(rr - mm);}

\StringTok{ss = round(rg);}

\StringTok{% Updated time as a vectors}
\StringTok{Vec_updatedTime = [cell2mat(FINAL_TABLE_UPDATED_TIME(:,1:end-1)), hh, mm, ss];}

\StringTok{% converting the date back to string}
\StringTok{formatOut = '}\NormalTok{yyyy-mm-dd HH:MM:SS PM}\StringTok{';}
\StringTok{Vec_updatedTimeSTRING = datestr(Vec_updatedTime(:, 1:end),formatOut);}

\StringTok{%% generate the compatible columns}
\StringTok{% capturedBy}
\StringTok{UNI_STRINGS = unique(OPriskDataSetexposure(2:end,9));}
\StringTok{% TraderID}
\StringTok{UNI_STRINGS1 = unique(OPriskDataSetexposure(2:end,11));}

\StringTok{for j = 1 : length(UNI_STRINGS)}
\StringTok{    }
\StringTok{    CapturedBy\{j\} = OPriskDataSetexposure(strcmp(OPriskDataSetexposure(:,9),...}
\StringTok{        UNI_STRINGS(j))==1,9);}
\StringTok{    % percentage proportion}
\StringTok{    LEngC(j) = length(CapturedBy\{j\})./length(OPriskDataSetexposure);}
\StringTok{    format long}
\StringTok{   N_STR(j) = ceil(LEngC(j).* length(THE_FINAL_TRADED_TIME));}
\StringTok{   }
\StringTok{   N_STRRR\{j\} = num2cell(zeros(N_STR(j),1));}
\StringTok{   }
\StringTok{end}


\StringTok{for j = 1 : length(UNI_STRINGS)}
\StringTok{   N_STRRR\{j\}(:,1) = (UNI_STRINGS(j,1));}
\StringTok{end}
\StringTok{%}
\StringTok{CAPTUREDBY_TOTAL = vertcat(N_STRRR\{1,1\},N_STRRR\{1,2\},...}
\StringTok{    N_STRRR\{1,3\},N_STRRR\{1,4\},N_STRRR\{1,5\});}

\StringTok{CAPTUREDBY_TOTAL = CAPTUREDBY_TOTAL(1:length(THE_FINAL_TRADED_TIME));}
\StringTok{CAPTUREDBY_TOTAL = [CAPTUREDBY_TOTAL, num2cell(rand(length(CAPTUREDBY_TOTAL),1))];}
\StringTok{CAPTUREDBY_TOTAL = sortrows(CAPTUREDBY_TOTAL,2);}
\StringTok{%%}


\StringTok{%% generate the compatible columns}
\StringTok{% TraderID}
\StringTok{Tra_UNI_STRINGS = unique(OPriskDataSetexposure(2:end,11));}
\StringTok{% TraderID}
\StringTok{Tra_UNI_STRINGS1 = unique(OPriskDataSetexposure(2:end,11));}

\StringTok{for j = 1 : length(Tra_UNI_STRINGS)}
\StringTok{    }
\StringTok{    TraderID\{j\} = OPriskDataSetexposure(strcmp(OPriskDataSetexposure(:,11),...}
\StringTok{        Tra_UNI_STRINGS(j))==1,11);}
\StringTok{    % percentage proportion}
\StringTok{    LEngC(j) = length(TraderID\{j\})./length(OPriskDataSetexposure);}
\StringTok{    format long}
\StringTok{   TRAN_STR(j) = ceil(LEngC(j).* length(THE_FINAL_TRADED_TIME));}
\StringTok{   }
\StringTok{   TRAN_STRRR\{j\} = num2cell(zeros(TRAN_STR(j),1));}
\StringTok{   }
\StringTok{end}


\StringTok{for j = 1 : length(Tra_UNI_STRINGS)}
\StringTok{   TRAN_STRRR\{j\}(:,1) = (Tra_UNI_STRINGS(j,1));}
\StringTok{end}

\StringTok{TRADERID_TOTAL = vertcat(TRAN_STRRR\{1,1\},TRAN_STRRR\{1,2\},...}
\StringTok{    TRAN_STRRR\{1,3\},TRAN_STRRR\{1,4\},TRAN_STRRR\{1,5\},...}
\StringTok{    TRAN_STRRR\{1,6\},TRAN_STRRR\{1,7\});}

\StringTok{ TRADERID_TOTAL = TRADERID_TOTAL(1:length(THE_FINAL_TRADED_TIME));}
\StringTok{TRADERID_TOTAL = [TRADERID_TOTAL, num2cell(rand(length(TRADERID_TOTAL),1))];}
\StringTok{TRADERID_TOTAL = sortrows(TRADERID_TOTAL,2);}
\StringTok{%% Business lines}

\StringTok{BL1 = [cellstr('}\NormalTok{BL1}\StringTok{') cellstr('}\NormalTok{BL1}\StringTok{') ;...}
\StringTok{    cellstr('}\NormalTok{Credit Derivatives}\StringTok{') cellstr('}\NormalTok{Investment Banking}\StringTok{')]'}\KeywordTok{;}

\ExtensionTok{BL2}\NormalTok{ = [cellstr(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL2'}\NormalTok{);}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Rates'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'MM'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Equity'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Commodities'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Africa'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Options'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Bonds/Repos'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Forex'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Prime Services'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Credit Derivatives'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Management'}\NormalTok{)}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Group Treasury'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'SND'}\NormalTok{)]}\StringTok{';}

\StringTok{BL3 = [cellstr('}\ExtensionTok{BL3}\StringTok{') cellstr('}\NormalTok{BL3}\StringTok{') cellstr('}\NormalTok{BL3}\StringTok{') ;...}
\StringTok{    cellstr('}\NormalTok{Africa}\StringTok{') cellstr('}\NormalTok{MM}\StringTok{') cellstr('}\NormalTok{SND}\StringTok{')]'}\KeywordTok{;}

\ExtensionTok{BL4}\NormalTok{ = [cellstr(}\StringTok{'BL4'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL4'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL4'}\NormalTok{)}\ExtensionTok{...}
     \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL4'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL4'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL4'}\NormalTok{);}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'ACBB'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Credit Derivatives'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Funding'}\NormalTok{)}\ExtensionTok{...}
     \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'MM'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Portfolio Management'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'SND'}\NormalTok{)]}\StringTok{';}
\StringTok{ }
\StringTok{ BL5 = [cellstr('}\ExtensionTok{BL5}\StringTok{') cellstr('}\NormalTok{BL5}\StringTok{') ;...}
\StringTok{    cellstr('}\NormalTok{Credit Derivatives}\StringTok{') cellstr('}\NormalTok{MM}\StringTok{')]'}\KeywordTok{;}

\ExtensionTok{BL6}\NormalTok{ = [cellstr(}\StringTok{'BL6'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'BL6'}\NormalTok{) ;}\ExtensionTok{...}
    \ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Management'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Prime Services'}\NormalTok{)]}\StringTok{';}

\StringTok{BL7 = [cellstr('}\ExtensionTok{BL7}\StringTok{') cellstr('}\NormalTok{BL7}\StringTok{') ;...}
\StringTok{    cellstr('}\NormalTok{Portfolio Management}\StringTok{') cellstr('}\NormalTok{SND}\StringTok{')]'}\KeywordTok{;}

\ExtensionTok{BL9}\NormalTok{ = [cellstr(}\StringTok{'BL9'}\NormalTok{) }\ExtensionTok{cellstr}\NormalTok{(}\StringTok{'Portfolio Management'}\NormalTok{)];}

\ExtensionTok{%%}\NormalTok{ generate the compatible columns}
\ExtensionTok{%}\NormalTok{ Business line}
\ExtensionTok{BUB_UNI_STRINGS}\NormalTok{ = unique(OPriskDataSetexposure(2:end,22));}
\ExtensionTok{%}
\KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(BUB_UNI_STRINGS)}
    
    \ExtensionTok{Bus}\DataTypeTok{\{j\}}\NormalTok{ = OPriskDataSetexposure(strcmp(OPriskDataSetexposure(:,22),}\ExtensionTok{...}
        \ExtensionTok{BUB_UNI_STRINGS}\NormalTok{(j))==}\ExtensionTok{1}\NormalTok{,22);}
    \ExtensionTok{%}\NormalTok{ percentage proportion}
    \ExtensionTok{LEngB}\NormalTok{(j) = }\ExtensionTok{length}\NormalTok{(Bus}\DataTypeTok{\{j\}}\NormalTok{)}\ExtensionTok{.}\NormalTok{/}\ExtensionTok{length}\NormalTok{(OPriskDataSetexposure);}
    \ExtensionTok{format}\NormalTok{ long}
    \ExtensionTok{Bu_STR}\NormalTok{(j) = }\ExtensionTok{ceil}\NormalTok{(LEngB(j)}\ExtensionTok{.*}\NormalTok{ length(THE_FINAL_TRADED_TIME));}
    
    \ExtensionTok{BUs_STRRR}\DataTypeTok{\{j\}}\NormalTok{ = num2cell(zeros(Bu_STR(j),}\ExtensionTok{1}\NormalTok{));}
    
\ExtensionTok{end}


\KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(BUB_UNI_STRINGS)}
    \ExtensionTok{BUs_STRRR}\DataTypeTok{\{j\}}\NormalTok{(:,1) = }\KeywordTok{(}\ExtensionTok{BUB_UNI_STRINGS}\NormalTok{(j,1}\KeywordTok{)}\NormalTok{);}
\ExtensionTok{end}
\ExtensionTok{%}
\ExtensionTok{BUS_TOTAL}\NormalTok{ = vertcat(BUs_STRRR}\DataTypeTok{\{1,1\},BUs_STRRR\{1,2\}}\NormalTok{,...}
    \ExtensionTok{BUs_STRRR}\DataTypeTok{\{1,3\},BUs_STRRR\{1,4\},BUs_STRRR\{1,5\}}\NormalTok{,...}
    \ExtensionTok{BUs_STRRR}\DataTypeTok{\{1,6\}}\NormalTok{, BUs_STRRR}\DataTypeTok{\{1,7\}}\NormalTok{, BUs_STRRR}\DataTypeTok{\{1,8\}}\NormalTok{);}

\ExtensionTok{BUS_TOTAL}\NormalTok{ = BUS_TOTAL(1:length(THE_FINAL_TRADED_TIME));}
\ExtensionTok{BUS_TOTAL}\NormalTok{ = [BUS_TOTAL, num2cell(rand(length(BUS_TOTAL),}\ExtensionTok{1}\NormalTok{))];}
\ExtensionTok{BUS_TOTAL}\NormalTok{ = sortrows(BUS_TOTAL,2);}

\ExtensionTok{%%}\NormalTok{ BUSINESS LINES}

\ExtensionTok{BUSINESS_LINESL}\NormalTok{ = [BL1}\KeywordTok{;}\ExtensionTok{BL2}\KeywordTok{;}\ExtensionTok{BL3}\KeywordTok{;}\ExtensionTok{BL4}\KeywordTok{;}\ExtensionTok{BL5}\KeywordTok{;}\ExtensionTok{BL6}\KeywordTok{;}\ExtensionTok{BL7}\KeywordTok{;}\ExtensionTok{BL9}\NormalTok{]}\KeywordTok{;}

\KeywordTok{for} \ExtensionTok{j}\NormalTok{ = 1 : length(THE_FINAL_TRADED_TIME)}
    
    \ExtensionTok{BUSINESS_LINES}\DataTypeTok{\{j\}}\NormalTok{ = num2cell((j.^0)}\ExtensionTok{.*zeros}\NormalTok{(length(BUSINESS_LINESL),}\ExtensionTok{3}\NormalTok{));}
    \ExtensionTok{BUSINESS_LINES}\DataTypeTok{\{j\}}\NormalTok{(:,3) = }\ExtensionTok{num2cell}\NormalTok{((j.^0)}\ExtensionTok{.*rand}\NormalTok{(length(BUSINESS_LINESL),}\ExtensionTok{1}\NormalTok{));}
    \ExtensionTok{BUSINESS_LINES}\DataTypeTok{\{j\}}\NormalTok{(:,1:2) = }\ExtensionTok{BUSINESS_LINESL}\NormalTok{(:,1:2);}
    
    \ExtensionTok{POSITION}\DataTypeTok{\{j\}}\NormalTok{ = find(strcmp(BUSINESS_LINES}\DataTypeTok{\{j\}}\NormalTok{(:,1),}\ExtensionTok{...}
        \ExtensionTok{BUS_TOTAL}\NormalTok{(j,1))==}\ExtensionTok{1}\NormalTok{, 1, }\StringTok{'last'}\NormalTok{ );}
    \ExtensionTok{%}\NormalTok{ the desk}
    \ExtensionTok{DESK}\NormalTok{(j,1) = }\ExtensionTok{BUSINESS_LINESL}\NormalTok{(POSITION}\DataTypeTok{\{j\}}\NormalTok{,2);}
\ExtensionTok{end}
\ExtensionTok{%%}
\ExtensionTok{%}\NormalTok{ fill in the matrix}
\ExtensionTok{MATRIX}\NormalTok{(:,1) = }\KeywordTok{(}\ExtensionTok{THE_FINAL_TRADED_TIME}\NormalTok{(:,end}\KeywordTok{)}\NormalTok{);}
\ExtensionTok{MATRIX}\NormalTok{(:,7) = }\KeywordTok{(}\ExtensionTok{THE_FINAL_TRADED_TIME}\NormalTok{(:,end-1}\KeywordTok{)}\NormalTok{);}
\ExtensionTok{MATRIX}\NormalTok{(:,6) = }\KeywordTok{(}\ExtensionTok{THE_FINAL_TRADED_TIME}\NormalTok{(:,end-2}\KeywordTok{)}\NormalTok{);}
\ExtensionTok{MATRIX}\NormalTok{(:,4) = }\ExtensionTok{cell2mat}\NormalTok{(FINAL_TABLE_UPDATED_TIME(:,end));}
\ExtensionTok{MATRIX}\NormalTok{(:,3) = }\ExtensionTok{cell2mat}\NormalTok{(FINAL_TABLE_UPDATED_TIME(:,end-1));}


\ExtensionTok{MATRIX}\NormalTok{ = num2cell(MATRIX);}

\ExtensionTok{MATRIX}\NormalTok{(:,8) = }\ExtensionTok{DESK}\NormalTok{(:,1);}
\ExtensionTok{MATRIX}\NormalTok{(:,22) = }\ExtensionTok{BUS_TOTAL}\NormalTok{(:,1);}
\ExtensionTok{MATRIX}\NormalTok{(:,9) = }\ExtensionTok{CAPTUREDBY_TOTAL}\NormalTok{(:,1);}
\ExtensionTok{MATRIX}\NormalTok{(:,11) = }\ExtensionTok{TRADERID_TOTAL}\NormalTok{(:,1);}
\ExtensionTok{%}\NormalTok{ fill in the updated time and the traded time}
\ExtensionTok{MATRIX}\NormalTok{(:,2) = }\ExtensionTok{cellstr}\NormalTok{(Vec_updatedTimeSTRING);}
\ExtensionTok{MATRIX}\NormalTok{(:,5) = }\ExtensionTok{cellstr}\NormalTok{(Vec_tradedTimeSTRING);}


\ExtensionTok{%}\NormalTok{ % Exporting the results to Excel}
\ExtensionTok{%}\NormalTok{ filename = }\StringTok{'Hoohlo.xlsx'}\KeywordTok{;}
\ExtensionTok{%}\NormalTok{ writetable(cell2table(MATRIX ,...}
\ExtensionTok{%}     \StringTok{'VariableNames'}\NormalTok{,Headers),}\ExtensionTok{...}
\ExtensionTok{%}\NormalTok{     filename,}\StringTok{'Sheet'}\NormalTok{,1,}\StringTok{'Range'}\NormalTok{,}\StringTok{'A1'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\clearpage

\normalsize

\clearpage
\addcontentsline{toc}{chapter}{CURRICULUM VITA} \fancyhead[L]{Vita}
\fancyhead[R]{\thepage} \fancyfoot[C]{}

\vspace*{\fill}

\begin{center}
    CURRICULUM VITA
  \end{center}

\vspace*{\fill}

\clearpage


\end{document}
