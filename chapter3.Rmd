---
output: pdf_document
---
\doublespacing

\textbf{\section{Introduction}}
\label{sec3:Introduction}

The fundamental premise in the nature behind ORMFs, is to provide an exposure-based treatment of OpRisk losses which caters to modeling capital estimates for forward-looking aspects of ORM. This proves tricky due to the lag between the time the loss event occurs and the actual realised loss, i.e. by the very nature of OpRisk, there is a significant lag between the moment the OpRisk event is conceived to the moment the event is observed and accounted for. There is a gap in time between $\tau$ the moment the risk is conceived and the time $\tau+1$ when impact of the loss is realised. This timing paradox often results in questionable capital estimates, especially for those near misses, pending and realised losses that need to be captured in the model

\section{EBOR methodology for capturing forward-looking aspects of ORM}
\label{sec:EBOR methodology for capturing forward-looking aspects of ORM}

@einemann2018operational, in a theoretical paper, construct a mathematical framework for an EBOR model to quantify OpRisk for a portfolio of pending litigations. Their work unearths an invaluable contribution to the literature, discussing a strategy on how to integrate EBOR and LDA models by building hybrid frameworks which facilitate the migration of OpRisk types from a *classical* to an exposure-based treatment through a quantitative framework, capturing forward looking aspects of BEICF's [@einemann2018operational], a key source of the OpRisk data.

The measure of exposure we need to use depends specifically on projecting the number of Oprisk event types (frequency of losses) and is different to the measure if the target variable were the severity of the losses. We need historical exposure for experience rating because we need to be able to compare the loss experience of different years on a like-for-like basis and to adjust it to current exposure levels [@parodi2014pricing].\medskip 

With reference to section \ref{sec:EBOR methodology for capturing forward-looking aspects of ORM}, the fundamental premise behind the LDA is that each firm's OpRisk losses are a reflection of it's underlying Oprisk exposure. In particular, the assumption behind the use of the poisson model to estimate the frequency of losses, is that both the the intensity (or rate) of occurrence and the opportunity (or exposure) for counting are constant for all available observations.\medskip

\subsection{Limitations of the EBOR model}

In their model [@einemann2018operational], definition \ref{ssec:Definition of exposure} is particularly well-suited to the specific risk type dealt with in their paper i.e., the portfolio of litigation events, due to better usage of existing information and more plausible model behavior over the litigation life cycle, but is bound to under-perform for many other OpRisk event types, since these EBOR models are typically designed to quantify specific aspects of OpRisk - litigation risk have rather concentrated risk profiles. Furthermore, EBOR models are important due to wide applicability beyond capital calculation and its potential to evolve into an important tool for auditing process and early detection of potential losses.

\textbf{\section{Generalised Linear Models (GLM's)}}
\label{sec:Generalised Linear Models}

Operational riskiness in FIs grows as trading transactions grow in complexity, i.e. the more complex and numerous activity builds the higher the rate of which new cases occur at. Therefore, the rate of the hazard increases exponentialy over time. The scientifically interesting question is whether the data provides any evidence that the increase in the underlying hazard generation is slowing.\medskip

Hence if $\mu$ is the (rate) number of expected new events on day $\tau_i$, then
\singlespacing
\begin{eqnarray}
\mu = \gamma \mbox{exp}({\delta\tau_i}) \nonumber
\end{eqnarray}
\doublespacing
can be used as a model, where $\gamma$ and $\delta$ are unknown parameters. Taking a log link turns the model into Generalised Linear Model (GLM) form so that:
\singlespacing
\begin{eqnarray}
\mbox{log}(\mu) = \mbox{log}(\gamma) + \delta\tau_i = \beta_0 + \tau_i\beta_1
\end{eqnarray}
\doublespacing

Where the LHS is the rate of hazard over time $\tau$ and $\tau+1$, and the RHS is a linear in the parameters $\beta_0 = \mbox{log}(\gamma)$ and $\beta_1 = \delta$.\medskip

Since the poisson means are low; LossIndicator values are $1$ for realised losses and $0$ for pending losses, or near misses. Amending the model so other situations other than unrestricted spread of  "rogue" events are represented, adapted to the poisson case yields:

\singlespacing
\begin{eqnarray}\label{eqn:adaptedpoisson}
\mu = d_i\mbox{exp}(\beta_0 + \beta_1\tau_i + \beta_2{\tau_i}^2) 
\end{eqnarray}
\doublespacing

\subsection{GLM for count data}

The LHS of a GLM formula is the model's random component which shows the number of events per trading transaction over FI's portfolio; of which it is an observation given by the independent random variable $Y$, not i.i.d [@wood2006generalized and @covrig2015using]. $Y$ takes a (exponential) family argument, depending on parameters $\lambda$ who represents the average frequency of operational events. It is worth distinguishing between the response data $y$ which is an observation of $Y$.\medskip

\textbf{\section{A poisson regression operational hazard model}}
\label{sec:A poisson regression operational hazard model}

The target variable (LossIndicator) is a count, therefore the poisson distribution is a reasonable distribution to use for modeling. It's probability mass function (pdf) is:
\singlespacing
\begin{eqnarray}\label{eqn:Poisson}
Y &\sim& \mbox{poisson}(\lambda), \quad f(y;\lambda) = \frac{\lambda^y}{y!}\dot\exp{-y}\\
 &\mbox{where}& \quad y \in  \mathbb{N}, \mbox{and} \quad \lambda > 0 \nonumber
\end{eqnarray}
\doublespacing
the expectation and variance $E[Y] = \mbox{VaR}[Y] = \lambda$\footnotemark{If you were to guess an independent $Y_i$ from a random sample, the best guess is given by this expression} are equal to parameter $\lambda$.\medskip

The RHS is the model's systematic component, and specifies the linear predictor. It builds on equation \ref{eqn:adaptedpoisson} with $p+1$ parameters $\beta = (\beta_0\ldots,\beta_p)^t$ with $p$ explanatory variables:
\singlespacing
\begin{eqnarray}
\nu = \beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}, \qquad \mbox{where} \quad i = 1,\ldots,n
\end{eqnarray}
\doublespacing

If sample variables $Y_i \sim \mbox{Poisson}(\lambda_i)$, then $\mu = E[Y_i] = \lambda_i$; the link function between the random and systematic components, viz. a tranformation by the model by some function $g()$, which does not change features essential to to fitting, but rather a scaling in magnitude so that:

\singlespacing
\begin{eqnarray}\label{eqn:linkfcn }
\nu_i &=& g(\lambda_i) = \mbox{ln}\lambda_i, \qquad \mbox{that is} \nonumber \\
\nu &=& \beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}
\end{eqnarray}
\doublespacing

so the mean frequency or otherwise the rate $R$, will be predicted by the model\ldots

\singlespacing
\begin{eqnarray}\label{eqn:multmodel}
\lambda_i &=& d_i\mbox{exp}(\beta_0 + \sum_{j=1}^{p}\beta_jx_{ij}) \quad \mbox{or} \nonumber \\
\lambda_i &=& d_i\cdot e^{\beta_0}\cdot e^{\beta_1x_{i1}}\cdot e^{\beta_2x_{i2}} \ldots e^{\beta_px_{ip}}
\end{eqnarray}
\doublespacing

Where $d_i$ represents the risk exposure for transaction $i$. Taking logs on both sides of equation \ref{eqn:multmodel}, the regression model for the estimation of loss frequency is:

\singlespacing
\begin{eqnarray}
\mbox{ln}\lambda_i =  \mbox{ln}d_i + \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \ldots + \beta_px_{ip}
\end{eqnarray}
\doublespacing

where $\mbox{ln}d_i$ is the natural log of risk exposure, called the "offset variable".

\subsection{Interpretation}

Table \ref{tab_int} presents the various units produced for the various GLM links.

```{r, results="hide"}
tab <- data.frame("Link Function"=c("Identity", "Log", "Logit", "Probit", "Poisson", "Gamma", "Negative Binomial"),
                  "Average Marginal Effect"=c("Original Continuous Unit", "Original Continuous Unit", "Risk", "Risk", "Count", "Count", "Count"))
names(tab) = c("Link Function", "Average Marginal Effect")
options(xtable.comment = FALSE)
library(xtable)
tabx = xtable(tab, 
       label = "tab_int", 
       caption = "The generalized linear model link functions with their associated units of interpretation. Note: This list is not exhaustive and there are likely more GLMs that are used within prevention research.",
       align = c("l", "|l", "|c|"))
print.xtable(tabx, include.rownames = FALSE, caption.placement = "top",
             table.placement = "tb")
```

\begin{table}[tb]
\centering
\caption{The generalized linear model link functions with their associated units of interpretation.} 
\label{tab_int}
\begin{tabular}{lc}
\toprule
Link Function & Target variable Effect \\ 
\midrule
Identity & Original Continuous Unit \\ 
  Log & Original Continuous Unit \\ 
  Logit & Risk \\ 
  Probit & Risk \\ 
  Poisson & Count \\ 
  Gamma & Count \\ 
  Negative Binomial & Count \\ 
\bottomrule
\end{tabular}
\end{table}

The poisson distribution is restrictive due to the assumption made that the mean and variance of the number of events are equal. However, models for count data where means are low so that the number of zeros and ones in the data is exessive are well adapted to the poisson case [@wood2006generalized]. They characterise situations in OpRisk other than models when the unchecked spreading of negligent behaviour may result in an operational hazard. The negative binomial and/or quasipoisson regression models ascribe to data that exhibits *overdispersion*, wherein the variance is much larger than the mean for basic count data, therefore they have been eliminated in this paper. 

\textbf{\section{Research Objective 1}}
\label{sec:Research Objective 1}

To introduce a generalised additive model for location, scale and shape (GAMLSS) framework for OpRisk management, that captures exposures to forward-looking aspects of the OpRisk loss prediction problem, due to deep hierarchies in the features of covariates in the investment banking (IB) business environment, and internal control risk factors (BEICF) thereof.

\textbf{\section{Exploratory data analysis}}

The main source of the analysis dataset is made up of internal losses for the period between 1 January 2013 and 31st March 2013 at a FI. The method of data generation and collection is at the level of the individual trade deal, wherein deal information is drawn directly from the trade generation and and settlement system (TGSS) and edit detail from attribution reports generated in middle office profit \& loss (MOPL). The raw source consists of two separate datasets on a trade-by-trade basis of daily frequencies (number of events) and associated loss severities.\medskip

```{r, echo=FALSE, results='hide'}
file_loc <- "C:/Users/User/Documents/OpRiskPHDGitHub/OpRisk_PHD_Dissertate/OpRisk_PHD_Dissertation"
setwd(file_loc)
list.files(file_loc)

frequency <- openxlsx::read.xlsx("Raw_Formatted_Data.xlsx", check.names = TRUE, sheet = "Frequency")
severity <- openxlsx::read.xlsx("Raw_Formatted_Data.xlsx", check.names = TRUE, sheet = "Severity")
projdata <- openxlsx::read.xlsx("OPriskDataSet_exposure.xlsx", check.names = TRUE, sheet = "CleanedData")
```

```{r, echo=FALSE, results='hide'}
### Contents of Raw Formatted Data columns 
str(frequency)
str(severity)
str(projdata)

### Number of unique trade entries in contents of Raw Formatted Data
length(unique(frequency$Related.Trade))
length(unique(severity$Trd.Nbr))

### Number of intersecting trades in the frequency (from amendment tracker) and severity (from MOPL  attribution summary) data sets from Raw Formatted Data file
length(intersect(frequency$Related.Trade, severity$Trd.Nbr))

### Column labels in Raw Formatted Data (frequency & severity) and Cleaned Data
names(frequency)
names(severity)
dput(names(projdata))

### Renaming column entries in Cleaned Data  
names(projdata)[names(projdata) %in% "EventTypeCategoryLevel1"] <- "EventTypeCategoryLevel"
names(projdata)[names(projdata) %in% "BusinessLineLevel1"] <- "BusinessLineLevel"
names(projdata) <- sub("\\.", "", names(projdata))
dput(names(projdata))

projdata[] <- lapply(projdata, function(x) if (is.character(x)) toupper(trimws(x)) else x)

```

The raw frequency data consists of 58,953 observations of 15 variables, within the dataset there are 50,437 unique trades. The raw severity data consists of 6,766 observations of 20 variables, within the severity dataset there are 2,537 unique trades. The intersection between the frequency and severity datasets consists of 2,330 individual transactions which represent realised losses, pending and/or near misses. This dataset is comprised of 3-month risk correction detail, in the interval between 01 January 2013 and 31 March 2013. \medskip

```{r, echo=FALSE, results='hide'}
# Load packages
#============================================================
# We begin most scripts by loading the required packages.
# Here are some initial packages to load and others will be
# identified as we proceed through the script.
library(rattle, quietly = TRUE)    # Access OpRisk dataset and utilities.
library(magrittr, quietly = TRUE)  # For the %>% and %<>% pipeline operators
library(Hmisc, quietly = TRUE)
library(chron, quietly = TRUE)
library(dplyr, quietly = TRUE)

#The logical variable 'building' 
# is used to toggle between generating transformations, 
# when building a model and using the transformations, 
# when scoring a dataset.

building <- TRUE
scoring  <- ! building

# A pre-defined value is used to reset the random seed 
# so that results are repeatable.

 crv$seed <- 42 

# Load the dataset from file.

fname <- "file:///C:/Users/User/Documents/OpRiskPHDGitHub/OpRisk_PHD_Dissertate/OpRisk_PHD_Dissertation/OPriskDataSet_exposure.csv" 
crs$dataset <- read.csv(fname,
                        sep=";",
                        dec=",",
                        na.strings=c(".", "NA", "", "?"),
                        strip.white=TRUE, encoding="UTF-8")

# Build the train/validate/test datasets.
#============================================================
# nobs=2330 train=1631 validate=349 test=350

set.seed(crv$seed)

crs$nobs     <- nrow(crs$dataset)
crs$train    <- crs$sample <- sample(crs$nobs, 0.7*crs$nobs)
crs$validate <- sample(setdiff(seq_len(crs$nobs), crs$train), 0.15*crs$nobs)
crs$test     <- setdiff(setdiff(seq_len(crs$nobs), crs$train), crs$validate)

# The following variable selections have been noted.

crs$input     <- c("Trade", "UpdateTime", "UpdatedDay", "UpdatedTime",
                   "TradeTime", "TradedDay", "TradedTime", "CapturedBy",
                   "TradeStatus", "TraderId", "Instrument", "Reason",
                   "Nominal", "FloatRef", "LastResetDate", "LastResetRate",
                   "Theta", "Loss", "Unexplained", "EventTypeCategoryLevel1",
                   "BusinessLineLevel1", "LossIndicator", "exposure")

crs$numeric   <- c("Trade", "UpdatedDay", "UpdatedTime", "TradedDay",
                   "TradedTime", "Nominal", "LastResetRate", "Theta", "Loss",
                   "Unexplained", "LossIndicator", "exposure")

crs$categoric <- c("UpdateTime", "TradeTime", "CapturedBy", "TradeStatus",
                   "TraderId", "Instrument", "Reason", "FloatRef",
                   "LastResetDate", "EventTypeCategoryLevel1",
                   "BusinessLineLevel1")

crs$target    <- "LossIndicator"
crs$risk      <- NULL
crs$ident     <- NULL
crs$ignore    <- c("FloatRef", "LastResetDate", "LastResetRate", "Loss")
crs$weights   <- NULL

#Data summary
summary(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])

```

```{r, echo=FALSE, results='hide'}
contents(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])
```

\begin{table}[ht]
\centering
\caption{The contents of the traded transactions of the associated risk correction events.}
\begin{tabular}{lcc}
\toprule
  & \multicolumn{2}{c}{Storage} \\
Covariate     & Levels   & Type \\ 
\midrule
 Trade       &          & numeric \\
 UpdateTime  &          & numeric \\
 UpdatedDay  &          & numeric \\
 UpdatedTime &          & numeric \\
 TradeTime   &          & numeric \\
 TradedDay   &          & numeric \\
 TradedTime  &          & numeric \\
 Desk        &  10      & categorical \\
 CapturedBy  &  5       & categorical \\
 TradeStatus &  4       & categorical \\
 TraderId    &  7       & categorical \\
 Instrument  &  23      & categorical \\
 Reason      &  19      & categorical \\
 Loss        &          & numeric \\
 EventTypeCategoryLevel & 5  & categorical \\
 BusinessLineLevel      & 8  & categorical \\
 LossIndicator          & 2  & binary \\
 exposure               &    & numeric \\
 \bottomrule
\end{tabular}\label{tab_contents}
\end{table}

Two new variables are derived from the data; a target variable (LossIndicator) is a binary variable whereupon, a $1$ signifies a realised loss, and $0$ for those pending losses, or near misses. The *exposure* variable is computed by deducting the time between the trade amendment (UpdateTime) and the time when the trade was booked. It is measure that is rougly proportional to the risk of the transaction or a group of transactions. The idea is that if the exposure (e.g. the duration of a trade, the number of allocation(trade splits), etc.) doubles whilst everything else (e.g. the rate, nominal of the splits, and others) remains the same, then the risk also doubles.

In R, the GLM function works with two types of covariates/explanatory variables: numeric (continuous) and categorical (factor) variables as depicted in table \ref{tab_contents}. Multi-level categorical variables are recoded by building dummy variables corresponding to each level. This is achieved through an implemented algorithm in R, through a transformation as recommended for the estimation of the GLM, particularly in the estimation of the poisson regression model for count data.

The model revolves around the fact that for each categorical variable (covariate), previously transformed into a dummy variable, one must specify a reference category from which the corresponding observations under the same covariate are estimated and assigned a weight against in the model [@covrig2015using]. By default in the GLM, the first level of the categorical variable is taken as the reference level. As best practice,  @de2008generalized, @frees2010household, @denuit2007actuarial, @cameron2013regression recommend that for each categorical variable one should specify the modal class as the reference level; as this variable corresponds to the level with the highes order of predictability, excluding the dummy variable corrresponding to (weight coefficient = $1$) the biggest absolute frequency.

\textbf{\section{Description of the dataset}}
\label{sec:Description of the dataset}

In this section, section \ref{sec:Description of the dataset}, the dataset called *OpRiskDataSet_exposure*, provides data on the increase in the numbers of operational events over a three month period, beginning 01 January 2013 to end of 20 March 2013. For each transaction, there is information about: trading risk exposure, trading characteristics, causal factor characteristics and their cost.

\begin{figure}
\begin{subfigure}[b]{0.55\textwidth}
   \begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Intra-day Trend of Loss Severity} & \textbf{Trends of Loss Severities per Trader} \\
        \includegraphics[width=7.5cm]{IntraDayUpdatedTime.eps}
         &
         \includegraphics[width=7cm]{TrendTraderId.eps}
         \end{tabular}
    \end{frame}
\subcaption{Scatterplots}
   \label{Intra_Day_Trends} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
   \begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Loss per month} & \textbf{Trading frequency} \\
        \includegraphics[width=7.5cm]{UpdatedDayFreq.eps}
         &
         \includegraphics[width=7cm]{TradedDayFreq.eps}
         \end{tabular}
    \end{frame}
\subcaption{Histograms}
   \label{Hist_Loss_Freq}
\end{subfigure}
\caption[Numerical grid display]{(a) Scatterplots of intra-day trend analysis for logs of severities of operational events and trends incident activity for identifying the role of the trader originating the incidents. (b) As for (a) but in the form of histograms showing the frequency distrbution of the number daily operational indicents and the number of trades over a monthly period.} 
\end{figure}

\subsection{Characteristics of exposure}

The exposure of risk of type $i$, $d_i$ shows the daily duration, from when the trade was booked to the moment the operational risk event was observed and ended. This measure is defined this way when specifically applied to projecting the number of loss events (frequencies) and can be plotted as follows depicted in graphs depicted in Figure \ref{Exploration_analysis_exposure}.\medskip

```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}

# Generate a cummulative distribution function (ECDF) plot of the variable 'exposure'.
#============================================================
# Generate just the data for an Ecdf 
ds <- rbind(data.frame(dat=crs$dataset[crs$sample,][,"exposure"], grp="All"))

# The 'Hmisc' package provides the 'Ecdf' function.
library(Hmisc, quietly=TRUE)
# Plot the data.
#p05<-Ecdf(ds[ds$grp=="All",1], col="#E495A5", xlab="exposure", lwd=2, ylab=expression(Proportion <= x), subtitles=FALSE)

## BaseR codes for plot p05b<-plot(p05$x,p05$y,type='l',col='red')

# Benford's Law 
#============================================================
# The 'ggplot2' package provides the 'ggplot' function.
library(ggplot2, quietly=TRUE)
# The 'reshape' package provides the 'melt' function.
library(reshape, quietly=TRUE)
# Initialies the parameters.
var    <- "exposure"
digit  <- 1
len    <- 1

# Build the dataset
ds <- merge(benfordDistr(digit, len),
            digitDistr(crs$dataset[crs$sample,][var], digit, len, "All"))

# Plot the digital distribution
p <- plotDigitFreq(ds)
#print(p)
#============================================================
# Generate the plot.

p04 <- crs %>%
  with(dataset[sample,]) %>%
  dplyr::select(exposure) %>%
  ggplot2::ggplot(ggplot2::aes(x=exposure)) +
  ggplot2::geom_density(lty=3) +
  ggplot2::xlab("exposure") +
  ggplot2::ggtitle("Distribution of exposure") +
  ggplot2::labs(y="Density")

# Display the plots.

gridExtra::grid.arrange(p04, nrow=1)

```

\begin{figure}
\begin{frame}
      \centering
       \begin{tabular}{ccc}
        \textbf{Distribution} & \textbf{Density} & \textbf{Digital Analysis} \\
        \includegraphics[width=5cm]{Exposure_cdf.eps}
         &
         \includegraphics[width=5cm]{Dist_exposure.eps}
         &
         \includegraphics[width=5cm]{Benford.eps}
         \end{tabular}
    \end{frame}
        \captionof{figure}{A simple comparison of the Sigmoidal like features of the fat-tailed, right skewed distribution for exposure, and first-digit frequency distribution from the exposure data with the expected distribution according to Benford's Law}
    \label{Exploration_analysis_exposure}
\end{figure}

The variable follows a logistic trend on $[0,1]$, implying an FIs operational risk portfolio rises like a sigmoid function throughout the period of observation, typically starting from $0$, which then observes a plateau in growth. The average exposure is 389.99 or about 1 year.\medskip

Grid plots \ref{Exploration_analysis_exposure} portray the logistic function, together with a  simple comparison of first-digit frequency distribution analysis, according to Benford's Law, with exposure data distribution. The close fitting nature implies the data are uniformly distributed across several orders of magnitude, especially within the 1 year period.\medskip

\subsection{Characteristics of the covariates}

The characteristics of the operational risk portfolio are given by the following covariates: *UpdatedDay*, *UpdatedTime* - the day of the month and time of day the OpRisk incident occurs respectively; *TradedDay*, *TradedTime* - the day in the month and time of day the deal was originated respectively; The *LossIndicator*, is a binary variable (two variables): $0$, which indicates pending or near misses, and $1$, if the incident results in a realised loss, meaning that there is significant p\&L impact due to the OpRisk incident.\medskip

*Desk*, the location in the portfolio tree the incident originated, a factor variable with 10 categories; *CapturedBy*, the designated analyst who actions the incident, a factor variable with 5 categories; *TraderId*, the trader who originates the deal, a factor variable with 7 categories; *TradeStatus*, the live status of the deal, a factor variable with 4 categories; *Instrument*, the type of deal, a factor variable with 23 categories; *Reason*, a description of the cause of the OpRisk incident, a factor variable with 19 levels; *EventTypeCategoryLevel*, 7 OpRisk event types as per @risk2001supporting, a factor variable with 5 categories; *BusinessLineLevel*, 8 OpRisk business lines as per @risk2001supporting, a factor variable with 8 categories.\medskip

The factor variables were transformed into dummy variables using the following commands:
\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
# Remap factor variables and transform into numeric variables.
crs$dataset[["TNM_Desk"]] <- as.numeric(crs$dataset[["Desk"]])
crs$dataset[["TNM_CapturedBy"]] <- as.numeric(crs$dataset
                                              [["CapturedBy"]])
crs$dataset[["TNM_TraderId"]] <- as.numeric(crs$dataset[["TraderId"]])
crs$dataset[["TNM_Instrument"]] <- as.numeric(crs$dataset
                                              [["Instrument"]])
crs$dataset[["TNM_Reason"]] <- as.numeric(crs$dataset[["Reason"]])
crs$dataset[["TNM_EventTypeCategoryLevel1"]] <- as.numeric(crs$dataset
                                        [["EventTypeCategoryLevel1"]])
crs$dataset[["TNM_BusinessLineLevel1"]] <- as.numeric(crs$dataset
                                             [["BusinessLineLevel1"]])
```
\doublespacing

The continuous numerical variable *Loss*, shows the financial impact (severity) of the OpRisk incident in Rands, for the most part, (96.1\%) incidents result in pending losses and near misses, most realised losses (2.3\%) lie within [R$200,00$, R$300,000$] range, in the portfolio there are also 5 p\&L impacts higher than R$2.5$ million.\medskip

\subsection{Characteristics of daily operational activity}

The distribution of daily losses and/or pending/near misses by operational activities are represented in \ref{Exploratory_Time_Day_Frequency3plot}. Figure \ref{Exploratory_UpdateTime_Frequency3plot} shows that most operational events occur in times leading up to midday (i.e. 10:50AM to 11:50AM), the observed median is 11:39AM, and of these potential loss events, most realised losses occur closest to mid-day. The frequencies of the loss incidents in the analysed portfolio sharply decreases during the following period, i.e. from 12:10PM to 13:10PM, during which the least realised losses occur.\medskip

Figure \ref{Exploratory_UpdateDay_Frequency3plot} shows that operational activity increases in intensity in the  days leading up to the middle of the month, i.e. $10^{th}$ - $15^{th}$; the observed mean is $14.49$ days, and of these potential loss events, realised losses especially impact on the portfolio during these days.

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Exploratory data analysis
#___________________________________________________________________________________________________
# Update Time
### summary statistics
 summary(projdata$UpdatedTime)
 ### Histograms
 par(mfrow=c(1,3)) 
 hist(projdata$UpdatedTime, col = "blue", main = "All losses", xlab = "Update Time", ylab = "Frequency")
 hist(projdata$UpdatedTime[projdata$LossIndicator == 0], col = "red", main = "Near Misses", xlab = "Update Time", ylab = "Frequency")
 hist(projdata$UpdatedTime[projdata$LossIndicator == 1], col = "green", main = "Realised losses", xlab = "Update Time", ylab = "Frequency")
 par(mfrow=c(1,1))
#___________________________________________________________________________________________________
# # Update Day
 summary(projdata$UpdatedDay)
# # LossIndicator
 par(mfrow=c(1,3)) 
 hist(projdata$UpdatedDay, col = "#9999CC", main = "All losses", xlab = "Updated Day", ylab = "Frequency")
 hist(projdata$UpdatedDay[projdata$LossIndicator == 0], col = "#CC6666", main = "Near Misses", xlab = "Updated Day", ylab = "Frequency")
 hist(projdata$UpdatedDay[projdata$LossIndicator == 1], col = "#66CC99", main = "Realised losses", xlab = "Updated Day", ylab = "Frequency")
 par(mfrow=c(1,1))
 
```
\doublespacing

\begin{figure}
\centering
\begin{subfigure}[b]{0.55\textwidth}
   \includegraphics[width=1\linewidth]{Exploratory_UpdateTime_Frequency3plot.eps}
   \subcaption{Frequency distributions of operational incidents by the time in the day}
   \label{Exploratory_UpdateTime_Frequency3plot} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
   \includegraphics[width=1\linewidth]{Exploratory_UpdateDay_Frequency3plot.eps}
   \subcaption{Frequency distributions of operational incidents by the day in the month}
   \label{Exploratory_UpdateDay_Frequency3plot}
\end{subfigure}

\caption[Two numerical solutions: Histograms showing the distribution of UpdatedTime \& UpdatedDay by LossIndicator.]{The frequency distributions of All the losses, the realised losses, and pending/near misses of operational incidents by the day in the month when the indidents' occurred}
\label{Exploratory_Time_Day_Frequency3plot}
\end{figure}

Similarly, the influence of trading desk's on the frequency of operational events can be analysed on the basis of the portfolio's bidimensional distribution by variables *Desk* and *LossIndicator*, which shows the proportions realised losses vs pending and/or near misses for each particular desk. The bidimensional distribution of *Desk* and *LossIndicator* is presented in a contingency table, Table \ref{tab_Desk_Prop}, in which it's considered useful to calculate proportions for each desk category. 

\begin{figure}
\centering
\includegraphics[width=20cm,height=5cm]{Density_UpdateDay_TradedDay.eps}
\caption[Density plots showing a comparison of realised vs pending losses and/near misses over a month for the day in the month the OpRisk incident was updated to the day in the month trades were traded/booked]{Density plots showing a comparison of realised vs pending losses and/near misses over a month for the day in the month the OpRisk incident was updated to the day in the month trades were traded/booked}
\label{Desk_Proportions}
\end{figure}

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Desk analysis using tables
table(projdata$Desk, projdata$LossIndicator)
addmargins(table(projdata$Desk, projdata$LossIndicator), 2)
round(addmargins(prop.table(table(projdata$Desk, projdata$LossIndicator), 1), 2)*100, 1)
# Statistcal analysis
do.call("rbind", lapply(split(projdata$Loss, projdata$Desk), function(x) c(N = length(x), Mean = mean(x), SD = sd(x), Min = min(x), Max = max(x))))
```
\doublespacing

\begin{table}[ht]
\centering
\caption{Occurence of realised losses: proportions on desk categories}
\begin{tabular}{lccr}
\toprule
  & \multicolumn{3}{c}{No. of transactions} \\
Desk   & no Loss   & Loss & Total\\ 
\midrule
  Africa            &  49 & 10 &  59 \\
  Bonds/Repos       & 113 & 31 & 144 \\
  Commodities       & 282 & 45 & 327 \\
  Derivatives       & 205 & 24 & 229 \\
  Equity            & 269 & 66 & 335 \\
  Management/Other  &  41 &  2 &  43 \\
  Money Market      & 169 & 52 & 221 \\
  Prime Services    & 220 & 62 & 282 \\
  Rates             & 336 & 53 & 389 \\
  Structured Notes  & 275 & 26 & 301 \\
 \bottomrule
\end{tabular}\label{tab_Desk_Prop}
\end{table}

Thus, as illustratred in figure \ref{Desk_Proportions}, from 23,5\%; the highest proportion of realised losses per desk is the Money Market (MM) desk, the figures are decreasing, followed by Prime Services (22\%); Bonds/Repos (21,5\%); Equity (19,7\%); Africa (16,9\%); Commodities (13,8\%); Rates (13,6\%); Derivatives (10,5\%); Structured Notes (SND) (8.6\%), to the least proportion in the Management/Other, a category where only 4,7\% of operations activities were realised as losses.      

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Plot Desk category distribution
p03 <- crs %>%
  with(dataset[sample,]) %>%
  dplyr::mutate(LossIndicator=as.factor(LossIndicator)) %>%
  dplyr::select(Desk, LossIndicator) %>%
  dplyr::group_by(Desk, LossIndicator) %>%
  dplyr::summarise(n = n()) %>%
  ggplot2::ggplot(ggplot2::aes(x=Desk, y=n, fill=LossIndicator)) +
  ggplot2::geom_bar(stat="identity") +
  ggplot2::ggtitle("Desk category distribution") +
  ggplot2::theme_minimal() +
  ggplot2::theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggplot2::ylab("Frequency")

# Create new variable to proportion no. of realised losses
T01 <- crs %>%
  with(dataset[sample,]) %>%
  dplyr::mutate(LossIndicator=as.factor(LossIndicator)) %>%
  dplyr::select(Desk, LossIndicator) %>%
  dplyr::group_by(Desk, LossIndicator) %>%
  dplyr::summarise(n = n())

T02 <- T01 %>%
  group_by(Desk) %>%
  summarise(N=sum(n))

T03 <- inner_join(T01, T02)

# plot Desk category by proportion
T04 <- T03 %>%
  mutate(Prob=n/N) %>%
  filter(LossIndicator==1) %>%
  select(Desk, Prob) %>%
  arrange(desc(Prob)) %>%
  ggplot2::ggplot(ggplot2::aes(x=Desk, y=Prob, fill=Desk), alpha=0.55) +
  ggplot2::geom_bar(stat="identity", fill="grey", colour="black", show.legend = FALSE)+
  ggplot2::ggtitle("Proportion of losses per Desk") +
  ggplot2::theme_minimal()+
  ggplot2::theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggplot2::ylab("Loss Ratio (n/N)")+
  ggplot2::xlab("Desk")

#Display both plots in one row
gridExtra::grid.arrange(p03, T04, nrow = 1)
```
\doublespacing

\begin{figure}
\centering
\includegraphics[width=20cm,height=5cm]{Exploratory_Desk_Proportions.eps}
\caption[Desk category by realised losses]{Histograms showing the proportions of realised losses vs all losses including pending and/or near misses by desk category}
\label{Desk_Proportions}
\end{figure}

This behaviour can be extended beyond the trading desk, as represented in Figure \ref{Mosaic_Instr_Trd_Tec}, a mosaic plot grid presenting the structure of the OpRisk portfolio by Instrument, TraderId, CapturedBy \footnote{i.e. the type of financial instrument, the trader who originated the incident on the deal, and the role of the technical support personnel who is involved in the query resolution.} and the operational losses.

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Instrument
unique(projdata$Instrument)
table(projdata$LossIndicator, projdata$Instrument)
plot(table(projdata$LossIndicator, projdata$Instrument), main="By Instrument", col=rainbow(20), las=1)

# Trader
unique(projdata$TraderId)
table(projdata$TraderId, projdata$LossIndicator)
round(addmargins(prop.table(table(projdata$TraderId, projdata$LossIndicator), 1), 2)*100, 1)
plot(table(projdata$LossIndicator, projdata$TraderId), main="By Trader", col=rainbow(20), las=1)

# Captured By
table(projdata$CapturedBy, projdata$LossIndicator)
round(addmargins(prop.table(table(projdata$CapturedBy, projdata$LossIndicator), 1), 2)*100, 1)
plot(table(projdata$LossIndicator, projdata$CapturedBy), main="By Tech Support", col=rainbow(20), las=1)
```
\doublespacing

\begin{figure}
\begin{frame}
      \centering
       \begin{tabular}{cc}
        \textbf{Type of instrument traded} & \textbf{Role identification} \\
        \includegraphics[width=7.5cm]{Single_Instr.eps}
         &
         \includegraphics[width=7.5cm]{Stacked_TrId_TechSup.eps}
         \end{tabular}
    \end{frame}
    \caption{Mosaic grid plots for the bidimensional distribution by traded instrument, the trader originating the operational event, and by the technical support personnel involved in query resolution, against the dummy variable showing if a realised loss was reported.}
    \label{Mosaic_Instr_Trd_Tec}
\end{figure}

One can notice that the width of the bars corresponding to the different categories, i.e. Instrument, TraderId, CapturedBy, is given by their proportion in the sample.  In particular, for the category 'at least one realised loss', Figure \ref{Mosaic_Trader} portrays a increase "riskiness" trending up from Associate to AMBA, Analyst, Vice Principal, Managing Director, Director, up to the risky ATS category, which are automated trading system generated trades.\medskip

Figure \ref{Mosaic_Tech_Support} for the category 'at least one realised loss', portrays a decreasing trend, slowing in riskiness from Unauthorised users downward to Tech Support, Mid Office, Prod controller down to the least risky Prod Accountant. This intepretation makes sense given unauthorised users are more likely to make impactful operational errors, technical support personnel would also be accountable for large impacts albiet for contrasting reasons, they are mandated to perform these deal adjustments which have unavoidable impacts associated with them, whereas the former group are unauthorised to perform adjustments therefore may lack the skill, or may have more devious intentions in their actions.\medskip   

In another mosaic plot, (\ref{Mosaic_Contingency}), the bidimensional distribution of transactions by trader and realised vs pending losses, conditional on the trade stautus in presented and analysed. Here, and in the contingency table, Table \ref{tab:Mosaic_Contingency}, we can clearly see the trends: in BO-BO confired status - an increasing realised losses from left (AMBA) to right, and the opposite for transaction performed in BO Confirmed status. Particularly, the biggest number of realised losses in both BO and BO-BO Confirmed statuses are for automated trading systems (ATS).\medskip

Table \ref{tab:Mosaic_Contingency} and Figure \ref{Mosaic_Contingency} are obtained with the following commands:

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
library(vcd)
STD <- structable(~TradeStatus + TraderId + LossIndicator
                                        , data = projdata)
MS01 <- mosaic(STD, condvars = 'TradeStatus', col=rainbow(20),
                  split_horizontal = c(TRUE, FALSE, TRUE))
```

\singlespacing
\begin{figure}
\centering
\textbf{Mosasic plot for trader identification and loss indicator, by trade status}
\includegraphics[width=\linewidth,height=0.75\linewidth]{Mosaic_Contingency.eps}
\caption[Portfolio structure by trader, trade status and number of realised losses]{A mosaic plot representing the structure of the operational risk portfolio by trader identification (TraderId), the status ofthe trade (TradeStatus) and the number of realised losses vs pending or near misses}
\label{Mosaic_Contingency}
\end{figure}
\doublespacing

Table \ref{tab:Crosstab_covariate} presents the most frequent category in the operational risk dataset for each possible covariate.

\begin{table}[htbp]
        \centering
        \textbf{Crosstab of trader identification and loss indicator, by trade status}
\singlespacing        
        \small
        \setlength\tabcolsep{2pt}
			\begin{tabular}{|p{2cm}|p{2cm}|l|l|l|l|l|l|p{2cm}|p{2cm}|} \hline
  			& & \multicolumn{7}{|c|}{Trader Identification} \\ \hline
  			TradeStatus & Loss Indicator & Amba & Analyst & Associate & ATS & Director & Mng Director & Vice Principal \\\hline
  			\multirow{2}{*}{BO-BO Confirmed} & 0 & 24 & 136 & 320 & 0 & 282 & 52 & 49 \\ \cline{2-9}
    							   & 1 & 2  &  15 & 43 & 0 & 50 & 18 & 16 \\\cline{2-9}
   			\multirow{2}{*}{BO Confirmed} & 0 & 17  & 299 & 153 & 13 & 257 & 102 & 153 \\ \cline{2-9}
    							   & 1 &  3 &  71 & 12 & 8 &  62 & 23 & 30 \\ \cline{2-9}
   			\multirow{2}{*}{Terminated} 	  & 0 &	83 & 9 & 1 & 0 & 0 & 2 & 1 \\ \cline{2-9}
    							  & 1 & 17 & 1 & 0 & 0 & 0 & 0 & 0 \\ \cline{2-9}
    		\multirow{2}{*}{Terminated/Void}  & 0 & 2 & 0 & 0 & 0 & 2 & 1 & 1 \\ \cline{2-9}
							       & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
			\end{tabular}
   			\caption{A contingency table showing the bidimensional distribution of transactions by trader identification vs realised and/or pending losses, conditional on the trade status}
        	\label{tab:Crosstab_covariate}
\end{table}
\doublespacing

\begin{table}[htbp]
        \centering
        \textbf{Modal classes for the categorical variables} 
\singlespacing        
        \small
        \setlength\tabcolsep{2pt}
			\begin{tabular}{|l|l|p{4cm}|} \hline
  			Variable & Modal class or category & Name of modal class \\\hline
  			Desk & Rates & DeskRates \\ \cline{1-3}
			CapturedBy & TECHSUPPORT & CapturedBy\_TECHSUPPORT \\ \cline{1-3}
    		TradeStatus & BO confirmed & TradeStatus\_BO confirmed \\ \cline{1-3}
			TraderId & DIRECTOR & TraderId\_DIRECTOR \\ \cline{1-3}
			Instrument & Swap & Instrument\_Swap \\ \cline{1-3}
			Reason & Trade enrichment for system flow  & Reason\_Trade enrichment for system flow \\ \cline{1-3}
    		EventTypeCategoryLevel & EL7 & EventTypeCategoryLevel\_EL7 \\ \cline{1-3}
			BusinessLineLevel & BL2 & BusinessLineLevel\_BL2 \\ \cline{1-3}	
			\end{tabular}
   			\caption{A contingency table showing the bidimensional distribution of transactions by trader identification vs realised and/or pending losses, conditional on the trade status}
        	\label{tab:Mosaic_Contingency}
\end{table}
\doublespacing

\textbf{\section{The estimation of some poisson regression generalised linear models (GLM's)}}
\label{sec:The estimation of some poisson regression generalised linear models (GLM's)}

Section \ref{sec:Generalised Linear Models} introduced a GLM for the start of the expected number of operational events in the early stages. We aim to estimate the mean OpRisk frequency through a poisson classification model given by equation \ref{eqn:Poisson} using the glm function. The mean daily loss frequency in the risk correction statistics is estimated through the poisson regression model. Let us consider a model where the *LossIndicator* is the target variable: The following fits the model (the log link is canonical for the poisson distribution, and hence the R default) and checks it.\medskip

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Set parameter values
crv$seed <- 42 # set random seed to make your partition reproducible
crv$taining.proportion <- 0.7 # proportion of data used for training
crv$validation.proportion <- 0.15 # proportion of data used for validation

# Load data
d <- read.csv("OPriskDataSet_exposure.csv",
              sep=";",
              dec=",",
              na.strings=c(".", "NA", "", "?"),
              strip.white=TRUE, encoding="UTF-8")

exposure <- d[,ncol(d)] 

d1 <- d %>%
  group_by(UpdatedDay,
           UpdatedTime,
           TradedDay,
           TradedTime,
           Desk,
           CapturedBy,
           TradeStatus,
           TraderId,
           Instrument,
           Reason,
           EventTypeCategoryLevel1,
           BusinessLineLevel1) %>%
  transmute(LossesIndicator = LossIndicator,
            Losses = Loss,
            exposure = exposure)
```
\doublespacing

In calling the GLM we specify the target variable *LossIndicator*; the explanatory variables are composed of of numeric, continuous and categorical variables. Where the variable in the argument of a GLM is categorical , one chose to specify the modal class as the reference level. A user defined function "getmode" has been created and the dataset is reordered using the "relevel" function in ![](C:\Users\User\Documents\OpRiskPHDGitHub\OpRisk_PHD_Dissertate\OpRisk_PHD_Dissertation\figures\smallorb_90.eps){width=5%}. These specifications were achieved in the following code chunk:

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
# Create function "getmode" which finds the modal class in
# the categorical variables
getmode <- function(x){
  u <- unique(x)
  as.integer(u[which.max(tabulate(match(x,u)))])
}
# Reorder the categorical variables so that the modal class 
# is specified as the reference level
for (i in 5:(ncol(d1) - 3)){
     d1[[i]] <- relevel(d1[[i]], getmode(d1[[i]]))
}

```
\doublespacing

Other GLM arguments; the afore-mentioned link function poisson(link="log"); data=d1, a data frame containing the OpRisk dataset and the r offset=log(exposure)`, that is the variable representing a component known apriori, coefficient $1$, introduced in the linear predictor [@covrig2015using].\medskip

Firstly, consider a GLM where we introduce two explanatory variables, one numerical variable, *UpdatedTime*, and another categorical variable *Desk*. This will be our global model. We will use *LossesIndicator* as the target variable, while these two unique variables will be explanatory variables:

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
freqfit1 <- glm(LossesIndicator ~ UpdatedTime + Desk, data=d1, 
               family=poisson(link = 'log'), offset = log(exposure))
```
\doublespacing

The output of the estimation is presented below, where variables who were found to be significant predictors are indicated. The coefficients of the categorical variable *Desk* are reordered and weighted against the modal class *DeskRates*, interestingly the modal class does not show up in the results section (as the coefficient of the modal class = $1$), given that the remaining classes are weighted against it.

\singlespacing
```{r, tidy=TRUE, echo=FALSE}
summary(freqfit1)
```
\doublespacing
Using this bivariate model, the estimated quarterly OpRisk (LossIndicators) frequency of realised losses for each  *Desk* category (excluding the insignificant ones) are:
\begin{list}{*}{}
\item $0,002099618  = e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.2515}$, for the combination of the \textbf{UpdateTime} and \textbf{DeskAfrica} category, which implies that frequency of realised losses for this combination of preditor variables is $3.4955824(=\cdot\exp{1.2515})$ fold (times) higher than the realised loss frequency of OpRisk causes in the reference desk category, viz. the \textbf{Rates} desk. 
\item $0,003546834 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.7758}$, for the combination of the \textbf{UpdateTime} and \textbf{DeskBonds/Repos} category, which implies that frequency of realised losses for this combination of preditor variables is $5,90500325(=\cdot\exp{0.8274})$ fold higher than causes in the reference desk category.
\item $0,001373903 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{0.8274}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $2,287363856(=\cdot\exp{0.8274})$ fold higher than the causes in the reference desk category.
\item $0,002360693= e^{-9.2147}\cdot e^{1.7972}\cdot e^{1.3687}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $3,930238063(=\cdot\exp{1.3687})$ fold higher than the causes in the reference desk category.
\item $0,001373903 = e^{-9.2147}\cdot e^{1.7972}\cdot e^{0.8274}$, for the combination with \textbf{DeskMM},an increase of $39\%)$ w.r.t the baseline (the \textbf{Rates} desk)
\item $0,005012603= e^{-9.2147}\cdot e^{1.7972}\cdot e^{2.1217}$, for the combination, which implies that frequency of realised losses for this combination of preditor variables is $8,345312467(=\cdot\exp{2.1217})$ fold higher than the causes in the reference desk category.
\end{list}

The predicted mean frequency of realised losses for OpRisk incident $i$, for the model freqfit1, is given by:

\singlespacing
\begin{eqnarray}
\mu_{i}& = &\mbox{exposure}_i\cdot e^{-9.2147\cdot \mbox{Intercept}_i}\cdot e^{1.7972\cdot \mbox{UpdatedTime}_i}\cdot e^{1.2515\cdot \mbox{DeskAfrica}_i}\nonumber\\
&\cdot&e^{1.7758\cdot \mbox{DeskBonds/Repos}_i}\cdot e^{0.8274\cdot \mbox{DeskCommodities}_i}\cdot e^{1.3687\cdot \mbox{DeskEquity}_i}\nonumber\\
&\cdot& e^{0.3910\cdot \mbox{DeskMM}_i}\cdot e^{2.1217\cdot \mbox{DeskPrime Services}_i}\cdot e^{-0.7055\cdot \mbox{DeskSND}_i}
\end{eqnarray}
\doublespacing

Ww now fit a more comprehensive model where we introduce more variables, in which show realised losses for quarterly OpRisk incidents for an all inclusive case. We will use "LossesIndicator" as the dependent variable, while the other variables will be predictor variables.

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
freqfit <- glm(LossesIndicator ~ UpdatedDay + UpdatedTime +
                 TradedDay + TradedTime + Desk + CapturedBy +
                 TradeStatus + TraderId + Instrument + Reason
               + EventTypeCategoryLevel1 + BusinessLineLevel1,
data=d1, family=poisson(link = 'log'), offset = log(exposure))
```
\doublespacing

Which yields output (in summarised form):

\singlespacing
\begin{verbatim}
Call:
glm(formula = LossesIndicator ~ UpdatedDay + UpdatedTime + TradedDay + 
    TradedTime + Desk + CapturedBy + TradeStatus + TraderId + 
    Instrument + Reason + EventTypeCategoryLevel1 + BusinessLineLevel1, 
    family = poisson(link = "log"), data = d1, offset = log(exposure))

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-4.6205  -0.3700  -0.1056  -0.0295   4.0726  

Coefficients:
                                                        Estimate Std. Error z value             Pr(>|z|)    
(Intercept)            -8.953252   0.604562 -14.809 < 0.0000000000000002 ***
UpdatedDay             -0.006976   0.008140  -0.857             0.391428    
UpdatedTime             1.113913   0.564165   1.974             0.048331 *  
TradedDay              -0.012303   0.006368  -1.932             0.053382 .  
TradedTime              0.101378   0.637529   0.159             0.873656    
DeskAfrica              1.899956   0.446050   4.260   0.0000204875303586 ***
DeskBonds/Repos         2.803220   0.334324   8.385 < 0.0000000000000002 ***
DeskCommodities         0.747527   0.364630   2.050             0.040355 *  
DeskDerivatives         0.683199   0.374174   1.826             0.067867 .  
DeskEquity              1.507079   0.321232   4.692   0.0000027113532659 ***
DeskManagement/Other   -2.054697   1.082815  -1.898             0.057755 .  
DeskMM                  1.544054   0.453315   3.406             0.000659 ***
DeskPrime Services     -0.028783   0.960227  -0.030             0.976087    
DeskSND                 0.766563   0.573844   1.336             0.181602  
\vdots                  \vdots     \vdots     \vdots            \vdots  

BusinessLineLevel1BL1   1.537103   0.636829   2.414             0.015792 *  
BusinessLineLevel1BL3  -0.359123   0.514434  -0.698             0.485119    
BusinessLineLevel1BL4  -1.384293   0.391691  -3.534             0.000409 ***
BusinessLineLevel1BL5  -1.169766   0.394350  -2.966             0.003014 ** 
BusinessLineLevel1BL6   1.250498   1.002141   1.248             0.212095    
BusinessLineLevel1BL7   0.875839   1.746369   0.502             0.616005    
BusinessLineLevel1BL9   4.214689   1.598598   2.636             0.008377 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 2767.9  on 2329  degrees of freedom
Residual deviance: 1821.2  on 2252  degrees of freedom
AIC: 2719.2

Number of Fisher Scoring iterations: 13
\end{verbatim}
\doublespacing

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
summary(freqfit)
```
\doublespacing

\subsubsection{Model selection and multimodel inference}

The selection of the best model from the list of possible combinations of predictor variables, traditionally follows of a process of propagating backward, comparing goodnes of fit tests at each stage. For example, if we compare the values of the Aikaike information criteria for the bivariate model freqfit1 and the multivariate model freqfit, by AICs; we see that for the first model the value 3225.7 and 2719.2 for the second, which suggests that the model in which we additionaly considered an all inclusive list of predictor variables is prefereble to the first. \medskip

In a similar way, we can estimate the models comparing each one which enables one to choose the most appropriate or "best" fit one, by first checking if the model is significant, i.e. if the Residual deviance and the corresponding number of degrees of freedom doesn't have a value significantly bigger than 1: In the latter model $\frac{1821.2}{2252} = 0.808703374$, and then retaining the one with the smaller AIC value.\medskip 

@Burnham2002 introduces information-theoretic approach that allows a data-based selection of a "best" model in the anaysis of our dataset, and a ranking and weighting of the remaining models. These approaches allow traditional (formal) statistical inference to be based on the selected "best" model, which is now based on more than one model (multimodel inference). To do this we are required to load the "MuMIn" package in ![](C:\Users\User\Documents\OpRiskPHDGitHub\OpRisk_PHD_Dissertate\OpRisk_PHD_Dissertation\figures\smallorb_90.eps){width=5%}. \medskip

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
require(MuMIn)
```
\doublespacing

Then, we use "dredge" function to generate models using combinations of the terms in the global model. The function will also calculate AICc values and rank models according to it. Note that AICc is AIC corrected for finite sample sizes. The process of analyzing data where the experimentalist has few or no a priori information, thus "all possible models" are considered by subjectively ad iteratively searching the data for patterns and "significance", is often called "data mining", "data snooping" or the term "data dredging". 

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
options(na.action=na.fail)
freqfits <- dredge(freqfit)

```
\doublespacing

The function "MuMLn::dredge" returns a list of $4096$ models, which is every combination of predictor variable in the global model freqfit. Model number 2942 is the best model and shows that all predictor variables included in the model have a positive effect on the target variable except for the preditor TrddD (\textbf{TradedDay}) which has a negative effect on the likelihood og a realised loss (target variable *LossIndicator*). Additionally, from the delta (=delta AIC) one cannot distinguish model 2942 from 3966 and 2878 since (using the common thumb rule) they have AIC < 2.\medskip

The top three models, models $2942, 3966$ \&  $2878$ each include nine, ten and seven predictor variables respectively, and where a variable doesn't have a value, it means that it was not included in the model, not that it does not have and effect. For example model $2942$ returns a combination of the seven variables $1/2/3/4/5/6/7/8/10$, corresponding to the following  output predictor variables (abbreviated in the header row) below:

\singlespacing
\begin{verbatim}
Model selection table 
     (Intrc) BsLL1 Desk ETCL1 Instr Reasn TrddD   TrdrI TrdSt UpdtT 
2942  -9.107     +    +     +     +     + -0.011660   +     + 1.2760000 
\end{verbatim}
\doublespacing

Information from the AICc's values suggest, that of the top three models have similar support, and their Akaike weights are not high relative to the $[0,1]$ weight range; This is characteristic of the endemic nature of data dredging, as the literature suggests [@Burnham2002], and should generally be avoided to curb attendant inferential problems if a single model is chosen, e.g the risk of finding spurious effects, overfitting, etc. .@Burnham2002 advises that model averaging is useful in finding a confirmatory result as estimates of precision should include model selection uncertainty. Even so, one can rule out many models on a priori grounds.\medskip    

We now use "get.models" function to generate a list in which its objects are the fitted models. We will also use the "model.avg" function to do a model averaging based on AICc. Note that "subset=TRUE" will make the function calculate the average model (or mean model) using all models. However, we want to get only the models that have delta AICc < 2; we threfore use "subset=delta<2"

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
adelmodel <- (model.avg(get.models(freqfits, subset=delta<2)))

```
\doublespacing

Now we have AICc values for our models and we have the average (mean) model.\medskip

Multimodel inference leads to more robust inferences, especially in the point of view that the selection of the model used to estimate the mean frequency must, at the same time, serve the ultimate root cause analysis objective of OpRisk control, that decide calculating capital requirement, in OpVaR measures, taking into account as many characteristics of the trading OpRisk dataset as possible, as well considering how the variables interact with each other. 

\textbf{\section{The estimation of some  generalised additive models for location scale and shape (GAMLSS) for severity of loss}}
\label{sec:The estimation of some  generalised additive models for location scale and shape (GAMLSS) for severity of loss}

We introduce a Box-Cox Power Exponential distribution (BCPE), which is a four parameter distribution, for fitting a GAMLSS to estimate the (non-linear nature) mean OpRisk loss severity using the gamlss function. The mean daily loss severities in the risk correction statistics is estimated through the BCPE gamlss model.\medskip

The pdf of the BCPE distribution is defined as:
\singlespacing
\begin{eqnarray}
f(y|\mu,\sigma,\nu,\tau)&=&(y^{(\nu-1)/\mu^nu)}\cdot{frac{\tau}{\sigma}}\cdot \frac{e^(-0.5\cdot|\frac{z}{c}|^\tau)}{(c\cdot 2^(1+\frac{1}{tau})}\cdot \Gamma(\frac{1}{\tau}))\mbox{where}\\
 c=[2^(\frac{-2}{\tau})\cdot \frac{\Gamma(\frac{1}{\tau})}{\Gamma(\frac{3}{\tau})}]^(0.5)&,& \quad \mbox{where if} \qquad \nu!=0 \quad \mbox{then} \qquad z=\frac{(\frac{y}{\mu})^\nu-1}{\nu\cdot \sigma}\mbox{else}\\
 z=\frac{log\frac{y}{\mu}}{\sigma}&,& \quad \mbox{for} \quad y>0, \mu>0, \sigma>0, \nu=(\mbox{-Inf,+Inf})\quad \mbox{and}\quad \tau>0.
\end{eqnarray}
\doublespacing

The BCPE adjusts the obove density $f(y|\mu,\sigma,\nu,\tau)$, resulting from the condition $y>0$. See @stasinopoulos2017flexible . We now consider a model where the *Loss* is the target variable: The following fits the model and checks it.\medskip

\singlespacing
```{r, results="hide", fig.show="hide", fig.keep="none", echo=FALSE}
# Load data
D <- read.csv("OPriskDataSet_exposure_severity.csv",
              sep=";",
              dec=",",
              na.strings=c(".", "NA", "", "?"),
              strip.white=TRUE, encoding="UTF-8")
exposure <- D[,ncol(D)] 

D1 <- D %>%
  group_by(UpdatedDay,
           UpdatedTime,
           TradedDay,
           TradedTime,
           Desk,
           CapturedBy,
           TradeStatus,
           TraderId,
           Instrument,
           Reason,
           EventTypeCategoryLevel1,
           BusinessLineLevel1) %>% 
transmute(LossesIndicator = LossIndicator,
            Losses = Loss,
            exposure = exposure)
```
\doublespacing

\singlespacing
```{r, eval=FALSE, results="hide", fig.show="hide", fig.keep="none", echo=TRUE}
library(gamlss)
sf <- gamlss(Losses~cs(UpdatedDay + UpdatedTime + TradedDay +
                TradedTime + Desk + CapturedBy + TradeStatus 
               + TraderId + Instrument + Reason + 
                 EventTypeCategoryLevel1 + BusinessLineLevel1), 
sigma.formula=~cs(UpdatedDay + UpdatedTime + TradedDay + 
                TradedTime + Desk + CapturedBy + TradeStatus 
               + TraderId + Instrument + Reason + 
               EventTypeCategoryLevel1 + BusinessLineLevel1),
nu.formula=~cs(UpdatedDay + UpdatedTime + TradedDay + TradedTime
               + Desk + CapturedBy + TradeStatus + TraderId + 
              Instrument + Reason + EventTypeCategoryLevel1 + 
              BusinessLineLevel1),
 tau.formula=~cs(UpdatedDay + UpdatedTime + TradedDay + TradedTime
                + Desk + CapturedBy + TradeStatus + TraderId +
                Instrument + Reason + EventTypeCategoryLevel1 +
                BusinessLineLevel1),
data=D1, mu.start = NULL,  sigma.start = NULL, nu.start = NULL,
                              tau.start = NULL, family=BCPE)
```
\doublespacing


\singlespacing
