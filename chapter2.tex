\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}

\textbf{\section{Introduction}} \label{sec2:Introduction}

A look into literary sources for OpRisk indicates
{[}@acharyya2012current{]} that there is insufficient academic
literature that looks to characterize its theoretical roots, as it is a
relatively new discipline, choosing instead to focus on proposing a
solution to the quantification of OpRisk. This chapter seeks to provide
an overview of some of the antecedents of OpRisk measurement and
management in the banking industry. As such, this chapter provides a
discussion on why OpRisk is not trivial to quantify and attempts to
understand its properties in the context of risk aversion with the
thinking of practitioners and academics in this field.\medskip

According to @cruz2002modeling, FI's wish to measure the impact of
operational events upon profit and loss (P\&L), these events depict the
idea of explaining the \emph{volatility of earnings} due to OpRisk data
points which are directly observed and recorded. By seeking to
incorporate data intensive statistical approaches to help understand the
data, the framework analyses response variables that are decidedly
non-normal (including categorical outcomes and discrete counts) which
can shed further light on the understanding of firm-level OpRisk RC.
Lastly, a synopsis of gaps in the literature is presented.

\textbf{\section{The theoretical foundation of OpRisk}}
\label{sec:The theoretical foundation of OpRisk}

@hemrit2012major argue that common and systematic operational errors in
hypothetical situations poses presumtive evidence that OpRisk events,
assuming that the subjects have no reason to disguise their preferences,
are created sub-consciously. This study purports, supported by
experimental evidence, behavioural finance theories should take some of
this behaviour into account in trying to explain, in the context of a
model, how investors maximise a specific utility/value function.\medskip

Furthermore its argued by integrating OpRisk management into behavioral
finance
theory,\footnote{In behavioral finance, we investigate whether certain financial phenomena are the result of less than fully rational thinking [@barberis2003survey]},
that it may be possible to improve our understanding of firm level RC by
refining the resulting OpRisk models to account for these behavioral
traits - implying that people's economic preferences described in the
model, have an economic incentive to improve the OpRisk RC measure.
\medskip

@wiseman1997longitudinal suggest that managerial risk-taking attitudes
are influenced by the decision (performance) context in which they are
taken. In essence, managerial risk-taking attitude is considered as a
proxy for measuring OpRisk {[}@acharyya2012current{]}. In so doing,
@wiseman1997longitudinal investigate more comprehensive economic
theories, viz. prospect theory and the behavioural theory of the firm,
that prove relevant to complex organizations who present a more fitting
measure for OpRisk.\medskip 

In a theoretical paper, @wiseman1997longitudinal discussed several
organizational and behavioural theories, such as PT, which influence
managerial risk-taking attitudes. Their findings demonstrate that
behavioural views, such as PT and the behavioural theory of the firm
explain risk seeking and risk averse behaviour in the context of OpRisk
even after agency based influences are controlled for. Furthermore, they
challenge arguments that behavioral influences are masking underlying
root causes due to agency effects. Instead they argue for mixing
behavioral models with agency based views to obtain more complete
explanations of risk preferences and risk taking behavior
{[}@wiseman1997longitudinal{]}. \medskip 

Despite the reality that OpRisk does not lend itself to scientific
analysis in the way that market risk and credit risk do, someone must do
the analysis, value the RC measurement and hope the market reflects
this. Besides, financial markets are not objectively scientific, a large
percentage of successful people have been lucky in their forecasts, it
is not an area which lends itself to scientific analysis.

\textbf{\section{Overview of operational risk management}}
\label{sec:Overview of operational risk management}

It is important to note how OpRisk manifests itself:
@king2001operational has established the causes and sources of
operational loss events as observed phenomena associated with
operational errors and are wide ranging. By definition, the occurence of
a loss event is due to P\&L volatitlity from a payment, settlement or a
negative court ruling within the capital horizon over a time period (of
usually one year) {[}@einemann2018operational{]}. As such, P\&L
volatitlity is not only related to the way firms finance their business,
but also in the way they \emph{operate}.\medskip 

In operating practice, one assumes that on observing or on following
instructions we are consciously analysing and accurately executing our
tasks based on the information. However, the occurence of operational
loss events indicates that there are sub-concious faults in information
processing, which we are not consciously aware of. These operational
loss events are almost always initiated at the dealing phase of a
trading process, which more often than not implicates front office (FO)
personnel to bear the responsibility for the losses e.g., during the
trading process in cases where OpRisk events occur as a result of a
mismatch between the trade booked (booking in trade feed) and the
details agreed by the trader. The middle office (MO) and back offices
(BO) conduct the OpRisk management, who undertake a broad view of P\&L
attribution carried out from deal origination to settlement within the
perspective of strategic management, and detects the interrelationships
between OpRisk factors with others to conceptualise the potential
overall consequences {[}@acharyya2012current{]} e.g., in the
afore-mentioned example, human error (a sub-conscious phenomenon) is
usually quoted as the source of error, and the trade is fixed by
\lq\lq amending\rq\rq~or manually changing the trade details. \medskip

Furthermore, @acharyya2012current recognised that organizations may hold
OpRisk due to external causes, such as failure of third parties or
vendors (either intentionally or unintentionally), in maintaining
promises or contracts. The criticism in the literature is that no amount
of capital is realistically reliable for the determination of RC as a
buffer to OpRisk, particularly the effectiveness of the approach of
capital adequacy from external events, as there is effectively no
control over them.\medskip

\section{The loss collection data exercise (LCDE)}
\label{sec:The loss collection data exercise (LCDE)}

The main challenge in OpRisk modeling is in poor loss data quantities,
and low data quality. There are usually very few data points and are
often characterised by high frequency low severity (HFLS) and low
frequency high severity (LFHS) losses. It is common knowledge that HFLS
losses at the lower end of the spectrum tend to be ignored and are
therefore less likely to be reported, whereas low frequency high
severity losses (LFHS) are well guarded, and therefore not very likely
to be made public.\medskip

In this study, a new dataset with unique feature characteristics is
developed using the official loss data collection exercise (LDCE), as
defined by @basel2011operational for internal data. The dataset in
question is at the level of individual loss events, it is fundamental as
part of the study to know when they happened, and be able to identify
the root causes of losses arising from which OpRisk loss events.\medskip

The LCDE is carried out drawing statistics directly from the trade
generation and settlement system, which consists of a tractable set of
documented trade detail extracted at the most granular level, i.e.~on a
trade-by-trade basis {[}as per number of events (frequencies) and
associated losses (severities){]}, and then aggregated daily. The
dataset is split into proportions and trained, validated and tested. The
afore-mentioned LDCE, is an improved reflection of the risk factors by
singling out the value-adding processes associated with individual
losses, on a trade-by-trade level.

\textbf{\section{Current operational risk measurement modeling framework}}
\label{sec:Current operational risk measurement modeling framework}

Historical severity curves obtained from historical loss counts have
been widely considered to be the most reliable models when used in
OpRisk loss estimation. However they have not been successfull when used
as measures capturing forward-looking aspects of the OpRisk loss
prediction problem.

In this paper, we develop data intensive analysis techniques which yield
a more realistic estimation for underlying risk factors, through linking
risk factors to covariates based on internal control vulnerabilities
(ICV's). ICV's are selected as measures of trading risk exposure,
business environment and internal control factors (BEICF's) i.e., trade
characteristics and causal factors. For each loss event, information
such as unique trade identifier, trader identification, loss event
capture personnel, trade status and instrument type, loss event
description, loss amount, market variables, trading desk and business
line, beginning and ending date and time of the event, and settlement
time are given.

AMA's allow banks to use their internally generated risk estimates Under
Basel II; a first attempt internal measurement approach (IMA) capital
charge calculation for OpRisk (i.e. ) is similar to the Basel II model
for credit risk, where a loss event is a default in the credit risk
jargon. There are generally seven event type categories
{[}@risk2001supporting{]} and eight business lines. Potential losses are
decomposed into several (\(7 \times 8 = 56\)) sub-risks using event
types and business line combinations: e.g., execution, delivery \&
process management is one such category defined the risk that
operational losses/problems would take place in the banks transactions,
given as:

\begin{eqnarray}
\mathcal{\Large{C}}_{OpRisk}^{IMA} &=& \sum_{i=1}^8 \sum_{k=1}^7 \gamma_{ik}\epsilon_{ik} \\
where \quad \epsilon_{ik}&:& \quad \mbox{expected loss for business line $i$, risk type $k$} \nonumber \\
      \gamma_{ik}&:& \quad \mbox{scaling factor} \nonumber
\end{eqnarray}

\ref{BL/ET Matrix} depicts the formation of the \(BL/ET\) matrix; the
duration (time \(T+\tau\)) is represented along the depth ordinate.

\begin{figure}
\begin{tikzpicture}[every node/.style={minimum size=0.5cm},on grid]
\begin{scope}[every node/.append style={yslant=-0.5},yslant=-0.5]
  \shade[right color=gray!10, left color=black!50](0,0) rectangle +(8,8);
  \node at (0.5,7.5) {$\textbf{\mbox{BL}1}$};
  \node at (0.5,6.5) {$\color{green}{{X^l}_{11}}$};   
  \node at (1.5,7.5) {$\textbf{\mbox{BL}2}$};
  \node at (2.5,7.5) {$\textbf{\mbox{BL}3}$};
  \node at (3.5,7.5) {$\textbf{\mbox{BL}4}$};
  \node at (4.5,7.5) {$\textbf{\mbox{BL}5}$};
  \node at (5.5,7.5) {$\textbf{\mbox{BL}6}$};
  \node at (6.5,7.5) {$\textbf{\mbox{BL}7}$};
  \node at (7.5,7.5) {$\textbf{\mbox{BL}8}$};
  \node at (7.5,6.5) {$\color{blue}{{X^l}_{18}}$};  
  \draw (0,0) grid (8,8);
\end{scope}   
\begin{scope}[every node/.append style={yslant=0.5},yslant=0.5]  
  \shade[right color=gray!70,left color=gray!10](8,-8) rectangle +(8,8);
  \node at (11.5,-0.5) {$\mbox{T}+\tau \quad \Huge{\color{magenta}\xrightarrow{\hspace*{4cm}}}$};
  \node at (8.5,-1.5) {\textbf{EL1}};
  \node at (8.5,-2.5) {\textbf{EL2}};
  \node at (8.5,-3.5) {\textbf{EL3}};
  \node at (8.5,-4.5) {\textbf{EL4}};
  \node at (8.5,-5.5) {\textbf{EL5}};
  \node at (8.5,-6.5) {\textbf{EL6}};
  \node at (8.5,-7.5) {\textbf{EL7}};
  \node at (10.5,-1.5) {Internal Fraud};
  \node at (10.5,-2.5) {External Fraud};
  \node at (12.5,-3.5) {Employment Practices and Workplace Safety};
  \node at (12.0,-4.5) {Client Products and Business Practices};
  \node at (11.5,-5.5) {Damage to Physical Assets};
  \node at (12.0,-6.5) {Business Disruption and System Failure};
  \node at (12.5,-7.5) {Execution, Delivery and Process Management};
<!--  \draw (8,-8) grid (12,0); -->
\end{scope}   
\begin{scope}[every node/.append style={yslant=0.5,xslant=-1},yslant=0.5,xslant=-1]
<!-- \shade[bottom color=gray!10, top color=black!80] (8,0) rectangle +(12,8); -->
  \node at (10.0,7.5) {Corporate Finance};
  \node at (10.0,6.5) {Trading and Sales};
  \node at (10.0,5.5) {Retail Banking};
  \node at (10.0,4.5) {Commercial Banking};
  \node at (10.0,3.5) {Payment and Settlement};
  \node at (10.0,2.5) {Agency Services};
  \node at (10.0,1.5) {Asset Management};
  \node at (10.0,0.5) {Retail Brokerage};
<!-- \draw (12,0) grid (16,8); -->
\end{scope}  
\end{tikzpicture}
\label{BL/ET Matrix}
\caption{The 3-Dimensional BL/ET matrix for 7 event types and 8 business lines}
\end{figure}

\section{Loss Distribution Approach (LDA)}

The Loss Distribution Approach (LDA) is an AMA method whose main
objective is to provide realistic estimates to calculate VaR for OpRisk
RC in the banking sector and it's business units based on loss
distributions that accurately reflect the frequency and severity loss
distributions of the underlying data.\medskip

Having calculated separately the frequency and severity distributions,
we need to combine them into one aggregate loss distribution that allows
us to produce a value for the OpRisk VaR. There is no simple way of
aggregating the frequency and severity distribution. Numerical
approximation techniques (computer algorithms) successfully bridge the
divide between theory and implementation for the problems of
mathematical analysis. \medskip

The aggregated losses at time \(t\) are given by
\(\vartheta(t) = \sum_{n=1}^{N(t)} X_{n}\) (where X represents
individual operational losses). Frequency and severity distributions are
estimated, e.g., the poisson distribution is a representation of a
discrete variable commonly used to model operational event frequency
(counts), and a selection from continuous distributions which can be
linear (e.g.~gamma distribution) or non-linear (e.g.~lognormal
distribution) for operational loss severity amounts. The compound loss
distribution \(\mathbf{G}(t)\) can now be derived. Taking the aggregated
losses we obtain:

--\textgreater{}

\begin{equation}\label{Compound_losses}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}

--\textgreater{}

For most choices of \(N(t)\) and \(X_{n}\), the derivation of an
explicit formula for \(\mathbf{G}_{\vartheta(t)}(x)\) is, in most cases
impossible. \(\mathbf{G}(t)\) can only be obtained numerically using the
Monte Carlo method, Panjer's recursive approach, and the inverse of the
characteristic function {[}@frachot2001loss; @aue2006lda;
@panjer2006operational; \& others{]}. \medskip

After most complex banks adopted the LDA for accounting for RC,
significant biases and delimitations in loss data remain when trying to
attribute capital requirements to OpRisk losses {[}@frachot2001loss{]}.
OpRisk is related to the internal processes of the FI, hence the quality
and quantity of internal data (optimally combined with external data)
are of greater concern as the available data could be rare and/or of
poor quality. Such expositions are unsatisfactory if OpRisk, as
@cruz2002modeling professes, represents the next frontier in reducing
the riskiness associated with earnings.

\subsection{LDA model shortcomings}
\label{ssec:LDA model shortcomings}

@opdyke2014estimating advanced studies intending on eliminating bias
apparently due to heavy tailed distributions to further provide insight
on new techniques to deal with the issues that arise in LDA modeling,
keeping practitioners and academics at breadth with latest research in
OpRisk \texttt{VaR} theory. Recent work in LDA modeling has been found
wanting {[}@badescu2015modeling{]}, due to the very complex
characteristics of data sets in OpRisk \texttt{VaR} modeling, and even
when studies used quality data and adequate historical data points, as
pointed out in a recent paper by @hoohlo2015new, there is a qualitative
aspect in OpRisk modeling that is often ignored, but whose validity
should not be overlooked. \medskip

@opdyke2014estimating, @agostini2010combining, @de2015combining,
@galloppo2014review, and others explicate how greater accuracy,
precision and robustness uphold a valid and reliable estimate for OpRisk
capital as defined by Basel II/III. Transforming this basic knowledge
into \lq\lq risk culture\rq\rq~or firm-wide knowledge for the effective
management of OpRisk, serves as a starting point for a control function
providing attribution and accounting support within a framework,
methodology and theory for understanding OpRisk measurement. FI's are
beginning to implement sophisticated risk management systems similar to
those for market and credit risk, linking theories which govern how
these risk types are controlled to theories that govern financial losses
resulting from OpRisk events. \medskip

@de2015combining and @galloppo2014review sought to address the
shortcomings of @frachot2001loss by finding possible ways to improve the
problems of bias and data delimitation in operational risk management.
They follow the recent literature in finding a statistical-based model
for integrating internal data and external data as well as scenario
assessments in on endeavor to improve on accuracy of the capital
estimate.

\textbf{\section{A new class of models capturing forward-looking aspects}}
\label{sec:A new class of models capturing forward-looking aspects}

@agostini2010combining also argued that banks should adopt an integrated
model by combining a forward-looking component (scenario analysis) to
the historical operational \texttt{VaR}, further adding to the
literature through their integration model which is based on the idea of
estimating the parameters of the historical and subjective distributions
and then combining them by using the advanced CT. \medskip

The idea at the basis of CT is that a better estimation of the OpRisk
measure can be obtained by combining the two sources of information: The
historical loss data and expert's judgements, advocating for the
combined use of both experiences. @agostini2010combining seek to explain
through a weight called the credibility, the amount of credence given to
two components (historical and subjective) determined by statistical
uncertainty of information sources, as opposed to a weighted average
approach chosen on the basis of qualitative judgements.\medskip

Thus generating a more predictable and forward looking capital estimate.
He deemed the integration method as advantageous as it is self contained
and independent of any arbitrary choice in the weight of the historical
or subjective components of the model.

\subsection{Applicability of EBOR methodology for capturing forward-looking aspects of ORM}
\label{ssec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM}

@einemann2018operational, in a theoretical paper, construct a
mathematical framework for an EBOR model to quantify OpRisk for a
portfolio of pending litigations. Their work unearths an invaluable
contribution to the literature, discussing a strategy on how to
integrate EBOR and LDA models by building hybrid frameworks which
facilitate the migration of OpRisk types from a classical to an
exposure-based treatment through a quantitative framework, capturing
forward looking aspects of BEICF's
{[}@einemann2018operational{]}.\medskip

The fundamental premise of the tricky nature behind ORMF, is to provide
an exposure-based treatment of OpRisk losses which caters to modeling
capital estimates for forward-looking aspects of ORM due to the lag in
the loss data. By the very nature of OpRisk, there is usually a
significant lag between the moment the OpRisk event is conceived to the
moment the event is observed and accounted.i.e., there is a gap in time
between the moment the risk is conceived and the realised losses. This
timing paradox often results in questionable capital estimates,
especially for those near misses, pending and realised losses that need
to be captured in the model.\medskip

Exposure is residual risk, or the risk that remains after risk
treatments have been applied. In the ORMF context, it is defined as:

\subsection{Definition of exposure}
\label{ssec:Definition of exposure}

The \textbf{exposure} of risk type \(i\), \(d_{i}\) is the time
interval, expressed in units of time, from the initial moment when the
event happened, until the occurrence of a risk correction.\medskip 

\subsection{Definition of rate}
\label{ssec:Definition of rate}

The \textbf{rate} is defined as mean count per unit exposure, i.e.

\begin{eqnarray}
R &=& \frac{\mu}{\tau} \qquad \mbox{where} \qquad R = \mbox{rate,} \quad \tau = \mbox{exposure}, \mbox{and} \\
\mu &=& \mbox{mean count over an exposure duration of} \tau \nonumber
\end{eqnarray}

\textbf{\section{Intepretation}} \label{sec:Intepretation}

In turn, with reference to
\ref{ssec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
the fundamental premise behind the LDA is that each firm's OpRisk losses
are a reflection of it's underlying Oprisk exposure. In particular, the
assumption behind the use of the poisson model to estimate the frequency
of losses, is that both the the intensity (or rate) of occurrence and
the opportunity (or exposure) for counting are constant for all
available observations.\medskip

The measure of exposure we need to use depends specifically on
projecting the number of Oprisk event types (frequency of losses) and is
different to the measure if the target variable were the severity of the
losses. We need historical exposure for experience rating because we
need to be able to compare the loss experience of different years on a
like-for-like basis and to adjust it to current exposure
levels{[}@parodi2014pricing{]}.\medskip 

When observed counts all have the same exposure, modeling the mean count
\(\mu\) as a function of explanatory variables \(x_{1},\ldots,x_{p}\) is
the same as modeling the rate \(R\).

\textbf{\section{Benefits and Limitations}}
\label{sec:Benefits and Limitations}

These approaches in
\ref{sec:Current operational risk measurement modeling framework}, were
found to have significant advantages over conventional LDA methods,
proposing that an optimal mix of the two modeling elements could more
accurately predict OpRisk \texttt{VaR} over traditional methods.
Particularly @agostini2010combining, whose integration model represents
a benchmark in OpRisk measurement by including a component in the AMA
model that is not obtained by a direct average of historical and
subjective VaR.\medskip

Instead, the basic idea of the integration methodology in
\ref{sec:A new class of models capturing forward-looking aspects} is to
estimate the parameters of the frequency and severity distributions
based on the historical losses and correct them; via a statistical
theory, to include information coming from the scenario analysis. The
method has the advantage of being completely self contained and
independent of any arbitrary choice in the weight of the historical or
subjective component of the model, made by the analyst. The components
weights are derived in an objective and robust way, based on the
statistical uncertainty of information sources, rather than through risk
managers choices based on qualitative motivations. \medskip

However, they could not explain the prerequisite coherence between the
historical and subjective distribution function needed in order for the
model to work; particularly when a number of papers
{[}@chau2014robust{]}, propose using mixtures of (heavy tailed)
distributions commonly used in the setting of OpRisk capital estimation
{[}@opdyke2014estimating{]}.\medskip

In
\ref{ssec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
their model {[}@einemann2018operational{]} is particularly well-suited
to the specific risk type dealt with in their paper i.e., the portfolio
of litigation events, due to better usage of existing information and
more plausible model behavior over the litigation life cycle, but is
bound to under-perform for many other OpRisk event types, since these
EBOR models are typically designed to quantify specific aspects of
OpRisk - litigation risk have rather concentrated risk profiles.
However, EBOR models are important due to wide applicability beyond
capital calculation and its potential to evolve into an important tool
for auditing process and early detection of potential losses.\medskip

\textbf{\section{Gap in the Literature}}
\label{sec:Gap in the Literature}

There is cognitive pressure which seeks to remove information which we
are largely unaware of, because they are undetectable to human senses
that no one could ever see them. We seek to remove this pressure,
effectively lowering uncertainty and allowing us to position ourselves
to develop a defense against our cognitive biases. It is through
patterns in that information that we are largely unaware of that
predictions could arise; or that, OpRisk management incorporates rather
than dismiss the many alternatives that were not imagined, the
possibility of market inefficiencies or finding value in unusual places.

\textbf{\section{Conclusion}} \label{sec:Conclusion}

A substantial body of evidence suggests that loss aversion, the tendency
to be more sensitive to losses than to gains plays an important role in
determining how people evaluate risky gambles. In this paper we evidence
that human choice behavoir can substantially deviate from neoclassical
norms.\medskip

PT takes into account the loss avoidance agents and common attitudes
toward risk or chance that cannot be captured by EUT; which is not
testing for that inherent bias, so as to expect the probability of
making the same operational error in future to be overcompensated for
i.e., If an institution suffers from an OpRisk event and survives, it's
highly unlikely to suffer the same loss in the future because they will
over-provide for particular operational loss due to their natural risk
aversion. This is a testable proposition which fits normal behavioral
patterns and is consistent with risk averse behaviour.


\end{document}
