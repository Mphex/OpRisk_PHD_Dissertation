\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}

\doublespacing

\textbf{\section{Introduction}} \label{sec:Introduction}

A look into literary sources for OpRisk indicates
{[}@acharyya2012current{]} that there is insufficient academic
literature that looks to characterize its theoretical roots, as it is a
relatively new discipline, choosing instead to focus on proposing a
solution to the quantification of OpRisk. This chapter seeks to provide
an overview of some of the antecedents of OpRisk measurement and
management in the banking industry. As such, this chapter provides a
discussion on why OpRisk is not trivial to quantify and attempts to
understand its properties in the context of risk aversion with the
thinking of practitioners and academics in this field.\medskip

According to @cruz2002modeling, FI's wish to measure the impact of
operational events upon profit and loss (P\&L), these events depict the
idea of explaining the \emph{volatility of earnings} due to OpRisk data
points which are directly observed and recorded. By seeking to
incorporate data intensive statistical approaches to help understand the
data, the framework analyses response variables that are decidedly
non-normal (including categorical outcomes and discrete counts) which
can shed further light on the understanding of firm-level OpRisk RC.
Lastly, a synopsis of gaps in the literature is presented.

\section{The theoretical foundation of OpRisk}
\label{sec:The theoretical foundation of OpRisk}

@hemrit2012major argue that common and systematic operational errors in
hypothetical situations poses presumtive evidence that OpRisk events,
assuming that the subjects have no reason to disguise their preferences,
are created sub-consciously. This study purports, supported by
experimental evidence, behavioural finance theories should take some of
this behaviour into account in trying to explain, in the context of a
model, how investors maximise a specific utility/value function.
Furthermore its argued by integrating OpRisk management into behavioral
finance
theory,\footnote{In behavioral finance, we investigate whether certain financial phenomena are the result of less than fully rational thinking [@barberis2003survey]},
that it may be possible to improve our understanding of firm level RC by
refining the resulting OpRisk models to account for these behavioral
traits - implying that people's economic preferences described in the
model, have an economic incentive to improve the OpRisk RC measure.
\medskip

There is cognitive pressure which seeks to remove information which we
are largely unaware of, because they are undetectable to human senses
that no one could ever see them. We seek to remove this pressure,
effectively lowering uncertainty and allowing us to position ourselves
to develop a defense against our cognitive biases. It is through
patterns in that information that we are largely unaware of that
predictions could arise; or that, OpRisk management incorporates rather
than dismiss the many alternatives that were not imagined, the
possibility of market inefficiencies or finding value in unusual places.
\medskip

In a theoretical paper, @wiseman1997longitudinal discussed several
organizational and behavioural theories, such as PT, which influence
managerial risk-taking attitudes. Their findings demonstrate that
behavioural views, such as PT and the behavioural theory of the firm
explain risk seeking and risk averse behaviour in the context of OpRisk
even after agency based influences are controlled for. Furthermore, they
challenge arguments that behavioral influences are masking underlying
root causes due to agency effects. Instead they argue for mixing
behavioral models with agency based views to obtain more complete
explanations of risk preferences and risk taking behavior
{[}@wiseman1997longitudinal{]}. \medskip 

\begin{quote}
 \lq\lq An analysis of literature suggests that the theoretical foundation of operational risk has evolved from the field of strategic management research. Although there is in-sufficient academic literature that explicitly gives theoretical foundation of operational risk, there are considerable works of strategists that can be utilised to establish a conceptual framework of operational risk for financial firms \rq\rq\ [@acharyya2012current, pg.8].
\end{quote}

\section{Overview of operational risk management}
\label{sec:Overview of operational risk management}

It is important to note how OpRisk manifests itself;
@king2001operational establishes the causes and sources of operational
events are observed phenomena associated with operational errors and are
wide ranging. As such, P\&L volatitlity is not only related to the way
firms finance their business, but also in the way they \emph{operate},
these operational events are almost always initiated at the dealing
phase of a trading process. OpRisk management undertakes a broad view of
P\&L attribution carried out from deal origination to settlement within
the perspective of strategic management and detects the
interrelationships between operational risk factors with others to
conceptualise the potential overall consequences
{[}@acharyya2012current{]}.\medskip

we assume that looking at or on hearing an instruction we are
consciously analysing and accurately executing it based on the
information that our senses reveal. However, this isn't entirely true in
OpRisk because operational events that occur indicate that they were
experienced before one is consciously aware of them e.g., during the
trading process in cases where OpRisk events occur as a result of a
mismatch between the trade booked (booking in trade feed) and the
details agreed by the trader; human error (a sub-conscious phenomenon)
is usually quoted as the source of error, and the trade is fixed by
\lq\lq amending\rq\rq~or manually changing the trade details. \medskip

Furthermore, @acharyya2012current recognised that organizations may hold
OpRisk due to external causes, such as failure of third parties or
vendors (either intentionally or unintentionally), in maintaining
promises or contracts. The criticism in the literature is that no amount
of capital is realistically reliable for the determination of RC as a
buffer to OpRisk, particularly the effectiveness of the approach of
capital adequacy from external events, as there is effectively no
control over them.\medskip

Despite the reality that OpRisk does not lend itself to scientific
analysis in the way that market risk and credit risk do, someone must do
the analysis, value the RC measurement and hope the market reflects
this. Besides, financial markets are not objectively scientific, a large
percentage of successful people have been lucky in their forecasts, it
is not an area which lends itself to scientific analysis.\medskip

\section{Operational risk measurement}
\label{sec:Operational risk measurement}\subsection{Loss Distribution Approach}

The Loss Distribution Approach (LDA) is an AMA method whose main
objective is to provide realistic estimates to calculate VaR for OpRisk
RC in the banking sector and it's business units based on loss
distributions that accurately reflect the frequency and severity loss
distributions of the underlying data. Having calculated separately the
frequency and severity distributions, we need to combine them into one
aggregate loss distribution that allows us to produce a value for the
OpRisk VaR. There is no simple way of aggregating the frequency and
severity distribution. Numerical approximation techniques (computer
algorithms) successfully bridge the divide between theory and
implementation for the problems of mathematical analysis. \medskip

The aggregated losses at time \(t\) are given by
\(\vartheta(t) = \sum_{n=1}^{N(t)} X_{n}\) (where X represents
individual operational losses). Frequency and severity distributions are
estimated, e.g., the poisson distribution is a representation of a
discrete variable commonly used to model operational event frequency
(counts), and a selection from continuous distributions which can be
linear (e.g.~gamma distribution) or non-linear (e.g.~lognormal
distribution) for operational loss severity amounts. The compound loss
distribution \(\mathbf{G}(t)\) can now be derived. Taking the aggregated
losses we obtain:

\begin{equation}\label{eqn1}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}

For most choices of \(N(t)\) and \(X_{n}\), the derivation of an
explicit formula for

\begin{math}\mathbf{G}_{\vartheta(t)}(x) \end{math}

is, in most cases impossible.

\begin{math} \mathbf{G}(t)\end{math}

can only be obtained numerically using the Monte Carlo method, Panjer's
recursive approach, and the inverse of the characteristic function
(@frachot2001loss); @aue2006lda); @panjer2006operational; \& others).
\medskip

After most complex banks adopted the LDA for accounting for RC,
significant biases and delimitations in loss data remain when trying to
attribute capital requirements to OpRisk losses {[}@frachot2001loss{]}.
OpRisk is related to the internal processes of the FI, hence the quality
and quantity of internal data (optimally combined with external data)
are of greater concern as the available data could be rare and/or of
poor quality. Such expositions are unsatisfactory if OpRisk, as
@cruz2002modeling professes, represents the next frontier in reducing
the riskiness associated with earnings.

\subsection{LDA model shortcomings}
\label{ssec:LDA model shortcomings}

@opdyke2014estimating advanced studies intending on eliminating bias
apparently due to heavy tailed distributions to further provide insight
on new techniques to deal with the issues that arise in LDA modeling,
keeping practitioners and academics at breadth with latest research in
OpRisk \texttt{VaR} theory. Recent work in LDA modeling has been found
wanting {[}@badescu2015modeling{]}, due to the very complex
characteristics of data sets in OpRisk \texttt{VaR} modeling, and even
when studies used quality data and adequate historical data points, as
pointed out in a recent paper by @hoohlo2015new, there is a qualitative
aspect in OpRisk modeling that is often ignored, but whose validity
should not be overlooked. \medskip

@opdyke2014estimating, @agostini2010combining, @de2015combining,
@galloppo2014review, and others explicate how greater accuracy,
precision and robustness uphold a valid and reliable estimate for OpRisk
capital as defined by Basel II/III. Transforming this basic knowledge
into \lq\lq risk culture\rq\rq~or firm-wide knowledge for the effective
management of OpRisk, serves as a starting point for a control function
providing attribution and accounting support within a framework,
methodology and theory for understanding OpRisk measurement. FI's are
beginning to implement sophisticated risk management systems similar to
those for market and credit risk, linking theories which govern how
these risk types are controlled to theories that govern financial losses
resulting from OpRisk events. \medskip

@de2015combining and @galloppo2014review sought to address the
shortcomings of @frachot2001loss by finding possible ways to improve the
problems of bias and data delimitation in operational risk management.
They follow the recent literature in finding a statistical-based model
for integrating internal data and external data as well as scenario
assessments in on endeavor to improve on accuracy of the capital
estimate.

\subsection{A new class of models capturing forward-looking aspects}
\label{ssec:A new class of models capturing forward-looking aspects}

@agostini2010combining also argued that banks should adopt an integrated
model by combining a forward-looking component (scenario analysis) to
the historical operational \texttt{VaR}, further adding to the
literature through their integration model which is based on the idea of
estimating the parameters of the historical and subjective distributions
and then combining them by using the advanced CT. \medskip

The idea at the basis of CT is that a better estimation of the OpRisk
measure can be obtained by combining the two sources of information: The
historical loss data and expert's judgements, advocating for the
combined use of both experiences. @agostini2010combining seek to explain
through a weight called the credibility, the amount of credence given to
two components (historical and subjective) determined by statistical
uncertainty of information sources, as opposed to a weighted average
approach chosen on the basis of qualitative judgements.\medskip

Thus generating a more predictable and forward looking capital estimate.
He deemed the integration method as advantageous as it is self contained
and independent of any arbitrary choice in the weight of the historical
or subjective components of the model.

\section{Applicability of EBOR methodology for capturing forward-looking aspects of ORM}
\label{sec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM}

@einemann2018operational, in a theoretical paper, construct a
mathematical framework for an EBOR model to quantify OpRisk for a
portfolio of pending litigations. Their work unearths an invaluable
contribution to the literature, discussing a strategy on how to
integrate EBOR and LDA models by building hybrid frameworks which
facilitate the migration of OpRisk types from a classical to an
exposure-based treatment through a quantitative framework, capturing
forward looking aspects of BEICF's {[}@einemann2018operational{]}.

The fundamental premise of the tricky nature behind ORMF, is to provide
an exposure-based treatment of OpRisk losses which caters to modeling
capital estimates for forward-looking aspects of ORM due to the lag in
the loss data. By the very nature of OpRisk, there is usually a
significant lag between the moment the OpRisk event is conceived to the
moment the event is observed and accounted.i.e., there is a gap in time
between the moment the risk is conceived and the realised losses. This
timing paradox often results in questionable capital estimates,
especially for those near misses, pending and realised losses that need
to be captured in the model.\medskip

\subsubsection{Definition of exposure}

Exposure is residual risk, or the risk that remains after risk
treatments have been applied. In the ORMF context, it is defined as:
\textbf{The risk exposure of risk type $i$, $d_{i}$ is the time interval, expressed in units of time, from the initial moment when the event happened, until the occurrence of a risk correction.}

The measure of exposure we need to use depends specifically on
projecting the number of Oprisk event types (frequency of losses) and is
different to the measure if the target variable were the severity of the
losses. We need historical exposure for experience rating because we
need to be able to compare the loss experience of different years on a
like-for-like basis and to adjust it to current exposure
levels{[}@parodi2014pricing{]}.

\section{Intepretation}
\label{sec:Intepretation}

In turn, with reference to
\ref{sec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
the fundamental premise behind the LDA is that each firm's OpRisk losses
are a reflection of it's underlying Oprisk exposure. In particular, the
assumption behind the use of the poisson model to estimate the frequency
of losses, is that both the the intensity (or rate) of occurrence and
the opportunity (or exposure) for counting are constant for all
available observations.

\subsubsection{Definition of rate}

\lq\lq Rate\rq\rq~is defined as mean count per unit exposure. i.e.

\begin{equation}
R = \frac{\mu}{\tau} \qquad \mbox{where} \qquad \mbox{$R = $rate,} \quad \mbox{$\tau = $exposure} \quad \mbox{and} \quad \mbox{$\mu = $mean count over an exposure duration of $\tau$} \nonumber
\end{equation}

When observed counts all have the same exposure, modeling the mean count
\(\mu\) as a function of explanatory variables
\$x\_\{1\},\ldots,x\_\{p\} \$ is the same as modeling the rate.

\section{Benefits and Limitations}
\label{sec:Benefits and Limitations}

These approaches in \ref{sec:Operational risk measurement}, were found
to have significant advantages over conventional LDA methods, proposing
that an optimal mix of the two modeling elements could more accurately
predict OpRisk \texttt{VaR} over traditional methods. Particularly
@agostini2010combining, whose integration model represents a benchmark
in OpRisk measurement by including a component in the AMA model that is
not obtained by a direct average of historical and subjective
VaR.\medskip

Instead, the basic idea of the integration methodology in
\ref{ssec:A new class of models capturing forward-looking aspects} is to
estimate the parameters of the frequency and severity distributions
based on the historical losses and correct them; via a statistical
theory, to include information coming from the scenario analysis. The
method has the advantage of being completely self contained and
independent of any arbitrary choice in the weight of the historical or
subjective component of the model, made by the analyst. The components
weights are derived in an objective and robust way, based on the
statistical uncertainty of information sources, rather than through risk
managers choices based on qualitative motivations. \medskip

However, they could not explain the prerequisite coherence between the
historical and subjective distribution function needed in order for the
model to work; particularly when a number of papers
{[}@chau2014robust{]}, propose using mixtures of (heavy tailed)
distributions commonly used in the setting of OpRisk capital estimation
{[}@opdyke2014estimating{]}.\medskip

In
\ref{sec:Applicability of EBOR methodology for capturing forward-looking aspects of ORM},
their model {[}@einemann2018operational{]} is particularly well-suited
to the specific risk type dealt with in their paper i.e., the portfolio
of litigation events, due to better usage of existing information and
more plausible model behavior over the litigation life cycle, but is
bound to under-perform for many other OpRisk event types, since these
EBOR models are typically designed to quantify specific aspects of
OpRisk - litigation risk have rather concentrated risk profiles.
However, EBOR models are important due to wide applicability beyond
capital calculation and its potential to evolve into an important tool
for auditing process and early detection of potential losses.\medskip

\section{Gaps in the Literature}

\section{Interpretation}\label{interpretation}

Table \ref{tab_int} presents the various units that the AME will produce
for the various GLM links. It is important to note that AMEs are in the
outcome's original metrics whether they are probabilities, counts, or
something else. The interpretation, then, is ``for a one unit change in
the predictor there is an associated {[}AME{]} change in the outcome.''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"Link Function"}\NormalTok{=}\KeywordTok{c}\NormalTok{(}\StringTok{"Identity"}\NormalTok{, }\StringTok{"Log"}\NormalTok{, }\StringTok{"Logit"}\NormalTok{, }\StringTok{"Probit"}\NormalTok{, }\StringTok{"Poisson"}\NormalTok{, }\StringTok{"Gamma"}\NormalTok{, }\StringTok{"Negative Binomial"}\NormalTok{),}
                  \StringTok{"Average Marginal Effect"}\NormalTok{=}\KeywordTok{c}\NormalTok{(}\StringTok{"Original Continuous Unit"}\NormalTok{, }\StringTok{"Original Continuous Unit"}\NormalTok{, }\StringTok{"Risk"}\NormalTok{, }\StringTok{"Risk"}\NormalTok{, }\StringTok{"Count"}\NormalTok{, }\StringTok{"Count"}\NormalTok{, }\StringTok{"Count"}\NormalTok{))}
\KeywordTok{names}\NormalTok{(tab) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Link Function"}\NormalTok{, }\StringTok{"Average Marginal Effect"}\NormalTok{)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{xtable.comment =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{library}\NormalTok{(xtable)}
\NormalTok{tabx =}\StringTok{ }\KeywordTok{xtable}\NormalTok{(tab, }
       \DataTypeTok{label =} \StringTok{"tab_int"}\NormalTok{, }
       \DataTypeTok{caption =} \StringTok{"The generalized linear model link functions with their associated units of interpretation. Note: This list is not exhaustive and there are likely more GLMs that are used within prevention research."}\NormalTok{,}
       \DataTypeTok{align =} \KeywordTok{c}\NormalTok{(}\StringTok{"l"}\NormalTok{, }\StringTok{"|l"}\NormalTok{, }\StringTok{"|c|"}\NormalTok{))}
\KeywordTok{print.xtable}\NormalTok{(tabx, }\DataTypeTok{include.rownames =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{caption.placement =} \StringTok{"top"}\NormalTok{,}
             \DataTypeTok{table.placement =} \StringTok{"tb"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[tb]
\centering
\caption{The generalized linear model link functions with their associated units of interpretation. Note: This list is not exhaustive and there are likely more GLMs that are used within prevention research.} 
\label{tab_int}
\begin{tabular}{lc}
\toprule
Link Function & Average Marginal Effect \\ 
\midrule
Identity & Original Continuous Unit \\ 
  Log & Original Continuous Unit \\ 
  Logit & Risk \\ 
  Probit & Risk \\ 
  Poisson & Count \\ 
  Gamma & Count \\ 
  Negative Binomial & Count \\ 
\bottomrule
\end{tabular}
\end{table}

\textbf{\section{Conclusion}}

A substantial body of evidence suggests that loss aversion, the tendency
to be more sensitive to losses than to gains plays an important role in
determining how people evaluate risky gambles. In this paper we evidence
that human choice behavoir can substantially deviate from neoclassical
norms. PT takes into account the loss avoidance agents and common
attitudes toward risk or chance that cannot be captured by EUT; which is
not testing for that inherent bias, so as to expect the probability of
making the same operational error in future to be overcompensated for
i.e., If an institution suffers from an OpRisk event and survives, it's
highly unlikely to suffer the same loss in the future because they will
over-provide for particular operational loss due to their natural risk
aversion. This is a testable proposition which fits normal behavioral
patterns and is consistent with risk averse behaviour.


\end{document}
